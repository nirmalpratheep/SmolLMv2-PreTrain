{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d5a2069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rotary-embedding-torch in c:\\users\\ssuga\\anaconda3\\envs\\llm\\lib\\site-packages (0.8.6)\n",
      "Requirement already satisfied: transformers in c:\\users\\ssuga\\anaconda3\\envs\\llm\\lib\\site-packages (4.49.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\ssuga\\anaconda3\\envs\\llm\\lib\\site-packages (6.0.2)\n",
      "Requirement already satisfied: einops>=0.7 in c:\\users\\ssuga\\anaconda3\\envs\\llm\\lib\\site-packages (from rotary-embedding-torch) (0.8.1)\n",
      "Requirement already satisfied: torch>=2.0 in c:\\users\\ssuga\\anaconda3\\envs\\llm\\lib\\site-packages (from rotary-embedding-torch) (2.5.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\ssuga\\anaconda3\\envs\\llm\\lib\\site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in c:\\users\\ssuga\\anaconda3\\envs\\llm\\lib\\site-packages (from transformers) (0.29.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\ssuga\\anaconda3\\envs\\llm\\lib\\site-packages (from transformers) (2.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ssuga\\anaconda3\\envs\\llm\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\ssuga\\anaconda3\\envs\\llm\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\ssuga\\anaconda3\\envs\\llm\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\ssuga\\anaconda3\\envs\\llm\\lib\\site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\ssuga\\anaconda3\\envs\\llm\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\ssuga\\anaconda3\\envs\\llm\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\ssuga\\anaconda3\\envs\\llm\\lib\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\ssuga\\anaconda3\\envs\\llm\\lib\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.13.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ssuga\\anaconda3\\envs\\llm\\lib\\site-packages (from torch>=2.0->rotary-embedding-torch) (78.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\ssuga\\anaconda3\\envs\\llm\\lib\\site-packages (from torch>=2.0->rotary-embedding-torch) (1.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\ssuga\\anaconda3\\envs\\llm\\lib\\site-packages (from torch>=2.0->rotary-embedding-torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ssuga\\anaconda3\\envs\\llm\\lib\\site-packages (from torch>=2.0->rotary-embedding-torch) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\ssuga\\anaconda3\\envs\\llm\\lib\\site-packages (from sympy==1.13.1->torch>=2.0->rotary-embedding-torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\ssuga\\anaconda3\\envs\\llm\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ssuga\\anaconda3\\envs\\llm\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ssuga\\anaconda3\\envs\\llm\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ssuga\\anaconda3\\envs\\llm\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ssuga\\anaconda3\\envs\\llm\\lib\\site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ssuga\\anaconda3\\envs\\llm\\lib\\site-packages (from jinja2->torch>=2.0->rotary-embedding-torch) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ssuga\\anaconda3\\envs\\llm\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All dependencies imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "%pip install rotary-embedding-torch transformers pyyaml\n",
    "\n",
    "# Import all dependencies\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import inspect\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# Transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# YAML configuration\n",
    "import yaml\n",
    "\n",
    "# Rotary Embedding\n",
    "from rotary_embedding_torch import RotaryEmbedding\n",
    "\n",
    "print(\"All dependencies imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f5cd96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All imports are in the first cell\n",
    "# This cell is kept for backward compatibility or can be removed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b2bacb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-135M\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM2-135M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93fb5c08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(49152, 576)\n",
       "    (layers): ModuleList(\n",
       "      (0-29): 30 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
       "          (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
       "          (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
       "          (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
       "          (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
       "          (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((576,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=576, out_features=49152, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b9035ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(49152, 576)\n",
      "    (layers): ModuleList(\n",
      "      (0-29): 30 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "          (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "          (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "          (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "          (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "          (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((576,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=576, out_features=49152, bias=False)\n",
      ")\n",
      "134515008\n",
      "+-------------------------------------------------+------------+\n",
      "|                     Modules                     | Parameters |\n",
      "+-------------------------------------------------+------------+\n",
      "|            model.embed_tokens.weight            |  28311552  |\n",
      "|      model.layers.0.self_attn.q_proj.weight     |   331776   |\n",
      "|      model.layers.0.self_attn.k_proj.weight     |   110592   |\n",
      "|      model.layers.0.self_attn.v_proj.weight     |   110592   |\n",
      "|      model.layers.0.self_attn.o_proj.weight     |   331776   |\n",
      "|       model.layers.0.mlp.gate_proj.weight       |   884736   |\n",
      "|        model.layers.0.mlp.up_proj.weight        |   884736   |\n",
      "|       model.layers.0.mlp.down_proj.weight       |   884736   |\n",
      "|      model.layers.0.input_layernorm.weight      |    576     |\n",
      "|  model.layers.0.post_attention_layernorm.weight |    576     |\n",
      "|      model.layers.1.self_attn.q_proj.weight     |   331776   |\n",
      "|      model.layers.1.self_attn.k_proj.weight     |   110592   |\n",
      "|      model.layers.1.self_attn.v_proj.weight     |   110592   |\n",
      "|      model.layers.1.self_attn.o_proj.weight     |   331776   |\n",
      "|       model.layers.1.mlp.gate_proj.weight       |   884736   |\n",
      "|        model.layers.1.mlp.up_proj.weight        |   884736   |\n",
      "|       model.layers.1.mlp.down_proj.weight       |   884736   |\n",
      "|      model.layers.1.input_layernorm.weight      |    576     |\n",
      "|  model.layers.1.post_attention_layernorm.weight |    576     |\n",
      "|      model.layers.2.self_attn.q_proj.weight     |   331776   |\n",
      "|      model.layers.2.self_attn.k_proj.weight     |   110592   |\n",
      "|      model.layers.2.self_attn.v_proj.weight     |   110592   |\n",
      "|      model.layers.2.self_attn.o_proj.weight     |   331776   |\n",
      "|       model.layers.2.mlp.gate_proj.weight       |   884736   |\n",
      "|        model.layers.2.mlp.up_proj.weight        |   884736   |\n",
      "|       model.layers.2.mlp.down_proj.weight       |   884736   |\n",
      "|      model.layers.2.input_layernorm.weight      |    576     |\n",
      "|  model.layers.2.post_attention_layernorm.weight |    576     |\n",
      "|      model.layers.3.self_attn.q_proj.weight     |   331776   |\n",
      "|      model.layers.3.self_attn.k_proj.weight     |   110592   |\n",
      "|      model.layers.3.self_attn.v_proj.weight     |   110592   |\n",
      "|      model.layers.3.self_attn.o_proj.weight     |   331776   |\n",
      "|       model.layers.3.mlp.gate_proj.weight       |   884736   |\n",
      "|        model.layers.3.mlp.up_proj.weight        |   884736   |\n",
      "|       model.layers.3.mlp.down_proj.weight       |   884736   |\n",
      "|      model.layers.3.input_layernorm.weight      |    576     |\n",
      "|  model.layers.3.post_attention_layernorm.weight |    576     |\n",
      "|      model.layers.4.self_attn.q_proj.weight     |   331776   |\n",
      "|      model.layers.4.self_attn.k_proj.weight     |   110592   |\n",
      "|      model.layers.4.self_attn.v_proj.weight     |   110592   |\n",
      "|      model.layers.4.self_attn.o_proj.weight     |   331776   |\n",
      "|       model.layers.4.mlp.gate_proj.weight       |   884736   |\n",
      "|        model.layers.4.mlp.up_proj.weight        |   884736   |\n",
      "|       model.layers.4.mlp.down_proj.weight       |   884736   |\n",
      "|      model.layers.4.input_layernorm.weight      |    576     |\n",
      "|  model.layers.4.post_attention_layernorm.weight |    576     |\n",
      "|      model.layers.5.self_attn.q_proj.weight     |   331776   |\n",
      "|      model.layers.5.self_attn.k_proj.weight     |   110592   |\n",
      "|      model.layers.5.self_attn.v_proj.weight     |   110592   |\n",
      "|      model.layers.5.self_attn.o_proj.weight     |   331776   |\n",
      "|       model.layers.5.mlp.gate_proj.weight       |   884736   |\n",
      "|        model.layers.5.mlp.up_proj.weight        |   884736   |\n",
      "|       model.layers.5.mlp.down_proj.weight       |   884736   |\n",
      "|      model.layers.5.input_layernorm.weight      |    576     |\n",
      "|  model.layers.5.post_attention_layernorm.weight |    576     |\n",
      "|      model.layers.6.self_attn.q_proj.weight     |   331776   |\n",
      "|      model.layers.6.self_attn.k_proj.weight     |   110592   |\n",
      "|      model.layers.6.self_attn.v_proj.weight     |   110592   |\n",
      "|      model.layers.6.self_attn.o_proj.weight     |   331776   |\n",
      "|       model.layers.6.mlp.gate_proj.weight       |   884736   |\n",
      "|        model.layers.6.mlp.up_proj.weight        |   884736   |\n",
      "|       model.layers.6.mlp.down_proj.weight       |   884736   |\n",
      "|      model.layers.6.input_layernorm.weight      |    576     |\n",
      "|  model.layers.6.post_attention_layernorm.weight |    576     |\n",
      "|      model.layers.7.self_attn.q_proj.weight     |   331776   |\n",
      "|      model.layers.7.self_attn.k_proj.weight     |   110592   |\n",
      "|      model.layers.7.self_attn.v_proj.weight     |   110592   |\n",
      "|      model.layers.7.self_attn.o_proj.weight     |   331776   |\n",
      "|       model.layers.7.mlp.gate_proj.weight       |   884736   |\n",
      "|        model.layers.7.mlp.up_proj.weight        |   884736   |\n",
      "|       model.layers.7.mlp.down_proj.weight       |   884736   |\n",
      "|      model.layers.7.input_layernorm.weight      |    576     |\n",
      "|  model.layers.7.post_attention_layernorm.weight |    576     |\n",
      "|      model.layers.8.self_attn.q_proj.weight     |   331776   |\n",
      "|      model.layers.8.self_attn.k_proj.weight     |   110592   |\n",
      "|      model.layers.8.self_attn.v_proj.weight     |   110592   |\n",
      "|      model.layers.8.self_attn.o_proj.weight     |   331776   |\n",
      "|       model.layers.8.mlp.gate_proj.weight       |   884736   |\n",
      "|        model.layers.8.mlp.up_proj.weight        |   884736   |\n",
      "|       model.layers.8.mlp.down_proj.weight       |   884736   |\n",
      "|      model.layers.8.input_layernorm.weight      |    576     |\n",
      "|  model.layers.8.post_attention_layernorm.weight |    576     |\n",
      "|      model.layers.9.self_attn.q_proj.weight     |   331776   |\n",
      "|      model.layers.9.self_attn.k_proj.weight     |   110592   |\n",
      "|      model.layers.9.self_attn.v_proj.weight     |   110592   |\n",
      "|      model.layers.9.self_attn.o_proj.weight     |   331776   |\n",
      "|       model.layers.9.mlp.gate_proj.weight       |   884736   |\n",
      "|        model.layers.9.mlp.up_proj.weight        |   884736   |\n",
      "|       model.layers.9.mlp.down_proj.weight       |   884736   |\n",
      "|      model.layers.9.input_layernorm.weight      |    576     |\n",
      "|  model.layers.9.post_attention_layernorm.weight |    576     |\n",
      "|     model.layers.10.self_attn.q_proj.weight     |   331776   |\n",
      "|     model.layers.10.self_attn.k_proj.weight     |   110592   |\n",
      "|     model.layers.10.self_attn.v_proj.weight     |   110592   |\n",
      "|     model.layers.10.self_attn.o_proj.weight     |   331776   |\n",
      "|       model.layers.10.mlp.gate_proj.weight      |   884736   |\n",
      "|        model.layers.10.mlp.up_proj.weight       |   884736   |\n",
      "|       model.layers.10.mlp.down_proj.weight      |   884736   |\n",
      "|      model.layers.10.input_layernorm.weight     |    576     |\n",
      "| model.layers.10.post_attention_layernorm.weight |    576     |\n",
      "|     model.layers.11.self_attn.q_proj.weight     |   331776   |\n",
      "|     model.layers.11.self_attn.k_proj.weight     |   110592   |\n",
      "|     model.layers.11.self_attn.v_proj.weight     |   110592   |\n",
      "|     model.layers.11.self_attn.o_proj.weight     |   331776   |\n",
      "|       model.layers.11.mlp.gate_proj.weight      |   884736   |\n",
      "|        model.layers.11.mlp.up_proj.weight       |   884736   |\n",
      "|       model.layers.11.mlp.down_proj.weight      |   884736   |\n",
      "|      model.layers.11.input_layernorm.weight     |    576     |\n",
      "| model.layers.11.post_attention_layernorm.weight |    576     |\n",
      "|     model.layers.12.self_attn.q_proj.weight     |   331776   |\n",
      "|     model.layers.12.self_attn.k_proj.weight     |   110592   |\n",
      "|     model.layers.12.self_attn.v_proj.weight     |   110592   |\n",
      "|     model.layers.12.self_attn.o_proj.weight     |   331776   |\n",
      "|       model.layers.12.mlp.gate_proj.weight      |   884736   |\n",
      "|        model.layers.12.mlp.up_proj.weight       |   884736   |\n",
      "|       model.layers.12.mlp.down_proj.weight      |   884736   |\n",
      "|      model.layers.12.input_layernorm.weight     |    576     |\n",
      "| model.layers.12.post_attention_layernorm.weight |    576     |\n",
      "|     model.layers.13.self_attn.q_proj.weight     |   331776   |\n",
      "|     model.layers.13.self_attn.k_proj.weight     |   110592   |\n",
      "|     model.layers.13.self_attn.v_proj.weight     |   110592   |\n",
      "|     model.layers.13.self_attn.o_proj.weight     |   331776   |\n",
      "|       model.layers.13.mlp.gate_proj.weight      |   884736   |\n",
      "|        model.layers.13.mlp.up_proj.weight       |   884736   |\n",
      "|       model.layers.13.mlp.down_proj.weight      |   884736   |\n",
      "|      model.layers.13.input_layernorm.weight     |    576     |\n",
      "| model.layers.13.post_attention_layernorm.weight |    576     |\n",
      "|     model.layers.14.self_attn.q_proj.weight     |   331776   |\n",
      "|     model.layers.14.self_attn.k_proj.weight     |   110592   |\n",
      "|     model.layers.14.self_attn.v_proj.weight     |   110592   |\n",
      "|     model.layers.14.self_attn.o_proj.weight     |   331776   |\n",
      "|       model.layers.14.mlp.gate_proj.weight      |   884736   |\n",
      "|        model.layers.14.mlp.up_proj.weight       |   884736   |\n",
      "|       model.layers.14.mlp.down_proj.weight      |   884736   |\n",
      "|      model.layers.14.input_layernorm.weight     |    576     |\n",
      "| model.layers.14.post_attention_layernorm.weight |    576     |\n",
      "|     model.layers.15.self_attn.q_proj.weight     |   331776   |\n",
      "|     model.layers.15.self_attn.k_proj.weight     |   110592   |\n",
      "|     model.layers.15.self_attn.v_proj.weight     |   110592   |\n",
      "|     model.layers.15.self_attn.o_proj.weight     |   331776   |\n",
      "|       model.layers.15.mlp.gate_proj.weight      |   884736   |\n",
      "|        model.layers.15.mlp.up_proj.weight       |   884736   |\n",
      "|       model.layers.15.mlp.down_proj.weight      |   884736   |\n",
      "|      model.layers.15.input_layernorm.weight     |    576     |\n",
      "| model.layers.15.post_attention_layernorm.weight |    576     |\n",
      "|     model.layers.16.self_attn.q_proj.weight     |   331776   |\n",
      "|     model.layers.16.self_attn.k_proj.weight     |   110592   |\n",
      "|     model.layers.16.self_attn.v_proj.weight     |   110592   |\n",
      "|     model.layers.16.self_attn.o_proj.weight     |   331776   |\n",
      "|       model.layers.16.mlp.gate_proj.weight      |   884736   |\n",
      "|        model.layers.16.mlp.up_proj.weight       |   884736   |\n",
      "|       model.layers.16.mlp.down_proj.weight      |   884736   |\n",
      "|      model.layers.16.input_layernorm.weight     |    576     |\n",
      "| model.layers.16.post_attention_layernorm.weight |    576     |\n",
      "|     model.layers.17.self_attn.q_proj.weight     |   331776   |\n",
      "|     model.layers.17.self_attn.k_proj.weight     |   110592   |\n",
      "|     model.layers.17.self_attn.v_proj.weight     |   110592   |\n",
      "|     model.layers.17.self_attn.o_proj.weight     |   331776   |\n",
      "|       model.layers.17.mlp.gate_proj.weight      |   884736   |\n",
      "|        model.layers.17.mlp.up_proj.weight       |   884736   |\n",
      "|       model.layers.17.mlp.down_proj.weight      |   884736   |\n",
      "|      model.layers.17.input_layernorm.weight     |    576     |\n",
      "| model.layers.17.post_attention_layernorm.weight |    576     |\n",
      "|     model.layers.18.self_attn.q_proj.weight     |   331776   |\n",
      "|     model.layers.18.self_attn.k_proj.weight     |   110592   |\n",
      "|     model.layers.18.self_attn.v_proj.weight     |   110592   |\n",
      "|     model.layers.18.self_attn.o_proj.weight     |   331776   |\n",
      "|       model.layers.18.mlp.gate_proj.weight      |   884736   |\n",
      "|        model.layers.18.mlp.up_proj.weight       |   884736   |\n",
      "|       model.layers.18.mlp.down_proj.weight      |   884736   |\n",
      "|      model.layers.18.input_layernorm.weight     |    576     |\n",
      "| model.layers.18.post_attention_layernorm.weight |    576     |\n",
      "|     model.layers.19.self_attn.q_proj.weight     |   331776   |\n",
      "|     model.layers.19.self_attn.k_proj.weight     |   110592   |\n",
      "|     model.layers.19.self_attn.v_proj.weight     |   110592   |\n",
      "|     model.layers.19.self_attn.o_proj.weight     |   331776   |\n",
      "|       model.layers.19.mlp.gate_proj.weight      |   884736   |\n",
      "|        model.layers.19.mlp.up_proj.weight       |   884736   |\n",
      "|       model.layers.19.mlp.down_proj.weight      |   884736   |\n",
      "|      model.layers.19.input_layernorm.weight     |    576     |\n",
      "| model.layers.19.post_attention_layernorm.weight |    576     |\n",
      "|     model.layers.20.self_attn.q_proj.weight     |   331776   |\n",
      "|     model.layers.20.self_attn.k_proj.weight     |   110592   |\n",
      "|     model.layers.20.self_attn.v_proj.weight     |   110592   |\n",
      "|     model.layers.20.self_attn.o_proj.weight     |   331776   |\n",
      "|       model.layers.20.mlp.gate_proj.weight      |   884736   |\n",
      "|        model.layers.20.mlp.up_proj.weight       |   884736   |\n",
      "|       model.layers.20.mlp.down_proj.weight      |   884736   |\n",
      "|      model.layers.20.input_layernorm.weight     |    576     |\n",
      "| model.layers.20.post_attention_layernorm.weight |    576     |\n",
      "|     model.layers.21.self_attn.q_proj.weight     |   331776   |\n",
      "|     model.layers.21.self_attn.k_proj.weight     |   110592   |\n",
      "|     model.layers.21.self_attn.v_proj.weight     |   110592   |\n",
      "|     model.layers.21.self_attn.o_proj.weight     |   331776   |\n",
      "|       model.layers.21.mlp.gate_proj.weight      |   884736   |\n",
      "|        model.layers.21.mlp.up_proj.weight       |   884736   |\n",
      "|       model.layers.21.mlp.down_proj.weight      |   884736   |\n",
      "|      model.layers.21.input_layernorm.weight     |    576     |\n",
      "| model.layers.21.post_attention_layernorm.weight |    576     |\n",
      "|     model.layers.22.self_attn.q_proj.weight     |   331776   |\n",
      "|     model.layers.22.self_attn.k_proj.weight     |   110592   |\n",
      "|     model.layers.22.self_attn.v_proj.weight     |   110592   |\n",
      "|     model.layers.22.self_attn.o_proj.weight     |   331776   |\n",
      "|       model.layers.22.mlp.gate_proj.weight      |   884736   |\n",
      "|        model.layers.22.mlp.up_proj.weight       |   884736   |\n",
      "|       model.layers.22.mlp.down_proj.weight      |   884736   |\n",
      "|      model.layers.22.input_layernorm.weight     |    576     |\n",
      "| model.layers.22.post_attention_layernorm.weight |    576     |\n",
      "|     model.layers.23.self_attn.q_proj.weight     |   331776   |\n",
      "|     model.layers.23.self_attn.k_proj.weight     |   110592   |\n",
      "|     model.layers.23.self_attn.v_proj.weight     |   110592   |\n",
      "|     model.layers.23.self_attn.o_proj.weight     |   331776   |\n",
      "|       model.layers.23.mlp.gate_proj.weight      |   884736   |\n",
      "|        model.layers.23.mlp.up_proj.weight       |   884736   |\n",
      "|       model.layers.23.mlp.down_proj.weight      |   884736   |\n",
      "|      model.layers.23.input_layernorm.weight     |    576     |\n",
      "| model.layers.23.post_attention_layernorm.weight |    576     |\n",
      "|     model.layers.24.self_attn.q_proj.weight     |   331776   |\n",
      "|     model.layers.24.self_attn.k_proj.weight     |   110592   |\n",
      "|     model.layers.24.self_attn.v_proj.weight     |   110592   |\n",
      "|     model.layers.24.self_attn.o_proj.weight     |   331776   |\n",
      "|       model.layers.24.mlp.gate_proj.weight      |   884736   |\n",
      "|        model.layers.24.mlp.up_proj.weight       |   884736   |\n",
      "|       model.layers.24.mlp.down_proj.weight      |   884736   |\n",
      "|      model.layers.24.input_layernorm.weight     |    576     |\n",
      "| model.layers.24.post_attention_layernorm.weight |    576     |\n",
      "|     model.layers.25.self_attn.q_proj.weight     |   331776   |\n",
      "|     model.layers.25.self_attn.k_proj.weight     |   110592   |\n",
      "|     model.layers.25.self_attn.v_proj.weight     |   110592   |\n",
      "|     model.layers.25.self_attn.o_proj.weight     |   331776   |\n",
      "|       model.layers.25.mlp.gate_proj.weight      |   884736   |\n",
      "|        model.layers.25.mlp.up_proj.weight       |   884736   |\n",
      "|       model.layers.25.mlp.down_proj.weight      |   884736   |\n",
      "|      model.layers.25.input_layernorm.weight     |    576     |\n",
      "| model.layers.25.post_attention_layernorm.weight |    576     |\n",
      "|     model.layers.26.self_attn.q_proj.weight     |   331776   |\n",
      "|     model.layers.26.self_attn.k_proj.weight     |   110592   |\n",
      "|     model.layers.26.self_attn.v_proj.weight     |   110592   |\n",
      "|     model.layers.26.self_attn.o_proj.weight     |   331776   |\n",
      "|       model.layers.26.mlp.gate_proj.weight      |   884736   |\n",
      "|        model.layers.26.mlp.up_proj.weight       |   884736   |\n",
      "|       model.layers.26.mlp.down_proj.weight      |   884736   |\n",
      "|      model.layers.26.input_layernorm.weight     |    576     |\n",
      "| model.layers.26.post_attention_layernorm.weight |    576     |\n",
      "|     model.layers.27.self_attn.q_proj.weight     |   331776   |\n",
      "|     model.layers.27.self_attn.k_proj.weight     |   110592   |\n",
      "|     model.layers.27.self_attn.v_proj.weight     |   110592   |\n",
      "|     model.layers.27.self_attn.o_proj.weight     |   331776   |\n",
      "|       model.layers.27.mlp.gate_proj.weight      |   884736   |\n",
      "|        model.layers.27.mlp.up_proj.weight       |   884736   |\n",
      "|       model.layers.27.mlp.down_proj.weight      |   884736   |\n",
      "|      model.layers.27.input_layernorm.weight     |    576     |\n",
      "| model.layers.27.post_attention_layernorm.weight |    576     |\n",
      "|     model.layers.28.self_attn.q_proj.weight     |   331776   |\n",
      "|     model.layers.28.self_attn.k_proj.weight     |   110592   |\n",
      "|     model.layers.28.self_attn.v_proj.weight     |   110592   |\n",
      "|     model.layers.28.self_attn.o_proj.weight     |   331776   |\n",
      "|       model.layers.28.mlp.gate_proj.weight      |   884736   |\n",
      "|        model.layers.28.mlp.up_proj.weight       |   884736   |\n",
      "|       model.layers.28.mlp.down_proj.weight      |   884736   |\n",
      "|      model.layers.28.input_layernorm.weight     |    576     |\n",
      "| model.layers.28.post_attention_layernorm.weight |    576     |\n",
      "|     model.layers.29.self_attn.q_proj.weight     |   331776   |\n",
      "|     model.layers.29.self_attn.k_proj.weight     |   110592   |\n",
      "|     model.layers.29.self_attn.v_proj.weight     |   110592   |\n",
      "|     model.layers.29.self_attn.o_proj.weight     |   331776   |\n",
      "|       model.layers.29.mlp.gate_proj.weight      |   884736   |\n",
      "|        model.layers.29.mlp.up_proj.weight       |   884736   |\n",
      "|       model.layers.29.mlp.down_proj.weight      |   884736   |\n",
      "|      model.layers.29.input_layernorm.weight     |    576     |\n",
      "| model.layers.29.post_attention_layernorm.weight |    576     |\n",
      "|                model.norm.weight                |    576     |\n",
      "+-------------------------------------------------+------------+\n",
      "Total Trainable Params: 134515008\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "134515008"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model)\n",
    "print(sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad:\n",
    "            continue\n",
    "        params = parameter.numel()\n",
    "        table.add_row([name, params])\n",
    "        total_params += params\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params\n",
    "    \n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42338af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Loaded SmolLMV2 Configuration\n",
      "============================================================\n",
      "\n",
      "[Model Configuration]\n",
      "  model_type: llama\n",
      "  vocab_size: 49152\n",
      "  hidden_size: 576\n",
      "  intermediate_size: 1536\n",
      "  num_hidden_layers: 30\n",
      "  num_attention_heads: 18\n",
      "  num_key_value_heads: 6\n",
      "  max_position_embeddings: 8192\n",
      "  rms_norm_eps: 1e-05\n",
      "  use_cache: True\n",
      "  pad_token_id: None\n",
      "  bos_token_id: 1\n",
      "  eos_token_id: 2\n",
      "  mlp_bias: False\n",
      "  mlp_gate_proj_features: 1536\n",
      "  mlp_up_proj_features: 1536\n",
      "  mlp_down_proj_features: 576\n",
      "  hidden_act: silu\n",
      "  use_silu: True\n",
      "  pretrained_model_name_or_path: HuggingFaceTB/SmolLM2-135M\n",
      "  torch_dtype: float16\n",
      "  device_map: auto\n",
      "  trust_remote_code: False\n",
      "\n",
      "[Tokenizer Configuration]\n",
      "  tokenizer_name_or_path: HuggingFaceTB/SmolLM2-135M\n",
      "  use_fast: True\n",
      "  padding_side: right\n",
      "  truncation_side: right\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Load configuration from YAML file\n",
    "# All imports (yaml, dataclass, field, Optional) are in the first cell\n",
    "\n",
    "# Load defaults from config.yaml first\n",
    "def load_yaml_defaults(yaml_path: str = \"config.yaml\"):\n",
    "    \"\"\"Load default values from YAML file\"\"\"\n",
    "    with open(yaml_path, 'r') as f:\n",
    "        return yaml.safe_load(f) or {}\n",
    "\n",
    "# Load YAML to get defaults\n",
    "_yaml_defaults = load_yaml_defaults(\"config.yaml\")\n",
    "_model_defaults = _yaml_defaults.get(\"model\", {})\n",
    "_tokenizer_defaults = _yaml_defaults.get(\"tokenizer\", {})\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"Model architecture and initialization parameters\"\"\"\n",
    "    model_type: str = _model_defaults.get(\"model_type\", \"llama\")\n",
    "    vocab_size: int = _model_defaults.get(\"vocab_size\", 49152)\n",
    "    hidden_size: int = _model_defaults.get(\"hidden_size\", 576)\n",
    "    intermediate_size: int = _model_defaults.get(\"intermediate_size\", 1536)\n",
    "    num_hidden_layers: int = _model_defaults.get(\"num_hidden_layers\", 30)\n",
    "    num_attention_heads: int = _model_defaults.get(\"num_attention_heads\", 18)\n",
    "    num_key_value_heads: int = _model_defaults.get(\"num_key_value_heads\", 6)\n",
    "    max_position_embeddings: int = _model_defaults.get(\"max_position_embeddings\", 8192)\n",
    "    rms_norm_eps: float = _model_defaults.get(\"rms_norm_eps\", 1e-05)\n",
    "    use_cache: bool = _model_defaults.get(\"use_cache\", True)\n",
    "    pad_token_id: Optional[int] = _model_defaults.get(\"pad_token_id\", None)\n",
    "    bos_token_id: int = _model_defaults.get(\"bos_token_id\", 1)\n",
    "    eos_token_id: int = _model_defaults.get(\"eos_token_id\", 2)\n",
    "    \n",
    "    # MLP Configuration\n",
    "    mlp_bias: bool = _model_defaults.get(\"mlp_bias\", False)\n",
    "    mlp_gate_proj_features: int = _model_defaults.get(\"mlp_gate_proj_features\", 1536)\n",
    "    mlp_up_proj_features: int = _model_defaults.get(\"mlp_up_proj_features\", 1536)\n",
    "    mlp_down_proj_features: int = _model_defaults.get(\"mlp_down_proj_features\", 576)\n",
    "    \n",
    "    # Activation Function\n",
    "    hidden_act: str = _model_defaults.get(\"hidden_act\", \"silu\")\n",
    "    use_silu: bool = _model_defaults.get(\"use_silu\", True)\n",
    "    \n",
    "    # Model initialization\n",
    "    pretrained_model_name_or_path: str = _model_defaults.get(\"pretrained_model_name_or_path\", \"HuggingFaceTB/SmolLM2-135M\")\n",
    "    torch_dtype: str = _model_defaults.get(\"torch_dtype\", \"float16\")\n",
    "    device_map: str = _model_defaults.get(\"device_map\", \"auto\")\n",
    "    trust_remote_code: bool = _model_defaults.get(\"trust_remote_code\", False)\n",
    "\n",
    "@dataclass\n",
    "class TokenizerConfig:\n",
    "    \"\"\"Tokenizer configuration parameters\"\"\"\n",
    "    tokenizer_name_or_path: str = _tokenizer_defaults.get(\"tokenizer_name_or_path\", \"HuggingFaceTB/SmolLM2-135M\")\n",
    "    use_fast: bool = _tokenizer_defaults.get(\"use_fast\", True)\n",
    "    padding_side: str = _tokenizer_defaults.get(\"padding_side\", \"right\")\n",
    "    truncation_side: str = _tokenizer_defaults.get(\"truncation_side\", \"right\")\n",
    "\n",
    "@dataclass\n",
    "class SmolLMV2Config:\n",
    "    \"\"\"Main configuration class\"\"\"\n",
    "    model: ModelConfig = field(default_factory=ModelConfig)\n",
    "    tokenizer: TokenizerConfig = field(default_factory=TokenizerConfig)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_yaml(cls, yaml_path: str = \"config.yaml\"):\n",
    "        \"\"\"Load configuration from YAML file\"\"\"\n",
    "        with open(yaml_path, 'r') as f:\n",
    "            config_dict = yaml.safe_load(f) or {}\n",
    "        \n",
    "        # Create configs with defaults from YAML, then update from YAML\n",
    "        model_config = ModelConfig()\n",
    "        if \"model\" in config_dict:\n",
    "            for key, value in config_dict[\"model\"].items():\n",
    "                if hasattr(model_config, key):\n",
    "                    setattr(model_config, key, value)\n",
    "        \n",
    "        tokenizer_config = TokenizerConfig()\n",
    "        if \"tokenizer\" in config_dict:\n",
    "            for key, value in config_dict[\"tokenizer\"].items():\n",
    "                if hasattr(tokenizer_config, key):\n",
    "                    setattr(tokenizer_config, key, value)\n",
    "        \n",
    "        return cls(model=model_config, tokenizer=tokenizer_config)\n",
    "\n",
    "# Load configuration\n",
    "cfg = SmolLMV2Config.from_yaml(\"config.yaml\")\n",
    "\n",
    "# Display loaded configuration\n",
    "print(\"=\" * 60)\n",
    "print(\"Loaded SmolLMV2 Configuration\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n[Model Configuration]\")\n",
    "for key, value in cfg.model.__dict__.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "print(\"\\n[Tokenizer Configuration]\")\n",
    "for key, value in cfg.tokenizer.__dict__.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37bbc7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.num_key_value_heads = config.num_key_value_heads\n",
    "        self.head_dim = config.n_embd // config.n_head\n",
    "        \n",
    "        # Separate projections for q, k, v, o (LLaMA style)\n",
    "        # q_proj: all heads, k_proj and v_proj: only num_key_value_heads (GQA)\n",
    "        self.q_proj = nn.Linear(config.n_embd, config.n_embd, bias=False)\n",
    "        self.k_proj = nn.Linear(config.n_embd, config.num_key_value_heads * self.head_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(config.n_embd, config.num_key_value_heads * self.head_dim, bias=False)\n",
    "        self.o_proj = nn.Linear(config.n_embd, config.n_embd, bias=False)\n",
    "        \n",
    "        # Rotary positional embedding (RoPE) - applied to q and k in forward\n",
    "        self.rotary_emb = RotaryEmbedding(self.head_dim)\n",
    "        \n",
    "        # Causal mask\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size)).view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()  # batch size, sequence length, embedding dimensionality\n",
    "        \n",
    "        # Project to q, k, v\n",
    "        q = self.q_proj(x)  # (B, T, n_embd)\n",
    "        k = self.k_proj(x)  # (B, T, num_key_value_heads * head_dim)\n",
    "        v = self.v_proj(x)  # (B, T, num_key_value_heads * head_dim)\n",
    "        \n",
    "        # Reshape for multi-head attention\n",
    "        q = q.view(B, T, self.n_head, self.head_dim).transpose(1, 2)  # (B, n_head, T, head_dim)\n",
    "        k = k.view(B, T, self.num_key_value_heads, self.head_dim).transpose(1, 2)  # (B, num_key_value_heads, T, head_dim)\n",
    "        v = v.view(B, T, self.num_key_value_heads, self.head_dim).transpose(1, 2)  # (B, num_key_value_heads, T, head_dim)\n",
    "        \n",
    "        # Apply rotary positional embeddings to q and k (RoPE)\n",
    "        q = self.rotary_emb.rotate_queries_or_keys(q)\n",
    "        k = self.rotary_emb.rotate_queries_or_keys(k)\n",
    "        \n",
    "        # Repeat k and v for GQA (if num_key_value_heads < n_head)\n",
    "        if self.num_key_value_heads != self.n_head:\n",
    "            # Repeat k and v to match number of query heads\n",
    "            repeat_factor = self.n_head // self.num_key_value_heads\n",
    "            k = k.repeat_interleave(repeat_factor, dim=1)  # (B, n_head, T, head_dim)\n",
    "            v = v.repeat_interleave(repeat_factor, dim=1)  # (B, n_head, T, head_dim)\n",
    "        \n",
    "        # Causal self-attention: (B, n_head, T, head_dim) @ (B, n_head, head_dim, T) -> (B, n_head, T, T)\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(self.head_dim))\n",
    "        att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        y = att @ v  # (B, n_head, T, T) @ (B, n_head, T, head_dim) -> (B, n_head, T, head_dim)\n",
    "\n",
    "        # Re-assemble all head outputs: (B, n_head, T, head_dim) -> (B, T, C)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        \n",
    "        # Output projection\n",
    "        y = self.o_proj(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b351179",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaMLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.gate_proj = nn.Linear(config.hidden_size, config.intermediate_size, bias=config.mlp_bias)\n",
    "        self.up_proj = nn.Linear(config.hidden_size, config.intermediate_size, bias=config.mlp_bias)\n",
    "        self.down_proj = nn.Linear(config.intermediate_size, config.hidden_size, bias=config.mlp_bias)\n",
    "        \n",
    "        # Use activation from config.yaml\n",
    "        if config.hidden_act == \"silu\":\n",
    "            self.act_fn = nn.SiLU()\n",
    "        elif config.hidden_act == \"gelu\":\n",
    "            self.act_fn = nn.GELU()\n",
    "        elif config.hidden_act == \"relu\":\n",
    "            self.act_fn = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # LLaMA MLP: SiLU(gate_proj(x)) * up_proj(x)\n",
    "        gate = self.act_fn(self.gate_proj(x))\n",
    "        up = self.up_proj(x)\n",
    "        return self.down_proj(gate * up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "719419cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaRMSNorm(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(config.hidden_size))\n",
    "        # Ensure rms_norm_eps is a float (YAML might load it as string)\n",
    "        self.variance_epsilon = float(config.rms_norm_eps)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # LLaMA RMSNorm: sqrt(1/n * sum(x^2)) * weight\n",
    "        return self.weight * x / torch.sqrt(torch.mean(x * x, dim=-1, keepdim=True) + self.variance_epsilon)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"LlamaRMSNorm(hidden_size={self.weight.shape[0]}, eps={self.variance_epsilon})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73edd76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaDecoderLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LlamaRMSNorm(config)\n",
    "        self.attn = LlamaAttention(config)\n",
    "        self.ln_2 = LlamaRMSNorm(config)\n",
    "        self.mlp = LlamaMLP(config) \n",
    "\n",
    "    def forward(self, x):\n",
    "        # LLaMA decoder layer: ln1(x) + attn(ln1(x)) + ln2(x) + mlp(ln2(x))\n",
    "        residual = x\n",
    "        x = self.ln_1(x)\n",
    "        x = x + self.attn(x)\n",
    "        x = self.ln_2(x)\n",
    "        x = x + self.mlp(x)\n",
    "        x = x + residual\n",
    "        return x\n",
    "\n",
    "class LlamaDecoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.layers = nn.ModuleList([LlamaDecoderLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "class LlamaModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embed_tokens = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "        self.pos_embed = RotaryEmbedding(config.n_embd)\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.lm_head.weight = self.embed_tokens.weight\n",
    "        self.ln_f = LlamaRMSNorm(config)\n",
    "        self.transformer = LlamaDecoder(config)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean = 0.0, std = 0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std = 0.02)\n",
    "            \n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx is of shape (B, T)\n",
    "        B, T = idx.size()\n",
    "        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
    "        # forward the token embeddings (RoPE is applied in attention, not here)\n",
    "        x = self.embed_tokens(idx)  # token embeddings of shape (B, T, n_embd)\n",
    "        # forward the blocks of the transformer\n",
    "        x = self.transformer(x)\n",
    "        # forward the final layernorm and the classifier\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        return logits, loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def generate(self, input_ids, max_new_tokens):\n",
    "        \"\"\"\n",
    "        Generate new tokens up to max_new_tokens.\n",
    "        \n",
    "        Args:\n",
    "            input_ids: Input token IDs, shape (batch_size, seq_len)\n",
    "            max_new_tokens: Maximum number of new tokens to generate\n",
    "            \n",
    "        Returns:\n",
    "            Generated token IDs, shape (batch_size, seq_len + max_new_tokens)\n",
    "        \"\"\"\n",
    "        self.eval()  # Set to evaluation mode\n",
    "        input_ids = input_ids.to(self.config.device)\n",
    "        \n",
    "        # Start with the input sequence\n",
    "        generated_ids = input_ids.clone()\n",
    "        \n",
    "        # Generate tokens one at a time\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Get embeddings for current sequence\n",
    "            x = self.embed_tokens(generated_ids)\n",
    "            # Note: Rotary embeddings (RoPE) are applied in attention, not here\n",
    "            \n",
    "            # Pass through transformer\n",
    "            x = self.transformer(x)\n",
    "            x = self.ln_f(x)\n",
    "            \n",
    "            # Get logits for all positions\n",
    "            logits = self.lm_head(x)\n",
    "            \n",
    "            # Only use the last token's logits to predict next token\n",
    "            logits = logits[:, -1, :]  # Shape: (batch_size, vocab_size)\n",
    "            \n",
    "            # Greedy decoding: pick the token with highest probability\n",
    "            next_token = torch.argmax(logits, dim=-1, keepdim=True)  # Shape: (batch_size, 1)\n",
    "            \n",
    "            # Append the new token to the sequence\n",
    "            generated_ids = torch.cat([generated_ids, next_token], dim=1)\n",
    "        \n",
    "        # Return all generated tokens (input + newly generated)\n",
    "        return generated_ids\n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "760ebb27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STEP-BY-STEP EXPLANATION OF GENERATE METHOD\n",
      "================================================================================\n",
      "\n",
      "STEP 1: Input Preparation\n",
      "--------------------------------------------------------------------------------\n",
      "input_ids shape: torch.Size([1, 3])\n",
      "input_ids: tensor([[1234, 5678, 9012]])\n",
      "  - batch_size = 1 (one sequence)\n",
      "  - sequence_length = 3 (three tokens: 'The', 'weather', 'is')\n",
      "\n",
      "STEP 2: Embedding Layer (embed_tokens)\n",
      "--------------------------------------------------------------------------------\n",
      "After embedding, x shape: torch.Size([1, 3, 576])\n",
      "  - batch_size = 1\n",
      "  - sequence_length = 3\n",
      "  - hidden_size = 576\n",
      "  - Each token is now a 576-dimensional vector\n",
      "\n",
      "STEP 3: Positional Embedding (pos_embed)\n",
      "--------------------------------------------------------------------------------\n",
      "After positional embedding, x shape: torch.Size([1, 3, 576])\n",
      "  - Still same shape: (batch, seq_len, hidden_size)\n",
      "  - Now each token has both semantic and positional information\n",
      "\n",
      "STEP 4: Transformer Layers (transformer)\n",
      "--------------------------------------------------------------------------------\n",
      "After transformer, x shape: torch.Size([1, 3, 576])\n",
      "  - Still same shape: (batch, seq_len, hidden_size)\n",
      "  - Each position now has contextualized representations\n",
      "  - Token 'is' now knows about 'The' and 'weather'\n",
      "\n",
      "STEP 5: Final Layer Normalization (ln_f)\n",
      "--------------------------------------------------------------------------------\n",
      "After layer norm, x shape: torch.Size([1, 3, 576])\n",
      "  - Still same shape: (batch, seq_len, hidden_size)\n",
      "  - Normalized for stability\n",
      "\n",
      "STEP 6: Language Model Head (lm_head)\n",
      "--------------------------------------------------------------------------------\n",
      "After lm_head, logits shape: torch.Size([1, 3, 49152])\n",
      "  - batch_size = 1\n",
      "  - sequence_length = 3\n",
      "  - vocab_size = 49152\n",
      "\n",
      "What this means:\n",
      "  - For EACH position in the sequence, we have predictions for ALL tokens\n",
      "  - Position 0 ('The'): 49152 logits (probabilities for next token)\n",
      "  - Position 1 ('weather'): 49152 logits (probabilities for next token)\n",
      "  - Position 2 ('is'): 49152 logits (probabilities for next token)\n",
      "\n",
      "Example logits for position 2 ('is'):\n",
      "  First 10 logits: tensor([-1.8781,  0.1518, -1.0680,  1.6972,  0.5825,  0.5398,  1.3581, -1.3130,\n",
      "         0.5442, -0.7020])\n",
      "  - These are raw scores (not probabilities yet)\n",
      "  - Higher values = more likely token\n",
      "\n",
      "STEP 7: Extract Last Token Logits - logits[:, -1, :]\n",
      "--------------------------------------------------------------------------------\n",
      "This is the line you asked about!\n",
      "\n",
      "BEFORE slicing:\n",
      "  logits.shape = torch.Size([1, 3, 49152])  # (batch, seq_len, vocab_size)\n",
      "  logits = 1 batch  3 positions  49152 vocab tokens\n",
      "\n",
      "Slicing operation: logits[:, -1, :]\n",
      "  - ':' in first position = keep ALL batches\n",
      "  - '-1' in second position = take LAST position (last token)\n",
      "  - ':' in third position = keep ALL vocabulary tokens\n",
      "\n",
      "AFTER slicing:\n",
      "  logits_last.shape = torch.Size([1, 49152])  # (batch, vocab_size)\n",
      "  logits_last = 1 batch  49152 vocab tokens\n",
      "\n",
      "What we kept:\n",
      "  - We ONLY kept the logits from the LAST token position ('is')\n",
      "  - We DISCARDED logits from positions 0 ('The') and 1 ('weather')\n",
      "  - Why? Because we only need the LAST token to predict the NEXT token!\n",
      "\n",
      "STEP 8: Get Next Token (torch.argmax)\n",
      "--------------------------------------------------------------------------------\n",
      "next_token shape: torch.Size([1, 1])\n",
      "next_token: tensor([[28949]])\n",
      "\n",
      "What happened:\n",
      "  - torch.argmax finds the index with the highest value\n",
      "  - dim=-1 means we search along the vocabulary dimension\n",
      "  - keepdim=True keeps the dimension for concatenation\n",
      "  - Result: The token ID of the most likely next token\n",
      "\n",
      "================================================================================\n",
      "VISUAL SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Input sequence: ['The', 'weather', 'is']\n",
      "\n",
      "After forward pass:\n",
      "  logits shape: (1, 3, 49152)\n",
      "  \n",
      "   Position 0 ('The'):    49152 logits \n",
      "   Position 1 ('weather'): 49152 logits \n",
      "   Position 2 ('is'):     49152 logits   We need THIS one!\n",
      "  \n",
      "\n",
      "After logits[:, -1, :]:\n",
      "  logits shape: (1, 49152)\n",
      "  \n",
      "   Position 2 ('is'):    Only the last token's logits\n",
      "     49152 logits      \n",
      "  \n",
      "\n",
      "After argmax:\n",
      "  next_token: [token_id]   The most likely next token\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Detailed Step-by-Step Explanation of generate() method\n",
    "import torch\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STEP-BY-STEP EXPLANATION OF GENERATE METHOD\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: Input Preparation\n",
    "# ============================================================================\n",
    "print(\"STEP 1: Input Preparation\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Example: We want to generate text starting from \"The weather is\"\n",
    "# After tokenization, this becomes token IDs\n",
    "input_ids = torch.tensor([[1234, 5678, 9012]])  # Shape: (batch_size=1, sequence_length=3)\n",
    "print(f\"input_ids shape: {input_ids.shape}\")\n",
    "print(f\"input_ids: {input_ids}\")\n",
    "print(\"  - batch_size = 1 (one sequence)\")\n",
    "print(\"  - sequence_length = 3 (three tokens: 'The', 'weather', 'is')\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: Embedding Layer\n",
    "# ============================================================================\n",
    "print(\"STEP 2: Embedding Layer (embed_tokens)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Each token ID is converted to a dense vector\n",
    "# vocab_size = 49152, hidden_size = 576\n",
    "# embed_tokens converts token IDs to embeddings\n",
    "batch_size, seq_len = input_ids.shape\n",
    "hidden_size = 576\n",
    "\n",
    "# Simulate embedding: (batch, seq_len) -> (batch, seq_len, hidden_size)\n",
    "x = torch.randn(batch_size, seq_len, hidden_size)  # Simulated embeddings\n",
    "print(f\"After embedding, x shape: {x.shape}\")\n",
    "print(f\"  - batch_size = {batch_size}\")\n",
    "print(f\"  - sequence_length = {seq_len}\")\n",
    "print(f\"  - hidden_size = {hidden_size}\")\n",
    "print(\"  - Each token is now a 576-dimensional vector\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3: Positional Embedding\n",
    "# ============================================================================\n",
    "print(\"STEP 3: Positional Embedding (pos_embed)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Add positional information to embeddings\n",
    "pos_embed = torch.randn(batch_size, seq_len, hidden_size)  # Simulated\n",
    "x = x + pos_embed\n",
    "print(f\"After positional embedding, x shape: {x.shape}\")\n",
    "print(\"  - Still same shape: (batch, seq_len, hidden_size)\")\n",
    "print(\"  - Now each token has both semantic and positional information\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 4: Transformer Layers\n",
    "# ============================================================================\n",
    "print(\"STEP 4: Transformer Layers (transformer)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Pass through all transformer decoder layers\n",
    "# Each layer processes the entire sequence\n",
    "x = torch.randn(batch_size, seq_len, hidden_size)  # Simulated after transformer\n",
    "print(f\"After transformer, x shape: {x.shape}\")\n",
    "print(\"  - Still same shape: (batch, seq_len, hidden_size)\")\n",
    "print(\"  - Each position now has contextualized representations\")\n",
    "print(\"  - Token 'is' now knows about 'The' and 'weather'\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 5: Final Layer Norm\n",
    "# ============================================================================\n",
    "print(\"STEP 5: Final Layer Normalization (ln_f)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "x = torch.randn(batch_size, seq_len, hidden_size)  # Simulated after norm\n",
    "print(f\"After layer norm, x shape: {x.shape}\")\n",
    "print(\"  - Still same shape: (batch, seq_len, hidden_size)\")\n",
    "print(\"  - Normalized for stability\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 6: Language Model Head (lm_head)\n",
    "# ============================================================================\n",
    "print(\"STEP 6: Language Model Head (lm_head)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Convert hidden states to vocabulary logits\n",
    "vocab_size = 49152\n",
    "logits = torch.randn(batch_size, seq_len, vocab_size)  # Simulated logits\n",
    "print(f\"After lm_head, logits shape: {logits.shape}\")\n",
    "print(f\"  - batch_size = {batch_size}\")\n",
    "print(f\"  - sequence_length = {seq_len}\")\n",
    "print(f\"  - vocab_size = {vocab_size}\")\n",
    "print()\n",
    "print(\"What this means:\")\n",
    "print(\"  - For EACH position in the sequence, we have predictions for ALL tokens\")\n",
    "print(\"  - Position 0 ('The'): 49152 logits (probabilities for next token)\")\n",
    "print(\"  - Position 1 ('weather'): 49152 logits (probabilities for next token)\")\n",
    "print(\"  - Position 2 ('is'): 49152 logits (probabilities for next token)\")\n",
    "print()\n",
    "print(\"Example logits for position 2 ('is'):\")\n",
    "example_logits = logits[0, 2, :10]  # First 10 tokens\n",
    "print(f\"  First 10 logits: {example_logits}\")\n",
    "print(\"  - These are raw scores (not probabilities yet)\")\n",
    "print(\"  - Higher values = more likely token\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 7: Extract Last Token Logits (THE KEY LINE!)\n",
    "# ============================================================================\n",
    "print(\"STEP 7: Extract Last Token Logits - logits[:, -1, :]\")\n",
    "print(\"-\" * 80)\n",
    "print(\"This is the line you asked about!\")\n",
    "print()\n",
    "\n",
    "print(\"BEFORE slicing:\")\n",
    "print(f\"  logits.shape = {logits.shape}  # (batch, seq_len, vocab_size)\")\n",
    "print(f\"  logits = {logits.shape[0]} batch  {logits.shape[1]} positions  {logits.shape[2]} vocab tokens\")\n",
    "print()\n",
    "\n",
    "print(\"Slicing operation: logits[:, -1, :]\")\n",
    "print(\"  - ':' in first position = keep ALL batches\")\n",
    "print(\"  - '-1' in second position = take LAST position (last token)\")\n",
    "print(\"  - ':' in third position = keep ALL vocabulary tokens\")\n",
    "print()\n",
    "\n",
    "logits_last = logits[:, -1, :]\n",
    "print(\"AFTER slicing:\")\n",
    "print(f\"  logits_last.shape = {logits_last.shape}  # (batch, vocab_size)\")\n",
    "print(f\"  logits_last = {logits_last.shape[0]} batch  {logits_last.shape[1]} vocab tokens\")\n",
    "print()\n",
    "\n",
    "print(\"What we kept:\")\n",
    "print(\"  - We ONLY kept the logits from the LAST token position ('is')\")\n",
    "print(\"  - We DISCARDED logits from positions 0 ('The') and 1 ('weather')\")\n",
    "print(\"  - Why? Because we only need the LAST token to predict the NEXT token!\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 8: Get Next Token (Greedy Decoding)\n",
    "# ============================================================================\n",
    "print(\"STEP 8: Get Next Token (torch.argmax)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Find the token with highest logit (greedy decoding)\n",
    "next_token = torch.argmax(logits_last, dim=-1, keepdim=True)\n",
    "print(f\"next_token shape: {next_token.shape}\")\n",
    "print(f\"next_token: {next_token}\")\n",
    "print()\n",
    "print(\"What happened:\")\n",
    "print(\"  - torch.argmax finds the index with the highest value\")\n",
    "print(\"  - dim=-1 means we search along the vocabulary dimension\")\n",
    "print(\"  - keepdim=True keeps the dimension for concatenation\")\n",
    "print(\"  - Result: The token ID of the most likely next token\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# VISUAL SUMMARY\n",
    "# ============================================================================\n",
    "print(\"=\" * 80)\n",
    "print(\"VISUAL SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "print(\"Input sequence: ['The', 'weather', 'is']\")\n",
    "print()\n",
    "print(\"After forward pass:\")\n",
    "print(\"  logits shape: (1, 3, 49152)\")\n",
    "print(\"  \")\n",
    "print(\"   Position 0 ('The'):    49152 logits \")\n",
    "print(\"   Position 1 ('weather'): 49152 logits \")\n",
    "print(\"   Position 2 ('is'):     49152 logits   We need THIS one!\")\n",
    "print(\"  \")\n",
    "print()\n",
    "print(\"After logits[:, -1, :]:\")\n",
    "print(\"  logits shape: (1, 49152)\")\n",
    "print(\"  \")\n",
    "print(\"   Position 2 ('is'):    Only the last token's logits\")\n",
    "print(\"     49152 logits      \")\n",
    "print(\"  \")\n",
    "print()\n",
    "print(\"After argmax:\")\n",
    "print(\"  next_token: [token_id]   The most likely next token\")\n",
    "print()\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c85fee67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 11.2363\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Use tokenizer from config.yaml (loaded in cell 2 or 4)\n",
    "# If not available, load it from config\n",
    "if 'tokenizer' not in globals():\n",
    "    with open(\"config.yaml\", 'r') as f:\n",
    "        config_dict = yaml.safe_load(f)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        config_dict[\"tokenizer\"][\"tokenizer_name_or_path\"],\n",
    "        use_fast=config_dict[\"tokenizer\"][\"use_fast\"]\n",
    "    )\n",
    "\n",
    "# Read and tokenize input text\n",
    "with open('input.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Limit text and tokenize using tokenizer from config.yaml\n",
    "text = text[:1000]\n",
    "tokens = tokenizer.encode(text, add_special_tokens=False)  # Returns list of token IDs\n",
    "\n",
    "# Setup batch and sequence parameters\n",
    "B, T = 4, 32  # batch_size, sequence_length\n",
    "\n",
    "# Create input/target pairs from tokens\n",
    "buf = torch.tensor(tokens[:B*T + 1])  # Take B*T+1 tokens for input/target split\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "buf = buf.to(device)\n",
    "\n",
    "# Split into input (x) and target (y) - y is x shifted by 1\n",
    "x = buf[:-1].view(B, T)  # Shape: (B, T)\n",
    "y = buf[1:].view(B, T)   # Shape: (B, T)\n",
    "\n",
    "# Prepare model config from config.yaml\n",
    "# Add compatibility attributes for LlamaModel/LlamaAttention\n",
    "model_config = cfg.model\n",
    "model_config.n_embd = model_config.hidden_size\n",
    "model_config.n_head = model_config.num_attention_heads\n",
    "model_config.block_size = model_config.max_position_embeddings\n",
    "model_config.device = device\n",
    "model_config.num_key_value_heads = model_config.num_key_value_heads\n",
    "\n",
    "# Create LlamaModel using config from config.yaml\n",
    "model = LlamaModel(model_config)\n",
    "model.to(device)\n",
    "\n",
    "# Forward pass and compute loss\n",
    "logits, loss = model(x, y)\n",
    "print(f\"Loss: {loss.item():.4f}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dab24d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected loss for random prediction (vocab_size=49152): 10.8027\n",
      "This is the theoretical maximum loss when predicting uniformly at random.\n"
     ]
    }
   ],
   "source": [
    "# Calculate expected loss for random prediction (uniform distribution)\n",
    "# This represents the worst-case scenario: predicting uniformly at random\n",
    "# Formula: -log(1/vocab_size) = log(vocab_size)\n",
    "vocab_size = cfg.model.vocab_size\n",
    "random_loss = -torch.log(torch.tensor(1.0 / vocab_size))\n",
    "print(f\"Expected loss for random prediction (vocab_size={vocab_size}): {random_loss.item():.4f}\")\n",
    "print(f\"This is the theoretical maximum loss when predicting uniformly at random.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a90961e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaModel(\n",
       "  (embed_tokens): Embedding(49152, 576)\n",
       "  (pos_embed): RotaryEmbedding()\n",
       "  (lm_head): Linear(in_features=576, out_features=49152, bias=False)\n",
       "  (ln_f): LlamaRMSNorm(hidden_size=576, eps=1e-05)\n",
       "  (transformer): LlamaDecoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-29): 30 x LlamaDecoderLayer(\n",
       "        (ln_1): LlamaRMSNorm(hidden_size=576, eps=1e-05)\n",
       "        (attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
       "          (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
       "          (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
       "          (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (ln_2): LlamaRMSNorm(hidden_size=576, eps=1e-05)\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
       "          (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
       "          (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Explaining the model \n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c5bffa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaModel(\n",
      "  (embed_tokens): Embedding(49152, 576)\n",
      "  (pos_embed): RotaryEmbedding()\n",
      "  (lm_head): Linear(in_features=576, out_features=49152, bias=False)\n",
      "  (ln_f): LlamaRMSNorm(hidden_size=576, eps=1e-05)\n",
      "  (transformer): LlamaDecoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-29): 30 x LlamaDecoderLayer(\n",
      "        (ln_1): LlamaRMSNorm(hidden_size=576, eps=1e-05)\n",
      "        (attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "          (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "          (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "          (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "          (rotary_emb): RotaryEmbedding()\n",
      "        )\n",
      "        (ln_2): LlamaRMSNorm(hidden_size=576, eps=1e-05)\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "          (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "          (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "134515008\n",
      "+--------------------------------------------+------------+\n",
      "|                  Modules                   | Parameters |\n",
      "+--------------------------------------------+------------+\n",
      "|            embed_tokens.weight             |  28311552  |\n",
      "|                ln_f.weight                 |    576     |\n",
      "|      transformer.layers.0.ln_1.weight      |    576     |\n",
      "|  transformer.layers.0.attn.q_proj.weight   |   331776   |\n",
      "|  transformer.layers.0.attn.k_proj.weight   |   110592   |\n",
      "|  transformer.layers.0.attn.v_proj.weight   |   110592   |\n",
      "|  transformer.layers.0.attn.o_proj.weight   |   331776   |\n",
      "|      transformer.layers.0.ln_2.weight      |    576     |\n",
      "| transformer.layers.0.mlp.gate_proj.weight  |   884736   |\n",
      "|  transformer.layers.0.mlp.up_proj.weight   |   884736   |\n",
      "| transformer.layers.0.mlp.down_proj.weight  |   884736   |\n",
      "|      transformer.layers.1.ln_1.weight      |    576     |\n",
      "|  transformer.layers.1.attn.q_proj.weight   |   331776   |\n",
      "|  transformer.layers.1.attn.k_proj.weight   |   110592   |\n",
      "|  transformer.layers.1.attn.v_proj.weight   |   110592   |\n",
      "|  transformer.layers.1.attn.o_proj.weight   |   331776   |\n",
      "|      transformer.layers.1.ln_2.weight      |    576     |\n",
      "| transformer.layers.1.mlp.gate_proj.weight  |   884736   |\n",
      "|  transformer.layers.1.mlp.up_proj.weight   |   884736   |\n",
      "| transformer.layers.1.mlp.down_proj.weight  |   884736   |\n",
      "|      transformer.layers.2.ln_1.weight      |    576     |\n",
      "|  transformer.layers.2.attn.q_proj.weight   |   331776   |\n",
      "|  transformer.layers.2.attn.k_proj.weight   |   110592   |\n",
      "|  transformer.layers.2.attn.v_proj.weight   |   110592   |\n",
      "|  transformer.layers.2.attn.o_proj.weight   |   331776   |\n",
      "|      transformer.layers.2.ln_2.weight      |    576     |\n",
      "| transformer.layers.2.mlp.gate_proj.weight  |   884736   |\n",
      "|  transformer.layers.2.mlp.up_proj.weight   |   884736   |\n",
      "| transformer.layers.2.mlp.down_proj.weight  |   884736   |\n",
      "|      transformer.layers.3.ln_1.weight      |    576     |\n",
      "|  transformer.layers.3.attn.q_proj.weight   |   331776   |\n",
      "|  transformer.layers.3.attn.k_proj.weight   |   110592   |\n",
      "|  transformer.layers.3.attn.v_proj.weight   |   110592   |\n",
      "|  transformer.layers.3.attn.o_proj.weight   |   331776   |\n",
      "|      transformer.layers.3.ln_2.weight      |    576     |\n",
      "| transformer.layers.3.mlp.gate_proj.weight  |   884736   |\n",
      "|  transformer.layers.3.mlp.up_proj.weight   |   884736   |\n",
      "| transformer.layers.3.mlp.down_proj.weight  |   884736   |\n",
      "|      transformer.layers.4.ln_1.weight      |    576     |\n",
      "|  transformer.layers.4.attn.q_proj.weight   |   331776   |\n",
      "|  transformer.layers.4.attn.k_proj.weight   |   110592   |\n",
      "|  transformer.layers.4.attn.v_proj.weight   |   110592   |\n",
      "|  transformer.layers.4.attn.o_proj.weight   |   331776   |\n",
      "|      transformer.layers.4.ln_2.weight      |    576     |\n",
      "| transformer.layers.4.mlp.gate_proj.weight  |   884736   |\n",
      "|  transformer.layers.4.mlp.up_proj.weight   |   884736   |\n",
      "| transformer.layers.4.mlp.down_proj.weight  |   884736   |\n",
      "|      transformer.layers.5.ln_1.weight      |    576     |\n",
      "|  transformer.layers.5.attn.q_proj.weight   |   331776   |\n",
      "|  transformer.layers.5.attn.k_proj.weight   |   110592   |\n",
      "|  transformer.layers.5.attn.v_proj.weight   |   110592   |\n",
      "|  transformer.layers.5.attn.o_proj.weight   |   331776   |\n",
      "|      transformer.layers.5.ln_2.weight      |    576     |\n",
      "| transformer.layers.5.mlp.gate_proj.weight  |   884736   |\n",
      "|  transformer.layers.5.mlp.up_proj.weight   |   884736   |\n",
      "| transformer.layers.5.mlp.down_proj.weight  |   884736   |\n",
      "|      transformer.layers.6.ln_1.weight      |    576     |\n",
      "|  transformer.layers.6.attn.q_proj.weight   |   331776   |\n",
      "|  transformer.layers.6.attn.k_proj.weight   |   110592   |\n",
      "|  transformer.layers.6.attn.v_proj.weight   |   110592   |\n",
      "|  transformer.layers.6.attn.o_proj.weight   |   331776   |\n",
      "|      transformer.layers.6.ln_2.weight      |    576     |\n",
      "| transformer.layers.6.mlp.gate_proj.weight  |   884736   |\n",
      "|  transformer.layers.6.mlp.up_proj.weight   |   884736   |\n",
      "| transformer.layers.6.mlp.down_proj.weight  |   884736   |\n",
      "|      transformer.layers.7.ln_1.weight      |    576     |\n",
      "|  transformer.layers.7.attn.q_proj.weight   |   331776   |\n",
      "|  transformer.layers.7.attn.k_proj.weight   |   110592   |\n",
      "|  transformer.layers.7.attn.v_proj.weight   |   110592   |\n",
      "|  transformer.layers.7.attn.o_proj.weight   |   331776   |\n",
      "|      transformer.layers.7.ln_2.weight      |    576     |\n",
      "| transformer.layers.7.mlp.gate_proj.weight  |   884736   |\n",
      "|  transformer.layers.7.mlp.up_proj.weight   |   884736   |\n",
      "| transformer.layers.7.mlp.down_proj.weight  |   884736   |\n",
      "|      transformer.layers.8.ln_1.weight      |    576     |\n",
      "|  transformer.layers.8.attn.q_proj.weight   |   331776   |\n",
      "|  transformer.layers.8.attn.k_proj.weight   |   110592   |\n",
      "|  transformer.layers.8.attn.v_proj.weight   |   110592   |\n",
      "|  transformer.layers.8.attn.o_proj.weight   |   331776   |\n",
      "|      transformer.layers.8.ln_2.weight      |    576     |\n",
      "| transformer.layers.8.mlp.gate_proj.weight  |   884736   |\n",
      "|  transformer.layers.8.mlp.up_proj.weight   |   884736   |\n",
      "| transformer.layers.8.mlp.down_proj.weight  |   884736   |\n",
      "|      transformer.layers.9.ln_1.weight      |    576     |\n",
      "|  transformer.layers.9.attn.q_proj.weight   |   331776   |\n",
      "|  transformer.layers.9.attn.k_proj.weight   |   110592   |\n",
      "|  transformer.layers.9.attn.v_proj.weight   |   110592   |\n",
      "|  transformer.layers.9.attn.o_proj.weight   |   331776   |\n",
      "|      transformer.layers.9.ln_2.weight      |    576     |\n",
      "| transformer.layers.9.mlp.gate_proj.weight  |   884736   |\n",
      "|  transformer.layers.9.mlp.up_proj.weight   |   884736   |\n",
      "| transformer.layers.9.mlp.down_proj.weight  |   884736   |\n",
      "|     transformer.layers.10.ln_1.weight      |    576     |\n",
      "|  transformer.layers.10.attn.q_proj.weight  |   331776   |\n",
      "|  transformer.layers.10.attn.k_proj.weight  |   110592   |\n",
      "|  transformer.layers.10.attn.v_proj.weight  |   110592   |\n",
      "|  transformer.layers.10.attn.o_proj.weight  |   331776   |\n",
      "|     transformer.layers.10.ln_2.weight      |    576     |\n",
      "| transformer.layers.10.mlp.gate_proj.weight |   884736   |\n",
      "|  transformer.layers.10.mlp.up_proj.weight  |   884736   |\n",
      "| transformer.layers.10.mlp.down_proj.weight |   884736   |\n",
      "|     transformer.layers.11.ln_1.weight      |    576     |\n",
      "|  transformer.layers.11.attn.q_proj.weight  |   331776   |\n",
      "|  transformer.layers.11.attn.k_proj.weight  |   110592   |\n",
      "|  transformer.layers.11.attn.v_proj.weight  |   110592   |\n",
      "|  transformer.layers.11.attn.o_proj.weight  |   331776   |\n",
      "|     transformer.layers.11.ln_2.weight      |    576     |\n",
      "| transformer.layers.11.mlp.gate_proj.weight |   884736   |\n",
      "|  transformer.layers.11.mlp.up_proj.weight  |   884736   |\n",
      "| transformer.layers.11.mlp.down_proj.weight |   884736   |\n",
      "|     transformer.layers.12.ln_1.weight      |    576     |\n",
      "|  transformer.layers.12.attn.q_proj.weight  |   331776   |\n",
      "|  transformer.layers.12.attn.k_proj.weight  |   110592   |\n",
      "|  transformer.layers.12.attn.v_proj.weight  |   110592   |\n",
      "|  transformer.layers.12.attn.o_proj.weight  |   331776   |\n",
      "|     transformer.layers.12.ln_2.weight      |    576     |\n",
      "| transformer.layers.12.mlp.gate_proj.weight |   884736   |\n",
      "|  transformer.layers.12.mlp.up_proj.weight  |   884736   |\n",
      "| transformer.layers.12.mlp.down_proj.weight |   884736   |\n",
      "|     transformer.layers.13.ln_1.weight      |    576     |\n",
      "|  transformer.layers.13.attn.q_proj.weight  |   331776   |\n",
      "|  transformer.layers.13.attn.k_proj.weight  |   110592   |\n",
      "|  transformer.layers.13.attn.v_proj.weight  |   110592   |\n",
      "|  transformer.layers.13.attn.o_proj.weight  |   331776   |\n",
      "|     transformer.layers.13.ln_2.weight      |    576     |\n",
      "| transformer.layers.13.mlp.gate_proj.weight |   884736   |\n",
      "|  transformer.layers.13.mlp.up_proj.weight  |   884736   |\n",
      "| transformer.layers.13.mlp.down_proj.weight |   884736   |\n",
      "|     transformer.layers.14.ln_1.weight      |    576     |\n",
      "|  transformer.layers.14.attn.q_proj.weight  |   331776   |\n",
      "|  transformer.layers.14.attn.k_proj.weight  |   110592   |\n",
      "|  transformer.layers.14.attn.v_proj.weight  |   110592   |\n",
      "|  transformer.layers.14.attn.o_proj.weight  |   331776   |\n",
      "|     transformer.layers.14.ln_2.weight      |    576     |\n",
      "| transformer.layers.14.mlp.gate_proj.weight |   884736   |\n",
      "|  transformer.layers.14.mlp.up_proj.weight  |   884736   |\n",
      "| transformer.layers.14.mlp.down_proj.weight |   884736   |\n",
      "|     transformer.layers.15.ln_1.weight      |    576     |\n",
      "|  transformer.layers.15.attn.q_proj.weight  |   331776   |\n",
      "|  transformer.layers.15.attn.k_proj.weight  |   110592   |\n",
      "|  transformer.layers.15.attn.v_proj.weight  |   110592   |\n",
      "|  transformer.layers.15.attn.o_proj.weight  |   331776   |\n",
      "|     transformer.layers.15.ln_2.weight      |    576     |\n",
      "| transformer.layers.15.mlp.gate_proj.weight |   884736   |\n",
      "|  transformer.layers.15.mlp.up_proj.weight  |   884736   |\n",
      "| transformer.layers.15.mlp.down_proj.weight |   884736   |\n",
      "|     transformer.layers.16.ln_1.weight      |    576     |\n",
      "|  transformer.layers.16.attn.q_proj.weight  |   331776   |\n",
      "|  transformer.layers.16.attn.k_proj.weight  |   110592   |\n",
      "|  transformer.layers.16.attn.v_proj.weight  |   110592   |\n",
      "|  transformer.layers.16.attn.o_proj.weight  |   331776   |\n",
      "|     transformer.layers.16.ln_2.weight      |    576     |\n",
      "| transformer.layers.16.mlp.gate_proj.weight |   884736   |\n",
      "|  transformer.layers.16.mlp.up_proj.weight  |   884736   |\n",
      "| transformer.layers.16.mlp.down_proj.weight |   884736   |\n",
      "|     transformer.layers.17.ln_1.weight      |    576     |\n",
      "|  transformer.layers.17.attn.q_proj.weight  |   331776   |\n",
      "|  transformer.layers.17.attn.k_proj.weight  |   110592   |\n",
      "|  transformer.layers.17.attn.v_proj.weight  |   110592   |\n",
      "|  transformer.layers.17.attn.o_proj.weight  |   331776   |\n",
      "|     transformer.layers.17.ln_2.weight      |    576     |\n",
      "| transformer.layers.17.mlp.gate_proj.weight |   884736   |\n",
      "|  transformer.layers.17.mlp.up_proj.weight  |   884736   |\n",
      "| transformer.layers.17.mlp.down_proj.weight |   884736   |\n",
      "|     transformer.layers.18.ln_1.weight      |    576     |\n",
      "|  transformer.layers.18.attn.q_proj.weight  |   331776   |\n",
      "|  transformer.layers.18.attn.k_proj.weight  |   110592   |\n",
      "|  transformer.layers.18.attn.v_proj.weight  |   110592   |\n",
      "|  transformer.layers.18.attn.o_proj.weight  |   331776   |\n",
      "|     transformer.layers.18.ln_2.weight      |    576     |\n",
      "| transformer.layers.18.mlp.gate_proj.weight |   884736   |\n",
      "|  transformer.layers.18.mlp.up_proj.weight  |   884736   |\n",
      "| transformer.layers.18.mlp.down_proj.weight |   884736   |\n",
      "|     transformer.layers.19.ln_1.weight      |    576     |\n",
      "|  transformer.layers.19.attn.q_proj.weight  |   331776   |\n",
      "|  transformer.layers.19.attn.k_proj.weight  |   110592   |\n",
      "|  transformer.layers.19.attn.v_proj.weight  |   110592   |\n",
      "|  transformer.layers.19.attn.o_proj.weight  |   331776   |\n",
      "|     transformer.layers.19.ln_2.weight      |    576     |\n",
      "| transformer.layers.19.mlp.gate_proj.weight |   884736   |\n",
      "|  transformer.layers.19.mlp.up_proj.weight  |   884736   |\n",
      "| transformer.layers.19.mlp.down_proj.weight |   884736   |\n",
      "|     transformer.layers.20.ln_1.weight      |    576     |\n",
      "|  transformer.layers.20.attn.q_proj.weight  |   331776   |\n",
      "|  transformer.layers.20.attn.k_proj.weight  |   110592   |\n",
      "|  transformer.layers.20.attn.v_proj.weight  |   110592   |\n",
      "|  transformer.layers.20.attn.o_proj.weight  |   331776   |\n",
      "|     transformer.layers.20.ln_2.weight      |    576     |\n",
      "| transformer.layers.20.mlp.gate_proj.weight |   884736   |\n",
      "|  transformer.layers.20.mlp.up_proj.weight  |   884736   |\n",
      "| transformer.layers.20.mlp.down_proj.weight |   884736   |\n",
      "|     transformer.layers.21.ln_1.weight      |    576     |\n",
      "|  transformer.layers.21.attn.q_proj.weight  |   331776   |\n",
      "|  transformer.layers.21.attn.k_proj.weight  |   110592   |\n",
      "|  transformer.layers.21.attn.v_proj.weight  |   110592   |\n",
      "|  transformer.layers.21.attn.o_proj.weight  |   331776   |\n",
      "|     transformer.layers.21.ln_2.weight      |    576     |\n",
      "| transformer.layers.21.mlp.gate_proj.weight |   884736   |\n",
      "|  transformer.layers.21.mlp.up_proj.weight  |   884736   |\n",
      "| transformer.layers.21.mlp.down_proj.weight |   884736   |\n",
      "|     transformer.layers.22.ln_1.weight      |    576     |\n",
      "|  transformer.layers.22.attn.q_proj.weight  |   331776   |\n",
      "|  transformer.layers.22.attn.k_proj.weight  |   110592   |\n",
      "|  transformer.layers.22.attn.v_proj.weight  |   110592   |\n",
      "|  transformer.layers.22.attn.o_proj.weight  |   331776   |\n",
      "|     transformer.layers.22.ln_2.weight      |    576     |\n",
      "| transformer.layers.22.mlp.gate_proj.weight |   884736   |\n",
      "|  transformer.layers.22.mlp.up_proj.weight  |   884736   |\n",
      "| transformer.layers.22.mlp.down_proj.weight |   884736   |\n",
      "|     transformer.layers.23.ln_1.weight      |    576     |\n",
      "|  transformer.layers.23.attn.q_proj.weight  |   331776   |\n",
      "|  transformer.layers.23.attn.k_proj.weight  |   110592   |\n",
      "|  transformer.layers.23.attn.v_proj.weight  |   110592   |\n",
      "|  transformer.layers.23.attn.o_proj.weight  |   331776   |\n",
      "|     transformer.layers.23.ln_2.weight      |    576     |\n",
      "| transformer.layers.23.mlp.gate_proj.weight |   884736   |\n",
      "|  transformer.layers.23.mlp.up_proj.weight  |   884736   |\n",
      "| transformer.layers.23.mlp.down_proj.weight |   884736   |\n",
      "|     transformer.layers.24.ln_1.weight      |    576     |\n",
      "|  transformer.layers.24.attn.q_proj.weight  |   331776   |\n",
      "|  transformer.layers.24.attn.k_proj.weight  |   110592   |\n",
      "|  transformer.layers.24.attn.v_proj.weight  |   110592   |\n",
      "|  transformer.layers.24.attn.o_proj.weight  |   331776   |\n",
      "|     transformer.layers.24.ln_2.weight      |    576     |\n",
      "| transformer.layers.24.mlp.gate_proj.weight |   884736   |\n",
      "|  transformer.layers.24.mlp.up_proj.weight  |   884736   |\n",
      "| transformer.layers.24.mlp.down_proj.weight |   884736   |\n",
      "|     transformer.layers.25.ln_1.weight      |    576     |\n",
      "|  transformer.layers.25.attn.q_proj.weight  |   331776   |\n",
      "|  transformer.layers.25.attn.k_proj.weight  |   110592   |\n",
      "|  transformer.layers.25.attn.v_proj.weight  |   110592   |\n",
      "|  transformer.layers.25.attn.o_proj.weight  |   331776   |\n",
      "|     transformer.layers.25.ln_2.weight      |    576     |\n",
      "| transformer.layers.25.mlp.gate_proj.weight |   884736   |\n",
      "|  transformer.layers.25.mlp.up_proj.weight  |   884736   |\n",
      "| transformer.layers.25.mlp.down_proj.weight |   884736   |\n",
      "|     transformer.layers.26.ln_1.weight      |    576     |\n",
      "|  transformer.layers.26.attn.q_proj.weight  |   331776   |\n",
      "|  transformer.layers.26.attn.k_proj.weight  |   110592   |\n",
      "|  transformer.layers.26.attn.v_proj.weight  |   110592   |\n",
      "|  transformer.layers.26.attn.o_proj.weight  |   331776   |\n",
      "|     transformer.layers.26.ln_2.weight      |    576     |\n",
      "| transformer.layers.26.mlp.gate_proj.weight |   884736   |\n",
      "|  transformer.layers.26.mlp.up_proj.weight  |   884736   |\n",
      "| transformer.layers.26.mlp.down_proj.weight |   884736   |\n",
      "|     transformer.layers.27.ln_1.weight      |    576     |\n",
      "|  transformer.layers.27.attn.q_proj.weight  |   331776   |\n",
      "|  transformer.layers.27.attn.k_proj.weight  |   110592   |\n",
      "|  transformer.layers.27.attn.v_proj.weight  |   110592   |\n",
      "|  transformer.layers.27.attn.o_proj.weight  |   331776   |\n",
      "|     transformer.layers.27.ln_2.weight      |    576     |\n",
      "| transformer.layers.27.mlp.gate_proj.weight |   884736   |\n",
      "|  transformer.layers.27.mlp.up_proj.weight  |   884736   |\n",
      "| transformer.layers.27.mlp.down_proj.weight |   884736   |\n",
      "|     transformer.layers.28.ln_1.weight      |    576     |\n",
      "|  transformer.layers.28.attn.q_proj.weight  |   331776   |\n",
      "|  transformer.layers.28.attn.k_proj.weight  |   110592   |\n",
      "|  transformer.layers.28.attn.v_proj.weight  |   110592   |\n",
      "|  transformer.layers.28.attn.o_proj.weight  |   331776   |\n",
      "|     transformer.layers.28.ln_2.weight      |    576     |\n",
      "| transformer.layers.28.mlp.gate_proj.weight |   884736   |\n",
      "|  transformer.layers.28.mlp.up_proj.weight  |   884736   |\n",
      "| transformer.layers.28.mlp.down_proj.weight |   884736   |\n",
      "|     transformer.layers.29.ln_1.weight      |    576     |\n",
      "|  transformer.layers.29.attn.q_proj.weight  |   331776   |\n",
      "|  transformer.layers.29.attn.k_proj.weight  |   110592   |\n",
      "|  transformer.layers.29.attn.v_proj.weight  |   110592   |\n",
      "|  transformer.layers.29.attn.o_proj.weight  |   331776   |\n",
      "|     transformer.layers.29.ln_2.weight      |    576     |\n",
      "| transformer.layers.29.mlp.gate_proj.weight |   884736   |\n",
      "|  transformer.layers.29.mlp.up_proj.weight  |   884736   |\n",
      "| transformer.layers.29.mlp.down_proj.weight |   884736   |\n",
      "+--------------------------------------------+------------+\n",
      "Total Trainable Params: 134515008\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "134515008"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model)\n",
    "print(sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "def count_parameters(model):\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad:\n",
    "            continue\n",
    "        params = parameter.numel()\n",
    "        table.add_row([name, params])\n",
    "        total_params += params\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params\n",
    "    \n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcbb053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (341094 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 341094 tokens\n",
      "1 epoch = 41 batches\n",
      "\n",
      "================================================================================\n",
      "Starting Training with Profiling\n",
      "================================================================================\n",
      "Batch size: 8, Sequence length: 1024, Steps: 10\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Device selection\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set seed for reproducibility\n",
    "torch.manual_seed(1337)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(1337)\n",
    "\n",
    "# Training hyperparameters\n",
    "B = 8  # batch size\n",
    "T = 1024  # sequence length\n",
    "num_steps = 1  # number of training steps\n",
    "learning_rate = 3e-4\n",
    "\n",
    "# DataLoader class (similar to S13.ipynb but using tokenizer from config)\n",
    "class DataLoaderLite:\n",
    "    def __init__(self, B, T, tokenizer):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        # Load tokens from disk and store them in memory\n",
    "        with open('input.txt', 'r') as f:\n",
    "            text = f.read()\n",
    "        \n",
    "        # Tokenize using the tokenizer from config.yaml\n",
    "        tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "        self.tokens = torch.tensor(tokens)\n",
    "        print(f'Loaded {len(self.tokens)} tokens')\n",
    "        print(f'1 epoch = {len(self.tokens) // (B * T)} batches')\n",
    "\n",
    "        # State\n",
    "        self.current_position = 0\n",
    "    \n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "        buf = self.tokens[self.current_position: self.current_position + B * T + 1]\n",
    "        x = (buf[:-1]).view(B, T)  # inputs\n",
    "        y = (buf[1:]).view(B, T)  # targets\n",
    "        # Advance the position in the tensor\n",
    "        self.current_position += B * T\n",
    "        # If loading the next batch would be out of bounds, reset\n",
    "        if self.current_position + (B * T + 1) > len(self.tokens):\n",
    "            self.current_position = 0\n",
    "        return x, y\n",
    "\n",
    "# Ensure tokenizer is available (should be loaded from config.yaml in earlier cells)\n",
    "if 'tokenizer' not in globals():\n",
    "    with open(\"config.yaml\", 'r') as f:\n",
    "        config_dict = yaml.safe_load(f)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        config_dict[\"tokenizer\"][\"tokenizer_name_or_path\"],\n",
    "        use_fast=config_dict[\"tokenizer\"][\"use_fast\"]\n",
    "    )\n",
    "\n",
    "# Create data loader\n",
    "train_loader = DataLoaderLite(B=B, T=T, tokenizer=tokenizer)\n",
    "\n",
    "# Ensure model exists and is on correct device\n",
    "if 'model' not in globals():\n",
    "    # Create model config if needed\n",
    "    model_config = cfg.model\n",
    "    model_config.n_embd = model_config.hidden_size\n",
    "    model_config.n_head = model_config.num_attention_heads\n",
    "    model_config.block_size = model_config.max_position_embeddings\n",
    "    model_config.device = device\n",
    "    model_config.num_key_value_heads = model_config.num_key_value_heads\n",
    "    \n",
    "    # Create LlamaModel\n",
    "    model = LlamaModel(model_config)\n",
    "    model.to(device)\n",
    "else:\n",
    "    model = model.to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop with profiling\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Starting Training with Profiling\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Batch size: {B}, Sequence length: {T}, Steps: {num_steps}\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Setup profiler\n",
    "profiler_activities = [torch.profiler.ProfilerActivity.CPU]\n",
    "if torch.cuda.is_available():\n",
    "    profiler_activities.append(torch.profiler.ProfilerActivity.CUDA)\n",
    "\n",
    "# Create log directory if it doesn't exist\n",
    "os.makedirs('./log/profiler', exist_ok=True)\n",
    "\n",
    "with torch.profiler.profile(\n",
    "    activities=profiler_activities,\n",
    "    schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=2),\n",
    "    on_trace_ready=torch.profiler.tensorboard_trace_handler('./log/profiler'),\n",
    "    record_shapes=True,\n",
    "    profile_memory=True,\n",
    "    with_stack=True\n",
    ") as prof:\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        t0 = time.time()\n",
    "        \n",
    "        # Get batch\n",
    "        x, y = train_loader.next_batch()\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        logits, loss = model(x, y)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Synchronize for accurate timing\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "        \n",
    "        t1 = time.time()\n",
    "        dt = (t1 - t0) * 1000  # milliseconds\n",
    "        tokens_per_sec = (train_loader.B * train_loader.T) / (t1 - t0)\n",
    "        \n",
    "        # Step profiler\n",
    "        prof.step()\n",
    "        \n",
    "        # Print metrics\n",
    "        print(f'Step {step:2d} | Loss: {loss.item():.4f} | dt: {dt:6.2f}ms | tok/sec: {tokens_per_sec:8.2f}')\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Training Complete!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Final loss: {loss.item():.4f}\")\n",
    "\n",
    "# Print profiler summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Profiler Summary\")\n",
    "print(\"=\"*80)\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\" if torch.cuda.is_available() else \"cpu_time_total\", row_limit=20))\n",
    "\n",
    "# Export profiler trace (for TensorBoard)\n",
    "print(\"\\nProfiler trace saved to: ./log/profiler\")\n",
    "print(\"View with: tensorboard --logdir=./log/profiler\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9857aa8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
