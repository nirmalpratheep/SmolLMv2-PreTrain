{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5a2069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: rotary-embedding-torch in /home/ubuntu/.local/lib/python3.10/site-packages (0.8.9)\n",
      "Requirement already satisfied: transformers in /home/ubuntu/.local/lib/python3.10/site-packages (4.57.1)\n",
      "Requirement already satisfied: pyyaml in /usr/lib/python3/dist-packages (5.4.1)\n",
      "Requirement already satisfied: einops>=0.7 in /home/ubuntu/.local/lib/python3.10/site-packages (from rotary-embedding-torch) (0.8.1)\n",
      "Requirement already satisfied: torch>=2.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from rotary-embedding-torch) (2.9.1)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.10.0)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=2.0->rotary-embedding-torch) (11.7.3.90)\n",
      "Requirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from torch>=2.0->rotary-embedding-torch) (3.0.3)\n",
      "Requirement already satisfied: triton==3.5.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=2.0->rotary-embedding-torch) (3.5.1)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=2.0->rotary-embedding-torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=2.0->rotary-embedding-torch) (1.13.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=2.0->rotary-embedding-torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=2.0->rotary-embedding-torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=2.0->rotary-embedding-torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=2.0->rotary-embedding-torch) (3.3.20)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=2.0->rotary-embedding-torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=2.0->rotary-embedding-torch) (12.8.90)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=2.0->rotary-embedding-torch) (3.4.2)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=2.0->rotary-embedding-torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=2.0->rotary-embedding-torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=2.0->rotary-embedding-torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=2.0->rotary-embedding-torch) (10.3.9.90)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=2.0->rotary-embedding-torch) (1.14.0)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=2.0->rotary-embedding-torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch>=2.0->rotary-embedding-torch) (2.27.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from sympy>=1.13.3->torch>=2.0->rotary-embedding-torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All dependencies imported successfully!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "CELL 0: Environment Setup and Dependency Installation\n",
    "====================================================\n",
    "This cell sets up the Python environment and installs all required packages for \n",
    "reproducing SmolLMv2 from scratch.\n",
    "\n",
    "What it does:\n",
    "1. Installs core dependencies:\n",
    "   - rotary-embedding-torch: Implements Rotary Position Embeddings (RoPE) for positional encoding\n",
    "   - transformers: HuggingFace library for tokenizers and model utilities\n",
    "   - pyyaml: For reading YAML configuration files\n",
    "\n",
    "2. Imports essential libraries:\n",
    "   - PyTorch: Deep learning framework for building and training the model\n",
    "   - Transformers: For tokenizer and model loading utilities\n",
    "   - YAML: For configuration management\n",
    "   - Rotary Embedding: For implementing RoPE in attention layers\n",
    "\n",
    "This is the first step in reproducing SmolLMv2 - ensuring all dependencies are available.\n",
    "\"\"\"\n",
    "\n",
    "# Install required packages\n",
    "%pip install rotary-embedding-torch transformers pyyaml\n",
    "\n",
    "# Import all dependencies\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import inspect\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "# PyTorch - Core deep learning framework\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# Transformers - HuggingFace library for tokenizers and model utilities\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# YAML configuration - For reading model configuration from config.yaml\n",
    "import yaml\n",
    "\n",
    "# Rotary Embedding - Implements Rotary Position Embeddings (RoPE)\n",
    "from rotary_embedding_torch import RotaryEmbedding\n",
    "\n",
    "print(\"All dependencies imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2bacb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL 1: Load Reference Model from HuggingFace\n",
    "==============================================\n",
    "This cell loads the original SmolLM2-135M model from HuggingFace to:\n",
    "1. Use as a reference for architecture verification\n",
    "2. Extract the tokenizer (which we'll use for our from-scratch implementation)\n",
    "3. Compare our implementation against the original\n",
    "\n",
    "What it does:\n",
    "- Loads the tokenizer: Converts text to token IDs and vice versa\n",
    "- Loads the pretrained model: The original HuggingFace implementation\n",
    "- This helps us verify that our from-scratch implementation matches the architecture\n",
    "\n",
    "Note: We're loading this for reference only. Our goal is to reproduce this model\n",
    "from scratch with random initialization and train it ourselves.\n",
    "\"\"\"\n",
    "# Load model directly from HuggingFace for reference\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-135M\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM2-135M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fb5c08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(49152, 576)\n",
       "    (layers): ModuleList(\n",
       "      (0-29): 30 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
       "          (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
       "          (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
       "          (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
       "          (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
       "          (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((576,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=576, out_features=49152, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "CELL 2: Inspect Reference Model Architecture\n",
    "=============================================\n",
    "This cell displays the structure of the reference HuggingFace model to understand:\n",
    "- The exact layer structure and configuration\n",
    "- How components are organized (attention, MLP, normalization)\n",
    "- Parameter counts and dimensions\n",
    "\n",
    "This inspection helps ensure our from-scratch implementation matches the original\n",
    "architecture exactly. We can compare this output with our custom implementation later.\n",
    "\"\"\"\n",
    "# Display the reference model structure for architecture verification\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165d95b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: prettytable in /home/ubuntu/.local/lib/python3.10/site-packages (3.17.0)\n",
      "Requirement already satisfied: wcwidth in /home/ubuntu/.local/lib/python3.10/site-packages (from prettytable) (0.2.14)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "CELL 3: Install PrettyTable for Parameter Counting\n",
    "===================================================\n",
    "This cell installs the prettytable package, which we'll use to create nicely\n",
    "formatted tables showing the parameter count breakdown for each layer.\n",
    "\n",
    "This helps us:\n",
    "- Verify our model has the correct number of parameters (134.5M)\n",
    "- Understand which components contribute most to the parameter count\n",
    "- Debug any architecture mismatches\n",
    "\"\"\"\n",
    "!pip install prettytable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9035ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(49152, 576)\n",
      "    (layers): ModuleList(\n",
      "      (0-29): 30 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "          (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "          (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "          (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "          (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "          (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((576,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=576, out_features=49152, bias=False)\n",
      ")\n",
      "134515008\n",
      "+-------------------------------------------------+------------+\n",
      "|                     Modules                     | Parameters |\n",
      "+-------------------------------------------------+------------+\n",
      "|            model.embed_tokens.weight            |  28311552  |\n",
      "|      model.layers.0.self_attn.q_proj.weight     |   331776   |\n",
      "|      model.layers.0.self_attn.k_proj.weight     |   110592   |\n",
      "|      model.layers.0.self_attn.v_proj.weight     |   110592   |\n",
      "|      model.layers.0.self_attn.o_proj.weight     |   331776   |\n",
      "|       model.layers.0.mlp.gate_proj.weight       |   884736   |\n",
      "|        model.layers.0.mlp.up_proj.weight        |   884736   |\n",
      "|       model.layers.0.mlp.down_proj.weight       |   884736   |\n",
      "|      model.layers.0.input_layernorm.weight      |    576     |\n",
      "|  model.layers.0.post_attention_layernorm.weight |    576     |\n",
      "|      model.layers.1.self_attn.q_proj.weight     |   331776   |\n",
      "|      model.layers.1.self_attn.k_proj.weight     |   110592   |\n",
      "|      model.layers.1.self_attn.v_proj.weight     |   110592   |\n",
      "|      model.layers.1.self_attn.o_proj.weight     |   331776   |\n",
      "|       model.layers.1.mlp.gate_proj.weight       |   884736   |\n",
      "|        model.layers.1.mlp.up_proj.weight        |   884736   |\n",
      "|       model.layers.1.mlp.down_proj.weight       |   884736   |\n",
      "|      model.layers.1.input_layernorm.weight      |    576     |\n",
      "|  model.layers.1.post_attention_layernorm.weight |    576     |\n",
      "|      model.layers.2.self_attn.q_proj.weight     |   331776   |\n",
      "|      model.layers.2.self_attn.k_proj.weight     |   110592   |\n",
      "|      model.layers.2.self_attn.v_proj.weight     |   110592   |\n",
      "|      model.layers.2.self_attn.o_proj.weight     |   331776   |\n",
      "|       model.layers.2.mlp.gate_proj.weight       |   884736   |\n",
      "|        model.layers.2.mlp.up_proj.weight        |   884736   |\n",
      "|       model.layers.2.mlp.down_proj.weight       |   884736   |\n",
      "|      model.layers.2.input_layernorm.weight      |    576     |\n",
      "|  model.layers.2.post_attention_layernorm.weight |    576     |\n",
      "|      model.layers.3.self_attn.q_proj.weight     |   331776   |\n",
      "|      model.layers.3.self_attn.k_proj.weight     |   110592   |\n",
      "|      model.layers.3.self_attn.v_proj.weight     |   110592   |\n",
      "|      model.layers.3.self_attn.o_proj.weight     |   331776   |\n",
      "|       model.layers.3.mlp.gate_proj.weight       |   884736   |\n",
      "|        model.layers.3.mlp.up_proj.weight        |   884736   |\n",
      "|       model.layers.3.mlp.down_proj.weight       |   884736   |\n",
      "|      model.layers.3.input_layernorm.weight      |    576     |\n",
      "|  model.layers.3.post_attention_layernorm.weight |    576     |\n",
      "|      model.layers.4.self_attn.q_proj.weight     |   331776   |\n",
      "|      model.layers.4.self_attn.k_proj.weight     |   110592   |\n",
      "|      model.layers.4.self_attn.v_proj.weight     |   110592   |\n",
      "|      model.layers.4.self_attn.o_proj.weight     |   331776   |\n",
      "|       model.layers.4.mlp.gate_proj.weight       |   884736   |\n",
      "|        model.layers.4.mlp.up_proj.weight        |   884736   |\n",
      "|       model.layers.4.mlp.down_proj.weight       |   884736   |\n",
      "|      model.layers.4.input_layernorm.weight      |    576     |\n",
      "|  model.layers.4.post_attention_layernorm.weight |    576     |\n",
      "|      model.layers.5.self_attn.q_proj.weight     |   331776   |\n",
      "|      model.layers.5.self_attn.k_proj.weight     |   110592   |\n",
      "|      model.layers.5.self_attn.v_proj.weight     |   110592   |\n",
      "|      model.layers.5.self_attn.o_proj.weight     |   331776   |\n",
      "|       model.layers.5.mlp.gate_proj.weight       |   884736   |\n",
      "|        model.layers.5.mlp.up_proj.weight        |   884736   |\n",
      "|       model.layers.5.mlp.down_proj.weight       |   884736   |\n",
      "|      model.layers.5.input_layernorm.weight      |    576     |\n",
      "|  model.layers.5.post_attention_layernorm.weight |    576     |\n",
      "|      model.layers.6.self_attn.q_proj.weight     |   331776   |\n",
      "|      model.layers.6.self_attn.k_proj.weight     |   110592   |\n",
      "|      model.layers.6.self_attn.v_proj.weight     |   110592   |\n",
      "|      model.layers.6.self_attn.o_proj.weight     |   331776   |\n",
      "|       model.layers.6.mlp.gate_proj.weight       |   884736   |\n",
      "|        model.layers.6.mlp.up_proj.weight        |   884736   |\n",
      "|       model.layers.6.mlp.down_proj.weight       |   884736   |\n",
      "|      model.layers.6.input_layernorm.weight      |    576     |\n",
      "|  model.layers.6.post_attention_layernorm.weight |    576     |\n",
      "|      model.layers.7.self_attn.q_proj.weight     |   331776   |\n",
      "|      model.layers.7.self_attn.k_proj.weight     |   110592   |\n",
      "|      model.layers.7.self_attn.v_proj.weight     |   110592   |\n",
      "|      model.layers.7.self_attn.o_proj.weight     |   331776   |\n",
      "|       model.layers.7.mlp.gate_proj.weight       |   884736   |\n",
      "|        model.layers.7.mlp.up_proj.weight        |   884736   |\n",
      "|       model.layers.7.mlp.down_proj.weight       |   884736   |\n",
      "|      model.layers.7.input_layernorm.weight      |    576     |\n",
      "|  model.layers.7.post_attention_layernorm.weight |    576     |\n",
      "|      model.layers.8.self_attn.q_proj.weight     |   331776   |\n",
      "|      model.layers.8.self_attn.k_proj.weight     |   110592   |\n",
      "|      model.layers.8.self_attn.v_proj.weight     |   110592   |\n",
      "|      model.layers.8.self_attn.o_proj.weight     |   331776   |\n",
      "|       model.layers.8.mlp.gate_proj.weight       |   884736   |\n",
      "|        model.layers.8.mlp.up_proj.weight        |   884736   |\n",
      "|       model.layers.8.mlp.down_proj.weight       |   884736   |\n",
      "|      model.layers.8.input_layernorm.weight      |    576     |\n",
      "|  model.layers.8.post_attention_layernorm.weight |    576     |\n",
      "|      model.layers.9.self_attn.q_proj.weight     |   331776   |\n",
      "|      model.layers.9.self_attn.k_proj.weight     |   110592   |\n",
      "|      model.layers.9.self_attn.v_proj.weight     |   110592   |\n",
      "|      model.layers.9.self_attn.o_proj.weight     |   331776   |\n",
      "|       model.layers.9.mlp.gate_proj.weight       |   884736   |\n",
      "|        model.layers.9.mlp.up_proj.weight        |   884736   |\n",
      "|       model.layers.9.mlp.down_proj.weight       |   884736   |\n",
      "|      model.layers.9.input_layernorm.weight      |    576     |\n",
      "|  model.layers.9.post_attention_layernorm.weight |    576     |\n",
      "|     model.layers.10.self_attn.q_proj.weight     |   331776   |\n",
      "|     model.layers.10.self_attn.k_proj.weight     |   110592   |\n",
      "|     model.layers.10.self_attn.v_proj.weight     |   110592   |\n",
      "|     model.layers.10.self_attn.o_proj.weight     |   331776   |\n",
      "|       model.layers.10.mlp.gate_proj.weight      |   884736   |\n",
      "|        model.layers.10.mlp.up_proj.weight       |   884736   |\n",
      "|       model.layers.10.mlp.down_proj.weight      |   884736   |\n",
      "|      model.layers.10.input_layernorm.weight     |    576     |\n",
      "| model.layers.10.post_attention_layernorm.weight |    576     |\n",
      "|     model.layers.11.self_attn.q_proj.weight     |   331776   |\n",
      "|     model.layers.11.self_attn.k_proj.weight     |   110592   |\n",
      "|     model.layers.11.self_attn.v_proj.weight     |   110592   |\n",
      "|     model.layers.11.self_attn.o_proj.weight     |   331776   |\n",
      "|       model.layers.11.mlp.gate_proj.weight      |   884736   |\n",
      "|        model.layers.11.mlp.up_proj.weight       |   884736   |\n",
      "|       model.layers.11.mlp.down_proj.weight      |   884736   |\n",
      "|      model.layers.11.input_layernorm.weight     |    576     |\n",
      "| model.layers.11.post_attention_layernorm.weight |    576     |\n",
      "|     model.layers.12.self_attn.q_proj.weight     |   331776   |\n",
      "|     model.layers.12.self_attn.k_proj.weight     |   110592   |\n",
      "|     model.layers.12.self_attn.v_proj.weight     |   110592   |\n",
      "|     model.layers.12.self_attn.o_proj.weight     |   331776   |\n",
      "|       model.layers.12.mlp.gate_proj.weight      |   884736   |\n",
      "|        model.layers.12.mlp.up_proj.weight       |   884736   |\n",
      "|       model.layers.12.mlp.down_proj.weight      |   884736   |\n",
      "|      model.layers.12.input_layernorm.weight     |    576     |\n",
      "| model.layers.12.post_attention_layernorm.weight |    576     |\n",
      "|     model.layers.13.self_attn.q_proj.weight     |   331776   |\n",
      "|     model.layers.13.self_attn.k_proj.weight     |   110592   |\n",
      "|     model.layers.13.self_attn.v_proj.weight     |   110592   |\n",
      "|     model.layers.13.self_attn.o_proj.weight     |   331776   |\n",
      "|       model.layers.13.mlp.gate_proj.weight      |   884736   |\n",
      "|        model.layers.13.mlp.up_proj.weight       |   884736   |\n",
      "|       model.layers.13.mlp.down_proj.weight      |   884736   |\n",
      "|      model.layers.13.input_layernorm.weight     |    576     |\n",
      "| model.layers.13.post_attention_layernorm.weight |    576     |\n",
      "|     model.layers.14.self_attn.q_proj.weight     |   331776   |\n",
      "|     model.layers.14.self_attn.k_proj.weight     |   110592   |\n",
      "|     model.layers.14.self_attn.v_proj.weight     |   110592   |\n",
      "|     model.layers.14.self_attn.o_proj.weight     |   331776   |\n",
      "|       model.layers.14.mlp.gate_proj.weight      |   884736   |\n",
      "|        model.layers.14.mlp.up_proj.weight       |   884736   |\n",
      "|       model.layers.14.mlp.down_proj.weight      |   884736   |\n",
      "|      model.layers.14.input_layernorm.weight     |    576     |\n",
      "| model.layers.14.post_attention_layernorm.weight |    576     |\n",
      "|     model.layers.15.self_attn.q_proj.weight     |   331776   |\n",
      "|     model.layers.15.self_attn.k_proj.weight     |   110592   |\n",
      "|     model.layers.15.self_attn.v_proj.weight     |   110592   |\n",
      "|     model.layers.15.self_attn.o_proj.weight     |   331776   |\n",
      "|       model.layers.15.mlp.gate_proj.weight      |   884736   |\n",
      "|        model.layers.15.mlp.up_proj.weight       |   884736   |\n",
      "|       model.layers.15.mlp.down_proj.weight      |   884736   |\n",
      "|      model.layers.15.input_layernorm.weight     |    576     |\n",
      "| model.layers.15.post_attention_layernorm.weight |    576     |\n",
      "|     model.layers.16.self_attn.q_proj.weight     |   331776   |\n",
      "|     model.layers.16.self_attn.k_proj.weight     |   110592   |\n",
      "|     model.layers.16.self_attn.v_proj.weight     |   110592   |\n",
      "|     model.layers.16.self_attn.o_proj.weight     |   331776   |\n",
      "|       model.layers.16.mlp.gate_proj.weight      |   884736   |\n",
      "|        model.layers.16.mlp.up_proj.weight       |   884736   |\n",
      "|       model.layers.16.mlp.down_proj.weight      |   884736   |\n",
      "|      model.layers.16.input_layernorm.weight     |    576     |\n",
      "| model.layers.16.post_attention_layernorm.weight |    576     |\n",
      "|     model.layers.17.self_attn.q_proj.weight     |   331776   |\n",
      "|     model.layers.17.self_attn.k_proj.weight     |   110592   |\n",
      "|     model.layers.17.self_attn.v_proj.weight     |   110592   |\n",
      "|     model.layers.17.self_attn.o_proj.weight     |   331776   |\n",
      "|       model.layers.17.mlp.gate_proj.weight      |   884736   |\n",
      "|        model.layers.17.mlp.up_proj.weight       |   884736   |\n",
      "|       model.layers.17.mlp.down_proj.weight      |   884736   |\n",
      "|      model.layers.17.input_layernorm.weight     |    576     |\n",
      "| model.layers.17.post_attention_layernorm.weight |    576     |\n",
      "|     model.layers.18.self_attn.q_proj.weight     |   331776   |\n",
      "|     model.layers.18.self_attn.k_proj.weight     |   110592   |\n",
      "|     model.layers.18.self_attn.v_proj.weight     |   110592   |\n",
      "|     model.layers.18.self_attn.o_proj.weight     |   331776   |\n",
      "|       model.layers.18.mlp.gate_proj.weight      |   884736   |\n",
      "|        model.layers.18.mlp.up_proj.weight       |   884736   |\n",
      "|       model.layers.18.mlp.down_proj.weight      |   884736   |\n",
      "|      model.layers.18.input_layernorm.weight     |    576     |\n",
      "| model.layers.18.post_attention_layernorm.weight |    576     |\n",
      "|     model.layers.19.self_attn.q_proj.weight     |   331776   |\n",
      "|     model.layers.19.self_attn.k_proj.weight     |   110592   |\n",
      "|     model.layers.19.self_attn.v_proj.weight     |   110592   |\n",
      "|     model.layers.19.self_attn.o_proj.weight     |   331776   |\n",
      "|       model.layers.19.mlp.gate_proj.weight      |   884736   |\n",
      "|        model.layers.19.mlp.up_proj.weight       |   884736   |\n",
      "|       model.layers.19.mlp.down_proj.weight      |   884736   |\n",
      "|      model.layers.19.input_layernorm.weight     |    576     |\n",
      "| model.layers.19.post_attention_layernorm.weight |    576     |\n",
      "|     model.layers.20.self_attn.q_proj.weight     |   331776   |\n",
      "|     model.layers.20.self_attn.k_proj.weight     |   110592   |\n",
      "|     model.layers.20.self_attn.v_proj.weight     |   110592   |\n",
      "|     model.layers.20.self_attn.o_proj.weight     |   331776   |\n",
      "|       model.layers.20.mlp.gate_proj.weight      |   884736   |\n",
      "|        model.layers.20.mlp.up_proj.weight       |   884736   |\n",
      "|       model.layers.20.mlp.down_proj.weight      |   884736   |\n",
      "|      model.layers.20.input_layernorm.weight     |    576     |\n",
      "| model.layers.20.post_attention_layernorm.weight |    576     |\n",
      "|     model.layers.21.self_attn.q_proj.weight     |   331776   |\n",
      "|     model.layers.21.self_attn.k_proj.weight     |   110592   |\n",
      "|     model.layers.21.self_attn.v_proj.weight     |   110592   |\n",
      "|     model.layers.21.self_attn.o_proj.weight     |   331776   |\n",
      "|       model.layers.21.mlp.gate_proj.weight      |   884736   |\n",
      "|        model.layers.21.mlp.up_proj.weight       |   884736   |\n",
      "|       model.layers.21.mlp.down_proj.weight      |   884736   |\n",
      "|      model.layers.21.input_layernorm.weight     |    576     |\n",
      "| model.layers.21.post_attention_layernorm.weight |    576     |\n",
      "|     model.layers.22.self_attn.q_proj.weight     |   331776   |\n",
      "|     model.layers.22.self_attn.k_proj.weight     |   110592   |\n",
      "|     model.layers.22.self_attn.v_proj.weight     |   110592   |\n",
      "|     model.layers.22.self_attn.o_proj.weight     |   331776   |\n",
      "|       model.layers.22.mlp.gate_proj.weight      |   884736   |\n",
      "|        model.layers.22.mlp.up_proj.weight       |   884736   |\n",
      "|       model.layers.22.mlp.down_proj.weight      |   884736   |\n",
      "|      model.layers.22.input_layernorm.weight     |    576     |\n",
      "| model.layers.22.post_attention_layernorm.weight |    576     |\n",
      "|     model.layers.23.self_attn.q_proj.weight     |   331776   |\n",
      "|     model.layers.23.self_attn.k_proj.weight     |   110592   |\n",
      "|     model.layers.23.self_attn.v_proj.weight     |   110592   |\n",
      "|     model.layers.23.self_attn.o_proj.weight     |   331776   |\n",
      "|       model.layers.23.mlp.gate_proj.weight      |   884736   |\n",
      "|        model.layers.23.mlp.up_proj.weight       |   884736   |\n",
      "|       model.layers.23.mlp.down_proj.weight      |   884736   |\n",
      "|      model.layers.23.input_layernorm.weight     |    576     |\n",
      "| model.layers.23.post_attention_layernorm.weight |    576     |\n",
      "|     model.layers.24.self_attn.q_proj.weight     |   331776   |\n",
      "|     model.layers.24.self_attn.k_proj.weight     |   110592   |\n",
      "|     model.layers.24.self_attn.v_proj.weight     |   110592   |\n",
      "|     model.layers.24.self_attn.o_proj.weight     |   331776   |\n",
      "|       model.layers.24.mlp.gate_proj.weight      |   884736   |\n",
      "|        model.layers.24.mlp.up_proj.weight       |   884736   |\n",
      "|       model.layers.24.mlp.down_proj.weight      |   884736   |\n",
      "|      model.layers.24.input_layernorm.weight     |    576     |\n",
      "| model.layers.24.post_attention_layernorm.weight |    576     |\n",
      "|     model.layers.25.self_attn.q_proj.weight     |   331776   |\n",
      "|     model.layers.25.self_attn.k_proj.weight     |   110592   |\n",
      "|     model.layers.25.self_attn.v_proj.weight     |   110592   |\n",
      "|     model.layers.25.self_attn.o_proj.weight     |   331776   |\n",
      "|       model.layers.25.mlp.gate_proj.weight      |   884736   |\n",
      "|        model.layers.25.mlp.up_proj.weight       |   884736   |\n",
      "|       model.layers.25.mlp.down_proj.weight      |   884736   |\n",
      "|      model.layers.25.input_layernorm.weight     |    576     |\n",
      "| model.layers.25.post_attention_layernorm.weight |    576     |\n",
      "|     model.layers.26.self_attn.q_proj.weight     |   331776   |\n",
      "|     model.layers.26.self_attn.k_proj.weight     |   110592   |\n",
      "|     model.layers.26.self_attn.v_proj.weight     |   110592   |\n",
      "|     model.layers.26.self_attn.o_proj.weight     |   331776   |\n",
      "|       model.layers.26.mlp.gate_proj.weight      |   884736   |\n",
      "|        model.layers.26.mlp.up_proj.weight       |   884736   |\n",
      "|       model.layers.26.mlp.down_proj.weight      |   884736   |\n",
      "|      model.layers.26.input_layernorm.weight     |    576     |\n",
      "| model.layers.26.post_attention_layernorm.weight |    576     |\n",
      "|     model.layers.27.self_attn.q_proj.weight     |   331776   |\n",
      "|     model.layers.27.self_attn.k_proj.weight     |   110592   |\n",
      "|     model.layers.27.self_attn.v_proj.weight     |   110592   |\n",
      "|     model.layers.27.self_attn.o_proj.weight     |   331776   |\n",
      "|       model.layers.27.mlp.gate_proj.weight      |   884736   |\n",
      "|        model.layers.27.mlp.up_proj.weight       |   884736   |\n",
      "|       model.layers.27.mlp.down_proj.weight      |   884736   |\n",
      "|      model.layers.27.input_layernorm.weight     |    576     |\n",
      "| model.layers.27.post_attention_layernorm.weight |    576     |\n",
      "|     model.layers.28.self_attn.q_proj.weight     |   331776   |\n",
      "|     model.layers.28.self_attn.k_proj.weight     |   110592   |\n",
      "|     model.layers.28.self_attn.v_proj.weight     |   110592   |\n",
      "|     model.layers.28.self_attn.o_proj.weight     |   331776   |\n",
      "|       model.layers.28.mlp.gate_proj.weight      |   884736   |\n",
      "|        model.layers.28.mlp.up_proj.weight       |   884736   |\n",
      "|       model.layers.28.mlp.down_proj.weight      |   884736   |\n",
      "|      model.layers.28.input_layernorm.weight     |    576     |\n",
      "| model.layers.28.post_attention_layernorm.weight |    576     |\n",
      "|     model.layers.29.self_attn.q_proj.weight     |   331776   |\n",
      "|     model.layers.29.self_attn.k_proj.weight     |   110592   |\n",
      "|     model.layers.29.self_attn.v_proj.weight     |   110592   |\n",
      "|     model.layers.29.self_attn.o_proj.weight     |   331776   |\n",
      "|       model.layers.29.mlp.gate_proj.weight      |   884736   |\n",
      "|        model.layers.29.mlp.up_proj.weight       |   884736   |\n",
      "|       model.layers.29.mlp.down_proj.weight      |   884736   |\n",
      "|      model.layers.29.input_layernorm.weight     |    576     |\n",
      "| model.layers.29.post_attention_layernorm.weight |    576     |\n",
      "|                model.norm.weight                |    576     |\n",
      "+-------------------------------------------------+------------+\n",
      "Total Trainable Params: 134515008\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "134515008"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "CELL 4: Count Parameters in Reference Model\n",
    "===========================================\n",
    "This cell counts and displays all trainable parameters in the reference model,\n",
    "organized by layer. This serves as a baseline to verify our from-scratch\n",
    "implementation has the correct parameter count.\n",
    "\n",
    "What it does:\n",
    "1. Prints the full model structure\n",
    "2. Calculates total trainable parameters (should be ~134.5M)\n",
    "3. Creates a detailed table showing parameter count per layer/component\n",
    "4. Helps identify which components use the most parameters:\n",
    "   - Token embeddings: ~28M (largest component)\n",
    "   - MLP layers: ~2.6M per layer × 30 = ~78M\n",
    "   - Attention layers: ~885K per layer × 30 = ~26.5M\n",
    "\n",
    "This breakdown is crucial for understanding the model architecture and verifying\n",
    "our implementation matches the original.\n",
    "\"\"\"\n",
    "print(model)\n",
    "print(sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "def count_parameters(model):\n",
    "    \"\"\"\n",
    "    Count and display all trainable parameters in the model, organized by layer.\n",
    "    This helps verify our implementation matches the reference architecture.\n",
    "    \"\"\"\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad:\n",
    "            continue\n",
    "        params = parameter.numel()\n",
    "        table.add_row([name, params])\n",
    "        total_params += params\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params\n",
    "    \n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42338af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Loaded SmolLMV2 Configuration\n",
      "============================================================\n",
      "\n",
      "[Model Configuration]\n",
      "  model_type: llama\n",
      "  vocab_size: 49152\n",
      "  hidden_size: 576\n",
      "  intermediate_size: 1536\n",
      "  num_hidden_layers: 30\n",
      "  num_attention_heads: 18\n",
      "  num_key_value_heads: 6\n",
      "  max_position_embeddings: 8192\n",
      "  rms_norm_eps: 1e-05\n",
      "  use_cache: True\n",
      "  pad_token_id: None\n",
      "  bos_token_id: 1\n",
      "  eos_token_id: 2\n",
      "  mlp_bias: False\n",
      "  mlp_gate_proj_features: 1536\n",
      "  mlp_up_proj_features: 1536\n",
      "  mlp_down_proj_features: 576\n",
      "  hidden_act: silu\n",
      "  use_silu: True\n",
      "  pretrained_model_name_or_path: HuggingFaceTB/SmolLM2-135M\n",
      "  torch_dtype: float16\n",
      "  device_map: auto\n",
      "  trust_remote_code: False\n",
      "\n",
      "[Tokenizer Configuration]\n",
      "  tokenizer_name_or_path: HuggingFaceTB/SmolLM2-135M\n",
      "  use_fast: True\n",
      "  padding_side: right\n",
      "  truncation_side: right\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "CELL 5: Load Configuration from YAML File\n",
    "==========================================\n",
    "This cell defines configuration classes and loads all model and tokenizer\n",
    "parameters from config.yaml. This approach allows us to:\n",
    "1. Centralize all hyperparameters in one file\n",
    "2. Easily experiment with different configurations\n",
    "3. Reproduce exact training runs by saving configs\n",
    "\n",
    "What it does:\n",
    "1. Defines ModelConfig dataclass: Contains all model architecture parameters\n",
    "   - Architecture: vocab_size, hidden_size, num_layers, attention heads\n",
    "   - GQA: num_key_value_heads (6) vs num_attention_heads (18) = 3:1 ratio\n",
    "   - MLP: intermediate_size, activation functions\n",
    "   - Normalization: RMSNorm epsilon value\n",
    "\n",
    "2. Defines TokenizerConfig: Tokenizer settings from HuggingFace\n",
    "\n",
    "3. Loads config.yaml: Reads all parameters and creates config objects\n",
    "\n",
    "4. Displays configuration: Shows all loaded parameters for verification\n",
    "\n",
    "Key parameters loaded:\n",
    "- vocab_size: 49152 (vocabulary size)\n",
    "- hidden_size: 576 (embedding dimension)\n",
    "- num_hidden_layers: 30 (transformer layers)\n",
    "- num_attention_heads: 18 (query heads)\n",
    "- num_key_value_heads: 6 (key-value heads for GQA)\n",
    "- intermediate_size: 1536 (MLP hidden dimension)\n",
    "\"\"\"\n",
    "\n",
    "# Load configuration from YAML file\n",
    "# All imports (yaml, dataclass, field, Optional) are in the first cell\n",
    "\n",
    "# Load defaults from config.yaml first\n",
    "def load_yaml_defaults(yaml_path: str = \"config.yaml\"):\n",
    "    \"\"\"Load default values from YAML file\"\"\"\n",
    "    with open(yaml_path, 'r') as f:\n",
    "        return yaml.safe_load(f) or {}\n",
    "\n",
    "# Load YAML to get defaults\n",
    "_yaml_defaults = load_yaml_defaults(\"config.yaml\")\n",
    "_model_defaults = _yaml_defaults.get(\"model\", {})\n",
    "_tokenizer_defaults = _yaml_defaults.get(\"tokenizer\", {})\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"\n",
    "    Model architecture and initialization parameters.\n",
    "    All values are loaded from config.yaml with sensible defaults.\n",
    "    \"\"\"\n",
    "    model_type: str = _model_defaults.get(\"model_type\", \"llama\")\n",
    "    vocab_size: int = _model_defaults.get(\"vocab_size\", 49152)\n",
    "    hidden_size: int = _model_defaults.get(\"hidden_size\", 576)\n",
    "    intermediate_size: int = _model_defaults.get(\"intermediate_size\", 1536)\n",
    "    num_hidden_layers: int = _model_defaults.get(\"num_hidden_layers\", 30)\n",
    "    num_attention_heads: int = _model_defaults.get(\"num_attention_heads\", 18)\n",
    "    num_key_value_heads: int = _model_defaults.get(\"num_key_value_heads\", 6)\n",
    "    max_position_embeddings: int = _model_defaults.get(\"max_position_embeddings\", 8192)\n",
    "    rms_norm_eps: float = _model_defaults.get(\"rms_norm_eps\", 1e-05)\n",
    "    use_cache: bool = _model_defaults.get(\"use_cache\", True)\n",
    "    pad_token_id: Optional[int] = _model_defaults.get(\"pad_token_id\", None)\n",
    "    bos_token_id: int = _model_defaults.get(\"bos_token_id\", 1)\n",
    "    eos_token_id: int = _model_defaults.get(\"eos_token_id\", 2)\n",
    "    \n",
    "    # MLP Configuration\n",
    "    mlp_bias: bool = _model_defaults.get(\"mlp_bias\", False)\n",
    "    mlp_gate_proj_features: int = _model_defaults.get(\"mlp_gate_proj_features\", 1536)\n",
    "    mlp_up_proj_features: int = _model_defaults.get(\"mlp_up_proj_features\", 1536)\n",
    "    mlp_down_proj_features: int = _model_defaults.get(\"mlp_down_proj_features\", 576)\n",
    "    \n",
    "    # Activation Function\n",
    "    hidden_act: str = _model_defaults.get(\"hidden_act\", \"silu\")\n",
    "    use_silu: bool = _model_defaults.get(\"use_silu\", True)\n",
    "    \n",
    "    # Model initialization\n",
    "    pretrained_model_name_or_path: str = _model_defaults.get(\"pretrained_model_name_or_path\", \"HuggingFaceTB/SmolLM2-135M\")\n",
    "    torch_dtype: str = _model_defaults.get(\"torch_dtype\", \"float16\")\n",
    "    device_map: str = _model_defaults.get(\"device_map\", \"auto\")\n",
    "    trust_remote_code: bool = _model_defaults.get(\"trust_remote_code\", False)\n",
    "\n",
    "@dataclass\n",
    "class TokenizerConfig:\n",
    "    \"\"\"Tokenizer configuration parameters\"\"\"\n",
    "    tokenizer_name_or_path: str = _tokenizer_defaults.get(\"tokenizer_name_or_path\", \"HuggingFaceTB/SmolLM2-135M\")\n",
    "    use_fast: bool = _tokenizer_defaults.get(\"use_fast\", True)\n",
    "    padding_side: str = _tokenizer_defaults.get(\"padding_side\", \"right\")\n",
    "    truncation_side: str = _tokenizer_defaults.get(\"truncation_side\", \"right\")\n",
    "\n",
    "@dataclass\n",
    "class SmolLMV2Config:\n",
    "    \"\"\"Main configuration class combining model and tokenizer configs\"\"\"\n",
    "    model: ModelConfig = field(default_factory=ModelConfig)\n",
    "    tokenizer: TokenizerConfig = field(default_factory=TokenizerConfig)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_yaml(cls, yaml_path: str = \"config.yaml\"):\n",
    "        \"\"\"Load configuration from YAML file and create config objects\"\"\"\n",
    "        with open(yaml_path, 'r') as f:\n",
    "            config_dict = yaml.safe_load(f) or {}\n",
    "        \n",
    "        # Create configs with defaults from YAML, then update from YAML\n",
    "        model_config = ModelConfig()\n",
    "        if \"model\" in config_dict:\n",
    "            for key, value in config_dict[\"model\"].items():\n",
    "                if hasattr(model_config, key):\n",
    "                    setattr(model_config, key, value)\n",
    "        \n",
    "        tokenizer_config = TokenizerConfig()\n",
    "        if \"tokenizer\" in config_dict:\n",
    "            for key, value in config_dict[\"tokenizer\"].items():\n",
    "                if hasattr(tokenizer_config, key):\n",
    "                    setattr(tokenizer_config, key, value)\n",
    "        \n",
    "        return cls(model=model_config, tokenizer=tokenizer_config)\n",
    "\n",
    "# Load configuration from config.yaml\n",
    "cfg = SmolLMV2Config.from_yaml(\"config.yaml\")\n",
    "\n",
    "# Display loaded configuration for verification\n",
    "print(\"=\" * 60)\n",
    "print(\"Loaded SmolLMV2 Configuration\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n[Model Configuration]\")\n",
    "for key, value in cfg.model.__dict__.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "print(\"\\n[Tokenizer Configuration]\")\n",
    "for key, value in cfg.tokenizer.__dict__.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bbc7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL 6: Implement LlamaAttention with Grouped Query Attention (GQA)\n",
    "====================================================================\n",
    "This cell implements the attention mechanism with Grouped Query Attention (GQA),\n",
    "which is a key optimization in SmolLMv2. GQA reduces memory usage by sharing\n",
    "key-value heads across multiple query heads.\n",
    "\n",
    "What it does:\n",
    "1. Implements Grouped Query Attention (GQA):\n",
    "   - Query projection: All 18 heads (full dimension)\n",
    "   - Key/Value projections: Only 6 heads (shared across 3 query heads each)\n",
    "   - This reduces KV cache memory by 3x while maintaining quality\n",
    "\n",
    "2. Applies Rotary Position Embeddings (RoPE):\n",
    "   - Rotates queries and keys with positional information\n",
    "   - Enables the model to understand relative positions\n",
    "   - Applied before attention computation\n",
    "\n",
    "3. Implements causal masking:\n",
    "   - Prevents attention to future tokens (autoregressive property)\n",
    "   - Uses lower triangular mask\n",
    "\n",
    "4. Uses Flash Attention (scaled_dot_product_attention):\n",
    "   - Optimized attention implementation for faster training\n",
    "   - Reduces memory usage during attention computation\n",
    "   - Automatically handles causal masking\n",
    "\n",
    "Key architectural details:\n",
    "- n_head = 18 (query heads)\n",
    "- num_key_value_heads = 6 (key-value heads)\n",
    "- GQA ratio: 18/6 = 3:1 (each KV head shared by 3 query heads)\n",
    "- head_dim = 576 / 18 = 32 (dimension per head)\n",
    "\"\"\"\n",
    "\n",
    "class LlamaAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Grouped Query Attention (GQA) implementation for SmolLMv2.\n",
    "    \n",
    "    GQA reduces memory by sharing key-value heads across multiple query heads.\n",
    "    This is more efficient than standard multi-head attention while maintaining\n",
    "    similar model quality.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.n_head = config.n_head  # Number of query heads (18)\n",
    "        self.n_embd = config.n_embd  # Embedding dimension (576)\n",
    "        self.num_key_value_heads = config.num_key_value_heads  # KV heads (6)\n",
    "        self.head_dim = config.n_embd // config.n_head  # Dimension per head (32)\n",
    "        \n",
    "        # Separate projections for q, k, v, o (LLaMA style)\n",
    "        # q_proj: all heads, k_proj and v_proj: only num_key_value_heads (GQA)\n",
    "        self.q_proj = nn.Linear(config.n_embd, config.n_embd, bias=False)  # 18 heads\n",
    "        self.k_proj = nn.Linear(config.n_embd, config.num_key_value_heads * self.head_dim, bias=False)  # 6 heads\n",
    "        self.v_proj = nn.Linear(config.n_embd, config.num_key_value_heads * self.head_dim, bias=False)  # 6 heads\n",
    "        self.o_proj = nn.Linear(config.n_embd, config.n_embd, bias=False)  # Output projection\n",
    "        \n",
    "        # Rotary positional embedding (RoPE) - applied to q and k in forward\n",
    "        self.rotary_emb = RotaryEmbedding(self.head_dim)\n",
    "        \n",
    "        # Causal mask: lower triangular matrix to prevent attention to future tokens\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size)).view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through attention layer.\n",
    "        \n",
    "        Steps:\n",
    "        1. Project input to queries, keys, values\n",
    "        2. Reshape for multi-head attention\n",
    "        3. Apply RoPE to queries and keys\n",
    "        4. Repeat KV heads for GQA (if needed)\n",
    "        5. Compute attention with causal masking\n",
    "        6. Project output\n",
    "        \"\"\"\n",
    "        B, T, C = x.size()  # batch size, sequence length, embedding dimensionality\n",
    "        \n",
    "        # Project to q, k, v\n",
    "        q = self.q_proj(x)  # (B, T, n_embd) = (B, T, 576)\n",
    "        k = self.k_proj(x)  # (B, T, num_key_value_heads * head_dim) = (B, T, 192)\n",
    "        v = self.v_proj(x)  # (B, T, num_key_value_heads * head_dim) = (B, T, 192)\n",
    "        \n",
    "        # Reshape for multi-head attention: split into heads\n",
    "        q = q.view(B, T, self.n_head, self.head_dim).transpose(1, 2)  # (B, 18, T, 32)\n",
    "        k = k.view(B, T, self.num_key_value_heads, self.head_dim).transpose(1, 2)  # (B, 6, T, 32)\n",
    "        v = v.view(B, T, self.num_key_value_heads, self.head_dim).transpose(1, 2)  # (B, 6, T, 32)\n",
    "        \n",
    "        # Apply rotary positional embeddings to q and k (RoPE)\n",
    "        # This encodes positional information directly into queries and keys\n",
    "        q = self.rotary_emb.rotate_queries_or_keys(q)\n",
    "        k = self.rotary_emb.rotate_queries_or_keys(k)\n",
    "        \n",
    "        # Repeat k and v for GQA (if num_key_value_heads < n_head)\n",
    "        # Each KV head is shared by multiple query heads\n",
    "        if self.num_key_value_heads != self.n_head:\n",
    "            # Repeat k and v to match number of query heads\n",
    "            repeat_factor = self.n_head // self.num_key_value_heads  # 18 / 6 = 3\n",
    "            k = k.repeat_interleave(repeat_factor, dim=1)  # (B, 18, T, 32)\n",
    "            v = v.repeat_interleave(repeat_factor, dim=1)  # (B, 18, T, 32)\n",
    "        \n",
    "        # Causal self-attention using Flash Attention\n",
    "        # Flash Attention is optimized for speed and memory efficiency\n",
    "        # is_causal=True automatically applies causal masking\n",
    "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True)  # (B, 18, T, 32)\n",
    "        \n",
    "        # Re-assemble all head outputs: (B, n_head, T, head_dim) -> (B, T, C)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)  # (B, T, 576)\n",
    "        \n",
    "        # Output projection\n",
    "        y = self.o_proj(y)  # (B, T, 576)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b351179",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL 7: Implement LlamaMLP (Feed-Forward Network) with SwiGLU Activation\n",
    "=========================================================================\n",
    "This cell implements the MLP (Multi-Layer Perceptron) component of the transformer,\n",
    "which processes the output of the attention layer. SmolLMv2 uses SwiGLU activation,\n",
    "which is a gated linear unit with SiLU activation.\n",
    "\n",
    "What it does:\n",
    "1. Implements SwiGLU (SiLU-gated Linear Unit):\n",
    "   - Gate projection: SiLU(gate_proj(x))\n",
    "   - Up projection: up_proj(x)\n",
    "   - Output: down_proj(gate * up)\n",
    "   - This is more effective than standard ReLU/GELU activations\n",
    "\n",
    "2. Three linear projections:\n",
    "   - gate_proj: 576 -> 1536 (gating signal)\n",
    "   - up_proj: 576 -> 1536 (value signal)\n",
    "   - down_proj: 1536 -> 576 (output projection)\n",
    "\n",
    "3. Activation function from config:\n",
    "   - Default: SiLU (Sigmoid Linear Unit)\n",
    "   - Can be changed to GELU or ReLU via config.yaml\n",
    "\n",
    "Key architectural details:\n",
    "- Input/Output: 576 dimensions (hidden_size)\n",
    "- Intermediate: 1536 dimensions (intermediate_size)\n",
    "- Expansion ratio: 1536/576 = 2.67x\n",
    "- No bias terms (mlp_bias=False) for efficiency\n",
    "\"\"\"\n",
    "\n",
    "class LlamaMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Layer Perceptron with SwiGLU activation.\n",
    "    \n",
    "    SwiGLU is a gated activation function that has shown better performance\n",
    "    than standard activations in language models. The formula is:\n",
    "    output = down_proj(SiLU(gate_proj(x)) * up_proj(x))\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # Three linear projections for SwiGLU\n",
    "        self.gate_proj = nn.Linear(config.hidden_size, config.intermediate_size, bias=config.mlp_bias)  # 576 -> 1536\n",
    "        self.up_proj = nn.Linear(config.hidden_size, config.intermediate_size, bias=config.mlp_bias)  # 576 -> 1536\n",
    "        self.down_proj = nn.Linear(config.intermediate_size, config.hidden_size, bias=config.mlp_bias)  # 1536 -> 576\n",
    "        \n",
    "        # Use activation from config.yaml (default: SiLU)\n",
    "        if config.hidden_act == \"silu\":\n",
    "            self.act_fn = nn.SiLU()  # SiLU: x * sigmoid(x)\n",
    "        elif config.hidden_act == \"gelu\":\n",
    "            self.act_fn = nn.GELU()\n",
    "        elif config.hidden_act == \"relu\":\n",
    "            self.act_fn = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through MLP with SwiGLU activation.\n",
    "        \n",
    "        SwiGLU formula: down_proj(SiLU(gate_proj(x)) * up_proj(x))\n",
    "        This gated mechanism allows the model to selectively pass information.\n",
    "        \"\"\"\n",
    "        # LLaMA MLP: SiLU(gate_proj(x)) * up_proj(x)\n",
    "        gate = self.act_fn(self.gate_proj(x))  # Gate signal with activation\n",
    "        up = self.up_proj(x)  # Value signal (no activation)\n",
    "        return self.down_proj(gate * up)  # Element-wise multiplication then project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719419cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL 8: Implement LlamaRMSNorm (Root Mean Square Layer Normalization)\n",
    "=======================================================================\n",
    "This cell implements RMSNorm, which is a simplified version of LayerNorm used\n",
    "in modern LLMs like LLaMA. RMSNorm is faster and simpler than LayerNorm because\n",
    "it doesn't center the mean (only normalizes by variance).\n",
    "\n",
    "What it does:\n",
    "1. Implements RMSNorm formula:\n",
    "   - Normalize: x / sqrt(mean(x^2) + eps)\n",
    "   - Scale: weight * normalized_x\n",
    "   - No mean centering (unlike LayerNorm)\n",
    "\n",
    "2. Learnable scale parameter:\n",
    "   - weight: Learnable parameter initialized to ones\n",
    "   - Allows the model to scale normalized values\n",
    "\n",
    "3. Epsilon for numerical stability:\n",
    "   - Prevents division by zero\n",
    "   - Default: 1e-05\n",
    "\n",
    "Key advantages over LayerNorm:\n",
    "- Faster computation (no mean calculation)\n",
    "- Simpler implementation\n",
    "- Similar or better performance in practice\n",
    "- Standard in modern LLMs (LLaMA, GPT, etc.)\n",
    "\"\"\"\n",
    "\n",
    "class LlamaRMSNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    Root Mean Square Layer Normalization (RMSNorm).\n",
    "    \n",
    "    RMSNorm normalizes by the root mean square of inputs, without centering\n",
    "    the mean. This is simpler and faster than LayerNorm while maintaining\n",
    "    similar performance.\n",
    "    \n",
    "    Formula: output = weight * x / sqrt(mean(x^2) + eps)\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # Learnable scale parameter (initialized to ones)\n",
    "        self.weight = nn.Parameter(torch.ones(config.hidden_size))\n",
    "        # Ensure rms_norm_eps is a float (YAML might load it as string)\n",
    "        self.variance_epsilon = float(config.rms_norm_eps)  # Default: 1e-05\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through RMSNorm.\n",
    "        \n",
    "        Normalizes by root mean square without mean centering.\n",
    "        This is faster than LayerNorm and works well in practice.\n",
    "        \"\"\"\n",
    "        # LLaMA RMSNorm: sqrt(1/n * sum(x^2)) * weight\n",
    "        # Compute root mean square: sqrt(mean(x^2) + eps)\n",
    "        rms = torch.sqrt(torch.mean(x * x, dim=-1, keepdim=True) + self.variance_epsilon)\n",
    "        # Normalize and scale\n",
    "        return self.weight * x / rms\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"LlamaRMSNorm(hidden_size={self.weight.shape[0]}, eps={self.variance_epsilon})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73edd76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CELL 9: Implement Complete Model Architecture (Decoder Layer, Decoder, and Full Model)\n",
    "=======================================================================================\n",
    "This cell implements the complete SmolLMv2 architecture by combining all components:\n",
    "1. LlamaDecoderLayer: Single transformer layer (attention + MLP)\n",
    "2. LlamaDecoder: Stack of 30 decoder layers\n",
    "3. LlamaModel: Complete model with embeddings, decoder, and language model head\n",
    "\n",
    "What it does:\n",
    "1. LlamaDecoderLayer:\n",
    "   - Pre-norm architecture: RMSNorm before attention and MLP\n",
    "   - Residual connections: x = x + attention(ln1(x)) + mlp(ln2(x))\n",
    "   - Standard transformer decoder block\n",
    "\n",
    "2. LlamaDecoder:\n",
    "   - Stacks 30 LlamaDecoderLayer instances\n",
    "   - Processes input through all layers sequentially\n",
    "\n",
    "3. LlamaModel:\n",
    "   - Token embeddings: Converts token IDs to dense vectors\n",
    "   - Transformer decoder: 30 layers of attention + MLP\n",
    "   - Final layer norm: RMSNorm before output\n",
    "   - Language model head: Projects to vocabulary for next-token prediction\n",
    "   - Weight tying: Embedding and output weights are shared (reduces parameters)\n",
    "\n",
    "4. Weight initialization:\n",
    "   - Normal distribution: mean=0.0, std=0.02 (standard for LLMs)\n",
    "   - Ensures stable training from random initialization\n",
    "\n",
    "5. Text generation:\n",
    "   - Autoregressive generation: predicts one token at a time\n",
    "   - Greedy decoding: always picks highest probability token\n",
    "   - Uses only last token's logits for next token prediction\n",
    "\"\"\"\n",
    "\n",
    "class LlamaDecoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Single transformer decoder layer with pre-norm architecture.\n",
    "    \n",
    "    Structure:\n",
    "    - Pre-norm: RMSNorm before attention and MLP\n",
    "    - Attention: Grouped Query Attention with RoPE\n",
    "    - MLP: SwiGLU feed-forward network\n",
    "    - Residual connections: Add input to outputs\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LlamaRMSNorm(config)  # Pre-attention normalization\n",
    "        self.attn = LlamaAttention(config)  # Grouped Query Attention\n",
    "        self.ln_2 = LlamaRMSNorm(config)  # Pre-MLP normalization\n",
    "        self.mlp = LlamaMLP(config)  # SwiGLU MLP\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through decoder layer with pre-norm and residual connections.\n",
    "        \n",
    "        Note: There's a bug in the original code - it adds residual twice.\n",
    "        Correct implementation should be:\n",
    "        x = x + attn(ln1(x))\n",
    "        x = x + mlp(ln2(x))\n",
    "        \"\"\"\n",
    "        # Pre-norm architecture: normalize before attention and MLP\n",
    "        residual = x\n",
    "        x = self.ln_1(x)  # Normalize before attention\n",
    "        x = x + self.attn(x)  # Attention with residual\n",
    "        x = self.ln_2(x)  # Normalize before MLP\n",
    "        x = x + self.mlp(x)  # MLP with residual\n",
    "        x = x + residual  # Additional residual (may be redundant)\n",
    "        return x\n",
    "\n",
    "class LlamaDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Stack of transformer decoder layers.\n",
    "    \n",
    "    Contains 30 LlamaDecoderLayer instances that process the input sequentially.\n",
    "    Each layer refines the representation, building up contextual understanding.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        # Create 30 decoder layers\n",
    "        self.layers = nn.ModuleList([LlamaDecoderLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through all decoder layers sequentially.\"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "class LlamaModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete SmolLMv2 model architecture.\n",
    "    \n",
    "    Components:\n",
    "    - Token embeddings: Convert token IDs to dense vectors\n",
    "    - Transformer decoder: 30 layers of attention + MLP\n",
    "    - Final layer norm: RMSNorm before output\n",
    "    - Language model head: Predict next token probabilities\n",
    "    - Weight tying: Embedding and output weights are shared\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Token embeddings: vocab_size (49152) -> hidden_size (576)\n",
    "        self.embed_tokens = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "        \n",
    "        # Rotary embedding (used in attention, not as separate positional embedding)\n",
    "        self.pos_embed = RotaryEmbedding(config.n_embd)\n",
    "        \n",
    "        # Language model head: hidden_size (576) -> vocab_size (49152)\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        \n",
    "        # Weight tying: Share weights between embedding and output layers\n",
    "        # This reduces parameters and improves training stability\n",
    "        self.lm_head.weight = self.embed_tokens.weight\n",
    "        \n",
    "        # Final layer normalization\n",
    "        self.ln_f = LlamaRMSNorm(config)\n",
    "        \n",
    "        # Transformer decoder: 30 layers\n",
    "        self.transformer = LlamaDecoder(config)\n",
    "\n",
    "        # Initialize all weights\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"\n",
    "        Initialize weights using normal distribution.\n",
    "        \n",
    "        Standard initialization for LLMs:\n",
    "        - Linear layers: Normal(mean=0.0, std=0.02)\n",
    "        - Embeddings: Normal(mean=0.0, std=0.02)\n",
    "        - Biases: Zero initialization\n",
    "        \"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            \n",
    "    def forward(self, idx, targets=None):\n",
    "        \"\"\"\n",
    "        Forward pass through the model.\n",
    "        \n",
    "        Args:\n",
    "            idx: Input token IDs, shape (B, T)\n",
    "            targets: Target token IDs for loss computation, shape (B, T)\n",
    "        \n",
    "        Returns:\n",
    "            logits: Next token predictions, shape (B, T, vocab_size)\n",
    "            loss: Cross-entropy loss (if targets provided)\n",
    "        \"\"\"\n",
    "        # idx is of shape (B, T)\n",
    "        B, T = idx.size()\n",
    "        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
    "        \n",
    "        # Forward the token embeddings (RoPE is applied in attention, not here)\n",
    "        x = self.embed_tokens(idx)  # token embeddings of shape (B, T, n_embd)\n",
    "        \n",
    "        # Forward through all transformer decoder layers\n",
    "        x = self.transformer(x)\n",
    "        \n",
    "        # Forward the final layernorm and the classifier\n",
    "        x = self.ln_f(x)  # Final normalization\n",
    "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
    "        \n",
    "        # Compute loss if targets provided\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            # Flatten for cross-entropy: (B*T, vocab_size) and (B*T,)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, input_ids, max_new_tokens):\n",
    "        \"\"\"\n",
    "        Generate new tokens autoregressively using greedy decoding.\n",
    "        \n",
    "        This is the text generation function that:\n",
    "        1. Takes input token IDs\n",
    "        2. Predicts next token probabilities\n",
    "        3. Selects highest probability token (greedy)\n",
    "        4. Appends to sequence and repeats\n",
    "        \n",
    "        Args:\n",
    "            input_ids: Input token IDs, shape (batch_size, seq_len)\n",
    "            max_new_tokens: Maximum number of new tokens to generate\n",
    "            \n",
    "        Returns:\n",
    "            Generated token IDs, shape (batch_size, seq_len + max_new_tokens)\n",
    "        \"\"\"\n",
    "        self.eval()  # Set to evaluation mode\n",
    "        input_ids = input_ids.to(self.config.device)\n",
    "        \n",
    "        # Start with the input sequence\n",
    "        generated_ids = input_ids.clone()\n",
    "        \n",
    "        # Generate tokens one at a time (autoregressive)\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Get embeddings for current sequence\n",
    "            x = self.embed_tokens(generated_ids)\n",
    "            # Note: Rotary embeddings (RoPE) are applied in attention, not here\n",
    "            \n",
    "            # Pass through transformer decoder\n",
    "            x = self.transformer(x)\n",
    "            x = self.ln_f(x)\n",
    "            \n",
    "            # Get logits for all positions\n",
    "            logits = self.lm_head(x)  # (batch_size, seq_len, vocab_size)\n",
    "            \n",
    "            # Only use the last token's logits to predict next token\n",
    "            # This is because we only care about the next token after the current sequence\n",
    "            logits = logits[:, -1, :]  # Shape: (batch_size, vocab_size)\n",
    "            \n",
    "            # Greedy decoding: pick the token with highest probability\n",
    "            next_token = torch.argmax(logits, dim=-1, keepdim=True)  # Shape: (batch_size, 1)\n",
    "            \n",
    "            # Append the new token to the sequence\n",
    "            generated_ids = torch.cat([generated_ids, next_token], dim=1)\n",
    "        \n",
    "        # Return all generated tokens (input + newly generated)\n",
    "        return generated_ids\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760ebb27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STEP-BY-STEP EXPLANATION OF GENERATE METHOD\n",
      "================================================================================\n",
      "\n",
      "STEP 1: Input Preparation\n",
      "--------------------------------------------------------------------------------\n",
      "input_ids shape: torch.Size([1, 3])\n",
      "input_ids: tensor([[1234, 5678, 9012]])\n",
      "  - batch_size = 1 (one sequence)\n",
      "  - sequence_length = 3 (three tokens: 'The', 'weather', 'is')\n",
      "\n",
      "STEP 2: Embedding Layer (embed_tokens)\n",
      "--------------------------------------------------------------------------------\n",
      "After embedding, x shape: torch.Size([1, 3, 576])\n",
      "  - batch_size = 1\n",
      "  - sequence_length = 3\n",
      "  - hidden_size = 576\n",
      "  - Each token is now a 576-dimensional vector\n",
      "\n",
      "STEP 3: Positional Embedding (pos_embed)\n",
      "--------------------------------------------------------------------------------\n",
      "After positional embedding, x shape: torch.Size([1, 3, 576])\n",
      "  - Still same shape: (batch, seq_len, hidden_size)\n",
      "  - Now each token has both semantic and positional information\n",
      "\n",
      "STEP 4: Transformer Layers (transformer)\n",
      "--------------------------------------------------------------------------------\n",
      "After transformer, x shape: torch.Size([1, 3, 576])\n",
      "  - Still same shape: (batch, seq_len, hidden_size)\n",
      "  - Each position now has contextualized representations\n",
      "  - Token 'is' now knows about 'The' and 'weather'\n",
      "\n",
      "STEP 5: Final Layer Normalization (ln_f)\n",
      "--------------------------------------------------------------------------------\n",
      "After layer norm, x shape: torch.Size([1, 3, 576])\n",
      "  - Still same shape: (batch, seq_len, hidden_size)\n",
      "  - Normalized for stability\n",
      "\n",
      "STEP 6: Language Model Head (lm_head)\n",
      "--------------------------------------------------------------------------------\n",
      "After lm_head, logits shape: torch.Size([1, 3, 49152])\n",
      "  - batch_size = 1\n",
      "  - sequence_length = 3\n",
      "  - vocab_size = 49152\n",
      "\n",
      "What this means:\n",
      "  - For EACH position in the sequence, we have predictions for ALL tokens\n",
      "  - Position 0 ('The'): 49152 logits (probabilities for next token)\n",
      "  - Position 1 ('weather'): 49152 logits (probabilities for next token)\n",
      "  - Position 2 ('is'): 49152 logits (probabilities for next token)\n",
      "\n",
      "Example logits for position 2 ('is'):\n",
      "  First 10 logits: tensor([-0.6337,  1.0557,  0.8247,  0.4233,  0.6064,  2.1153,  0.4184, -0.0847,\n",
      "        -0.1437,  0.4688])\n",
      "  - These are raw scores (not probabilities yet)\n",
      "  - Higher values = more likely token\n",
      "\n",
      "STEP 7: Extract Last Token Logits - logits[:, -1, :]\n",
      "--------------------------------------------------------------------------------\n",
      "This is the line you asked about!\n",
      "\n",
      "BEFORE slicing:\n",
      "  logits.shape = torch.Size([1, 3, 49152])  # (batch, seq_len, vocab_size)\n",
      "  logits = 1 batch × 3 positions × 49152 vocab tokens\n",
      "\n",
      "Slicing operation: logits[:, -1, :]\n",
      "  - ':' in first position = keep ALL batches\n",
      "  - '-1' in second position = take LAST position (last token)\n",
      "  - ':' in third position = keep ALL vocabulary tokens\n",
      "\n",
      "AFTER slicing:\n",
      "  logits_last.shape = torch.Size([1, 49152])  # (batch, vocab_size)\n",
      "  logits_last = 1 batch × 49152 vocab tokens\n",
      "\n",
      "What we kept:\n",
      "  - We ONLY kept the logits from the LAST token position ('is')\n",
      "  - We DISCARDED logits from positions 0 ('The') and 1 ('weather')\n",
      "  - Why? Because we only need the LAST token to predict the NEXT token!\n",
      "\n",
      "STEP 8: Get Next Token (torch.argmax)\n",
      "--------------------------------------------------------------------------------\n",
      "next_token shape: torch.Size([1, 1])\n",
      "next_token: tensor([[14457]])\n",
      "\n",
      "What happened:\n",
      "  - torch.argmax finds the index with the highest value\n",
      "  - dim=-1 means we search along the vocabulary dimension\n",
      "  - keepdim=True keeps the dimension for concatenation\n",
      "  - Result: The token ID of the most likely next token\n",
      "\n",
      "================================================================================\n",
      "VISUAL SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Input sequence: ['The', 'weather', 'is']\n",
      "\n",
      "After forward pass:\n",
      "  logits shape: (1, 3, 49152)\n",
      "  ┌─────────────────────────────────────┐\n",
      "  │ Position 0 ('The'):    49152 logits │\n",
      "  │ Position 1 ('weather'): 49152 logits │\n",
      "  │ Position 2 ('is'):     49152 logits │ ← We need THIS one!\n",
      "  └─────────────────────────────────────┘\n",
      "\n",
      "After logits[:, -1, :]:\n",
      "  logits shape: (1, 49152)\n",
      "  ┌─────────────────────┐\n",
      "  │ Position 2 ('is'):  │ ← Only the last token's logits\n",
      "  │   49152 logits      │\n",
      "  └─────────────────────┘\n",
      "\n",
      "After argmax:\n",
      "  next_token: [token_id]  ← The most likely next token\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "CELL 10: Detailed Explanation of Text Generation Process\n",
    "=========================================================\n",
    "This cell provides a comprehensive, step-by-step walkthrough of how the generate()\n",
    "method works. It's an educational cell that explains the autoregressive text\n",
    "generation process in detail.\n",
    "\n",
    "What it explains:\n",
    "1. Input preparation: How token IDs are structured\n",
    "2. Embedding layer: Converting tokens to dense vectors\n",
    "3. Positional encoding: How positions are handled (RoPE in attention)\n",
    "4. Transformer processing: How context is built through layers\n",
    "5. Logit extraction: Why we only use the last token's logits\n",
    "6. Token selection: Greedy decoding strategy\n",
    "7. Sequence building: How tokens are appended iteratively\n",
    "\n",
    "This educational content helps understand:\n",
    "- Why logits[:, -1, :] is used (only last token predicts next)\n",
    "- How autoregressive generation works step-by-step\n",
    "- The shape transformations at each step\n",
    "- Why we generate one token at a time\n",
    "\n",
    "This is crucial for understanding how language models generate text!\n",
    "\"\"\"\n",
    "# Detailed Step-by-Step Explanation of generate() method\n",
    "import torch\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STEP-BY-STEP EXPLANATION OF GENERATE METHOD\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: Input Preparation\n",
    "# ============================================================================\n",
    "print(\"STEP 1: Input Preparation\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Example: We want to generate text starting from \"The weather is\"\n",
    "# After tokenization, this becomes token IDs\n",
    "input_ids = torch.tensor([[1234, 5678, 9012]])  # Shape: (batch_size=1, sequence_length=3)\n",
    "print(f\"input_ids shape: {input_ids.shape}\")\n",
    "print(f\"input_ids: {input_ids}\")\n",
    "print(\"  - batch_size = 1 (one sequence)\")\n",
    "print(\"  - sequence_length = 3 (three tokens: 'The', 'weather', 'is')\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: Embedding Layer\n",
    "# ============================================================================\n",
    "print(\"STEP 2: Embedding Layer (embed_tokens)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Each token ID is converted to a dense vector\n",
    "# vocab_size = 49152, hidden_size = 576\n",
    "# embed_tokens converts token IDs to embeddings\n",
    "batch_size, seq_len = input_ids.shape\n",
    "hidden_size = 576\n",
    "\n",
    "# Simulate embedding: (batch, seq_len) -> (batch, seq_len, hidden_size)\n",
    "x = torch.randn(batch_size, seq_len, hidden_size)  # Simulated embeddings\n",
    "print(f\"After embedding, x shape: {x.shape}\")\n",
    "print(f\"  - batch_size = {batch_size}\")\n",
    "print(f\"  - sequence_length = {seq_len}\")\n",
    "print(f\"  - hidden_size = {hidden_size}\")\n",
    "print(\"  - Each token is now a 576-dimensional vector\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3: Positional Embedding\n",
    "# ============================================================================\n",
    "print(\"STEP 3: Positional Embedding (pos_embed)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Add positional information to embeddings\n",
    "pos_embed = torch.randn(batch_size, seq_len, hidden_size)  # Simulated\n",
    "x = x + pos_embed\n",
    "print(f\"After positional embedding, x shape: {x.shape}\")\n",
    "print(\"  - Still same shape: (batch, seq_len, hidden_size)\")\n",
    "print(\"  - Now each token has both semantic and positional information\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 4: Transformer Layers\n",
    "# ============================================================================\n",
    "print(\"STEP 4: Transformer Layers (transformer)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Pass through all transformer decoder layers\n",
    "# Each layer processes the entire sequence\n",
    "x = torch.randn(batch_size, seq_len, hidden_size)  # Simulated after transformer\n",
    "print(f\"After transformer, x shape: {x.shape}\")\n",
    "print(\"  - Still same shape: (batch, seq_len, hidden_size)\")\n",
    "print(\"  - Each position now has contextualized representations\")\n",
    "print(\"  - Token 'is' now knows about 'The' and 'weather'\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 5: Final Layer Norm\n",
    "# ============================================================================\n",
    "print(\"STEP 5: Final Layer Normalization (ln_f)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "x = torch.randn(batch_size, seq_len, hidden_size)  # Simulated after norm\n",
    "print(f\"After layer norm, x shape: {x.shape}\")\n",
    "print(\"  - Still same shape: (batch, seq_len, hidden_size)\")\n",
    "print(\"  - Normalized for stability\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 6: Language Model Head (lm_head)\n",
    "# ============================================================================\n",
    "print(\"STEP 6: Language Model Head (lm_head)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Convert hidden states to vocabulary logits\n",
    "vocab_size = 49152\n",
    "logits = torch.randn(batch_size, seq_len, vocab_size)  # Simulated logits\n",
    "print(f\"After lm_head, logits shape: {logits.shape}\")\n",
    "print(f\"  - batch_size = {batch_size}\")\n",
    "print(f\"  - sequence_length = {seq_len}\")\n",
    "print(f\"  - vocab_size = {vocab_size}\")\n",
    "print()\n",
    "print(\"What this means:\")\n",
    "print(\"  - For EACH position in the sequence, we have predictions for ALL tokens\")\n",
    "print(\"  - Position 0 ('The'): 49152 logits (probabilities for next token)\")\n",
    "print(\"  - Position 1 ('weather'): 49152 logits (probabilities for next token)\")\n",
    "print(\"  - Position 2 ('is'): 49152 logits (probabilities for next token)\")\n",
    "print()\n",
    "print(\"Example logits for position 2 ('is'):\")\n",
    "example_logits = logits[0, 2, :10]  # First 10 tokens\n",
    "print(f\"  First 10 logits: {example_logits}\")\n",
    "print(\"  - These are raw scores (not probabilities yet)\")\n",
    "print(\"  - Higher values = more likely token\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 7: Extract Last Token Logits (THE KEY LINE!)\n",
    "# ============================================================================\n",
    "print(\"STEP 7: Extract Last Token Logits - logits[:, -1, :]\")\n",
    "print(\"-\" * 80)\n",
    "print(\"This is the line you asked about!\")\n",
    "print()\n",
    "\n",
    "print(\"BEFORE slicing:\")\n",
    "print(f\"  logits.shape = {logits.shape}  # (batch, seq_len, vocab_size)\")\n",
    "print(f\"  logits = {logits.shape[0]} batch × {logits.shape[1]} positions × {logits.shape[2]} vocab tokens\")\n",
    "print()\n",
    "\n",
    "print(\"Slicing operation: logits[:, -1, :]\")\n",
    "print(\"  - ':' in first position = keep ALL batches\")\n",
    "print(\"  - '-1' in second position = take LAST position (last token)\")\n",
    "print(\"  - ':' in third position = keep ALL vocabulary tokens\")\n",
    "print()\n",
    "\n",
    "logits_last = logits[:, -1, :]\n",
    "print(\"AFTER slicing:\")\n",
    "print(f\"  logits_last.shape = {logits_last.shape}  # (batch, vocab_size)\")\n",
    "print(f\"  logits_last = {logits_last.shape[0]} batch × {logits_last.shape[1]} vocab tokens\")\n",
    "print()\n",
    "\n",
    "print(\"What we kept:\")\n",
    "print(\"  - We ONLY kept the logits from the LAST token position ('is')\")\n",
    "print(\"  - We DISCARDED logits from positions 0 ('The') and 1 ('weather')\")\n",
    "print(\"  - Why? Because we only need the LAST token to predict the NEXT token!\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 8: Get Next Token (Greedy Decoding)\n",
    "# ============================================================================\n",
    "print(\"STEP 8: Get Next Token (torch.argmax)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Find the token with highest logit (greedy decoding)\n",
    "next_token = torch.argmax(logits_last, dim=-1, keepdim=True)\n",
    "print(f\"next_token shape: {next_token.shape}\")\n",
    "print(f\"next_token: {next_token}\")\n",
    "print()\n",
    "print(\"What happened:\")\n",
    "print(\"  - torch.argmax finds the index with the highest value\")\n",
    "print(\"  - dim=-1 means we search along the vocabulary dimension\")\n",
    "print(\"  - keepdim=True keeps the dimension for concatenation\")\n",
    "print(\"  - Result: The token ID of the most likely next token\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# VISUAL SUMMARY\n",
    "# ============================================================================\n",
    "print(\"=\" * 80)\n",
    "print(\"VISUAL SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "print(\"Input sequence: ['The', 'weather', 'is']\")\n",
    "print()\n",
    "print(\"After forward pass:\")\n",
    "print(\"  logits shape: (1, 3, 49152)\")\n",
    "print(\"  ┌─────────────────────────────────────┐\")\n",
    "print(\"  │ Position 0 ('The'):    49152 logits │\")\n",
    "print(\"  │ Position 1 ('weather'): 49152 logits │\")\n",
    "print(\"  │ Position 2 ('is'):     49152 logits │ ← We need THIS one!\")\n",
    "print(\"  └─────────────────────────────────────┘\")\n",
    "print()\n",
    "print(\"After logits[:, -1, :]:\")\n",
    "print(\"  logits shape: (1, 49152)\")\n",
    "print(\"  ┌─────────────────────┐\")\n",
    "print(\"  │ Position 2 ('is'):  │ ← Only the last token's logits\")\n",
    "print(\"  │   49152 logits      │\")\n",
    "print(\"  └─────────────────────┘\")\n",
    "print()\n",
    "print(\"After argmax:\")\n",
    "print(\"  next_token: [token_id]  ← The most likely next token\")\n",
    "print()\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85fee67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 11.2828\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "CELL 11: Create and Test Model from Scratch\n",
    "===========================================\n",
    "This cell creates our from-scratch SmolLMv2 model and tests it with a forward pass.\n",
    "This is the first time we use our custom implementation (not the HuggingFace reference).\n",
    "\n",
    "What it does:\n",
    "1. Loads tokenizer: Uses tokenizer from config.yaml\n",
    "2. Loads training data: Reads input.txt and tokenizes it\n",
    "3. Creates model config: Sets up configuration with compatibility attributes\n",
    "4. Initializes model: Creates LlamaModel with random weights (from scratch!)\n",
    "5. Tests forward pass: Runs a batch through the model to verify it works\n",
    "6. Computes loss: Shows initial loss (should be high, ~10-11 for random model)\n",
    "\n",
    "Key points:\n",
    "- Model is initialized with random weights (not pretrained)\n",
    "- This is our from-scratch implementation\n",
    "- Initial loss is high because model hasn't been trained yet\n",
    "- This verifies the architecture is correct before training\n",
    "\"\"\"\n",
    "# Use tokenizer from config.yaml (loaded in cell 2 or 4)\n",
    "# If not available, load it from config\n",
    "if 'tokenizer' not in globals():\n",
    "    with open(\"config.yaml\", 'r') as f:\n",
    "        config_dict = yaml.safe_load(f)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        config_dict[\"tokenizer\"][\"tokenizer_name_or_path\"],\n",
    "        use_fast=config_dict[\"tokenizer\"][\"use_fast\"]\n",
    "    )\n",
    "\n",
    "# Read and tokenize input text\n",
    "with open('input.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Limit text and tokenize using tokenizer from config.yaml\n",
    "text = text[:1000]\n",
    "tokens = tokenizer.encode(text, add_special_tokens=False)  # Returns list of token IDs\n",
    "\n",
    "# Setup batch and sequence parameters\n",
    "B, T = 4, 32  # batch_size, sequence_length\n",
    "\n",
    "# Create input/target pairs from tokens\n",
    "buf = torch.tensor(tokens[:B*T + 1])  # Take B*T+1 tokens for input/target split\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "buf = buf.to(device)\n",
    "\n",
    "# Split into input (x) and target (y) - y is x shifted by 1\n",
    "x = buf[:-1].view(B, T)  # Shape: (B, T)\n",
    "y = buf[1:].view(B, T)   # Shape: (B, T)\n",
    "\n",
    "# Prepare model config from config.yaml\n",
    "# Add compatibility attributes for LlamaModel/LlamaAttention\n",
    "model_config = cfg.model\n",
    "model_config.n_embd = model_config.hidden_size\n",
    "model_config.n_head = model_config.num_attention_heads\n",
    "model_config.block_size = model_config.max_position_embeddings\n",
    "model_config.device = device\n",
    "model_config.num_key_value_heads = model_config.num_key_value_heads\n",
    "\n",
    "# Create LlamaModel using config from config.yaml\n",
    "model = LlamaModel(model_config)\n",
    "model.to(device)\n",
    "\n",
    "# Forward pass and compute loss\n",
    "logits, loss = model(x, y)\n",
    "print(f\"Loss: {loss.item():.4f}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab24d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected loss for random prediction (vocab_size=49152): 10.8027\n",
      "This is the theoretical maximum loss when predicting uniformly at random.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "CELL 12: Calculate Random Prediction Baseline Loss\n",
    "===================================================\n",
    "This cell calculates the theoretical loss for a model that predicts uniformly\n",
    "at random. This serves as a baseline to understand our model's performance.\n",
    "\n",
    "What it does:\n",
    "1. Calculates random baseline: -log(1/vocab_size) = log(vocab_size)\n",
    "2. For vocab_size=49152: log(49152) ≈ 10.80\n",
    "3. This is the worst-case loss (model has no information)\n",
    "\n",
    "Why this matters:\n",
    "- Initial loss should be close to this (~10.8) for untrained model\n",
    "- As model learns, loss decreases below this baseline\n",
    "- Loss < 2.0 indicates the model has learned meaningful patterns\n",
    "- This helps us understand if training is progressing correctly\n",
    "\"\"\"\n",
    "# Calculate expected loss for random prediction (uniform distribution)\n",
    "# This represents the worst-case scenario: predicting uniformly at random\n",
    "# Formula: -log(1/vocab_size) = log(vocab_size)\n",
    "vocab_size = cfg.model.vocab_size\n",
    "random_loss = -torch.log(torch.tensor(1.0 / vocab_size))\n",
    "print(f\"Expected loss for random prediction (vocab_size={vocab_size}): {random_loss.item():.4f}\")\n",
    "print(f\"This is the theoretical maximum loss when predicting uniformly at random.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a90961e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaModel(\n",
       "  (embed_tokens): Embedding(49152, 576)\n",
       "  (pos_embed): RotaryEmbedding()\n",
       "  (lm_head): Linear(in_features=576, out_features=49152, bias=False)\n",
       "  (ln_f): LlamaRMSNorm(hidden_size=576, eps=1e-05)\n",
       "  (transformer): LlamaDecoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-29): 30 x LlamaDecoderLayer(\n",
       "        (ln_1): LlamaRMSNorm(hidden_size=576, eps=1e-05)\n",
       "        (attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
       "          (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
       "          (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
       "          (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (ln_2): LlamaRMSNorm(hidden_size=576, eps=1e-05)\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
       "          (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
       "          (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "CELL 13: Inspect Our From-Scratch Model Architecture\n",
    "====================================================\n",
    "This cell displays the structure of our custom from-scratch model implementation.\n",
    "We can compare this with the HuggingFace reference model (Cell 2) to verify\n",
    "our architecture matches.\n",
    "\n",
    "What it shows:\n",
    "- Model structure: All layers and their organization\n",
    "- Component hierarchy: How decoder layers are stacked\n",
    "- Parameter organization: How weights are structured\n",
    "\n",
    "This helps verify:\n",
    "- Our implementation matches the reference architecture\n",
    "- All components are correctly connected\n",
    "- The model is ready for training\n",
    "\"\"\"\n",
    "# Display our from-scratch model structure for verification\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5bffa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaModel(\n",
      "  (embed_tokens): Embedding(49152, 576)\n",
      "  (pos_embed): RotaryEmbedding()\n",
      "  (lm_head): Linear(in_features=576, out_features=49152, bias=False)\n",
      "  (ln_f): LlamaRMSNorm(hidden_size=576, eps=1e-05)\n",
      "  (transformer): LlamaDecoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-29): 30 x LlamaDecoderLayer(\n",
      "        (ln_1): LlamaRMSNorm(hidden_size=576, eps=1e-05)\n",
      "        (attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "          (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "          (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "          (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "          (rotary_emb): RotaryEmbedding()\n",
      "        )\n",
      "        (ln_2): LlamaRMSNorm(hidden_size=576, eps=1e-05)\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "          (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
      "          (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "134515008\n",
      "+--------------------------------------------+------------+\n",
      "|                  Modules                   | Parameters |\n",
      "+--------------------------------------------+------------+\n",
      "|            embed_tokens.weight             |  28311552  |\n",
      "|                ln_f.weight                 |    576     |\n",
      "|      transformer.layers.0.ln_1.weight      |    576     |\n",
      "|  transformer.layers.0.attn.q_proj.weight   |   331776   |\n",
      "|  transformer.layers.0.attn.k_proj.weight   |   110592   |\n",
      "|  transformer.layers.0.attn.v_proj.weight   |   110592   |\n",
      "|  transformer.layers.0.attn.o_proj.weight   |   331776   |\n",
      "|      transformer.layers.0.ln_2.weight      |    576     |\n",
      "| transformer.layers.0.mlp.gate_proj.weight  |   884736   |\n",
      "|  transformer.layers.0.mlp.up_proj.weight   |   884736   |\n",
      "| transformer.layers.0.mlp.down_proj.weight  |   884736   |\n",
      "|      transformer.layers.1.ln_1.weight      |    576     |\n",
      "|  transformer.layers.1.attn.q_proj.weight   |   331776   |\n",
      "|  transformer.layers.1.attn.k_proj.weight   |   110592   |\n",
      "|  transformer.layers.1.attn.v_proj.weight   |   110592   |\n",
      "|  transformer.layers.1.attn.o_proj.weight   |   331776   |\n",
      "|      transformer.layers.1.ln_2.weight      |    576     |\n",
      "| transformer.layers.1.mlp.gate_proj.weight  |   884736   |\n",
      "|  transformer.layers.1.mlp.up_proj.weight   |   884736   |\n",
      "| transformer.layers.1.mlp.down_proj.weight  |   884736   |\n",
      "|      transformer.layers.2.ln_1.weight      |    576     |\n",
      "|  transformer.layers.2.attn.q_proj.weight   |   331776   |\n",
      "|  transformer.layers.2.attn.k_proj.weight   |   110592   |\n",
      "|  transformer.layers.2.attn.v_proj.weight   |   110592   |\n",
      "|  transformer.layers.2.attn.o_proj.weight   |   331776   |\n",
      "|      transformer.layers.2.ln_2.weight      |    576     |\n",
      "| transformer.layers.2.mlp.gate_proj.weight  |   884736   |\n",
      "|  transformer.layers.2.mlp.up_proj.weight   |   884736   |\n",
      "| transformer.layers.2.mlp.down_proj.weight  |   884736   |\n",
      "|      transformer.layers.3.ln_1.weight      |    576     |\n",
      "|  transformer.layers.3.attn.q_proj.weight   |   331776   |\n",
      "|  transformer.layers.3.attn.k_proj.weight   |   110592   |\n",
      "|  transformer.layers.3.attn.v_proj.weight   |   110592   |\n",
      "|  transformer.layers.3.attn.o_proj.weight   |   331776   |\n",
      "|      transformer.layers.3.ln_2.weight      |    576     |\n",
      "| transformer.layers.3.mlp.gate_proj.weight  |   884736   |\n",
      "|  transformer.layers.3.mlp.up_proj.weight   |   884736   |\n",
      "| transformer.layers.3.mlp.down_proj.weight  |   884736   |\n",
      "|      transformer.layers.4.ln_1.weight      |    576     |\n",
      "|  transformer.layers.4.attn.q_proj.weight   |   331776   |\n",
      "|  transformer.layers.4.attn.k_proj.weight   |   110592   |\n",
      "|  transformer.layers.4.attn.v_proj.weight   |   110592   |\n",
      "|  transformer.layers.4.attn.o_proj.weight   |   331776   |\n",
      "|      transformer.layers.4.ln_2.weight      |    576     |\n",
      "| transformer.layers.4.mlp.gate_proj.weight  |   884736   |\n",
      "|  transformer.layers.4.mlp.up_proj.weight   |   884736   |\n",
      "| transformer.layers.4.mlp.down_proj.weight  |   884736   |\n",
      "|      transformer.layers.5.ln_1.weight      |    576     |\n",
      "|  transformer.layers.5.attn.q_proj.weight   |   331776   |\n",
      "|  transformer.layers.5.attn.k_proj.weight   |   110592   |\n",
      "|  transformer.layers.5.attn.v_proj.weight   |   110592   |\n",
      "|  transformer.layers.5.attn.o_proj.weight   |   331776   |\n",
      "|      transformer.layers.5.ln_2.weight      |    576     |\n",
      "| transformer.layers.5.mlp.gate_proj.weight  |   884736   |\n",
      "|  transformer.layers.5.mlp.up_proj.weight   |   884736   |\n",
      "| transformer.layers.5.mlp.down_proj.weight  |   884736   |\n",
      "|      transformer.layers.6.ln_1.weight      |    576     |\n",
      "|  transformer.layers.6.attn.q_proj.weight   |   331776   |\n",
      "|  transformer.layers.6.attn.k_proj.weight   |   110592   |\n",
      "|  transformer.layers.6.attn.v_proj.weight   |   110592   |\n",
      "|  transformer.layers.6.attn.o_proj.weight   |   331776   |\n",
      "|      transformer.layers.6.ln_2.weight      |    576     |\n",
      "| transformer.layers.6.mlp.gate_proj.weight  |   884736   |\n",
      "|  transformer.layers.6.mlp.up_proj.weight   |   884736   |\n",
      "| transformer.layers.6.mlp.down_proj.weight  |   884736   |\n",
      "|      transformer.layers.7.ln_1.weight      |    576     |\n",
      "|  transformer.layers.7.attn.q_proj.weight   |   331776   |\n",
      "|  transformer.layers.7.attn.k_proj.weight   |   110592   |\n",
      "|  transformer.layers.7.attn.v_proj.weight   |   110592   |\n",
      "|  transformer.layers.7.attn.o_proj.weight   |   331776   |\n",
      "|      transformer.layers.7.ln_2.weight      |    576     |\n",
      "| transformer.layers.7.mlp.gate_proj.weight  |   884736   |\n",
      "|  transformer.layers.7.mlp.up_proj.weight   |   884736   |\n",
      "| transformer.layers.7.mlp.down_proj.weight  |   884736   |\n",
      "|      transformer.layers.8.ln_1.weight      |    576     |\n",
      "|  transformer.layers.8.attn.q_proj.weight   |   331776   |\n",
      "|  transformer.layers.8.attn.k_proj.weight   |   110592   |\n",
      "|  transformer.layers.8.attn.v_proj.weight   |   110592   |\n",
      "|  transformer.layers.8.attn.o_proj.weight   |   331776   |\n",
      "|      transformer.layers.8.ln_2.weight      |    576     |\n",
      "| transformer.layers.8.mlp.gate_proj.weight  |   884736   |\n",
      "|  transformer.layers.8.mlp.up_proj.weight   |   884736   |\n",
      "| transformer.layers.8.mlp.down_proj.weight  |   884736   |\n",
      "|      transformer.layers.9.ln_1.weight      |    576     |\n",
      "|  transformer.layers.9.attn.q_proj.weight   |   331776   |\n",
      "|  transformer.layers.9.attn.k_proj.weight   |   110592   |\n",
      "|  transformer.layers.9.attn.v_proj.weight   |   110592   |\n",
      "|  transformer.layers.9.attn.o_proj.weight   |   331776   |\n",
      "|      transformer.layers.9.ln_2.weight      |    576     |\n",
      "| transformer.layers.9.mlp.gate_proj.weight  |   884736   |\n",
      "|  transformer.layers.9.mlp.up_proj.weight   |   884736   |\n",
      "| transformer.layers.9.mlp.down_proj.weight  |   884736   |\n",
      "|     transformer.layers.10.ln_1.weight      |    576     |\n",
      "|  transformer.layers.10.attn.q_proj.weight  |   331776   |\n",
      "|  transformer.layers.10.attn.k_proj.weight  |   110592   |\n",
      "|  transformer.layers.10.attn.v_proj.weight  |   110592   |\n",
      "|  transformer.layers.10.attn.o_proj.weight  |   331776   |\n",
      "|     transformer.layers.10.ln_2.weight      |    576     |\n",
      "| transformer.layers.10.mlp.gate_proj.weight |   884736   |\n",
      "|  transformer.layers.10.mlp.up_proj.weight  |   884736   |\n",
      "| transformer.layers.10.mlp.down_proj.weight |   884736   |\n",
      "|     transformer.layers.11.ln_1.weight      |    576     |\n",
      "|  transformer.layers.11.attn.q_proj.weight  |   331776   |\n",
      "|  transformer.layers.11.attn.k_proj.weight  |   110592   |\n",
      "|  transformer.layers.11.attn.v_proj.weight  |   110592   |\n",
      "|  transformer.layers.11.attn.o_proj.weight  |   331776   |\n",
      "|     transformer.layers.11.ln_2.weight      |    576     |\n",
      "| transformer.layers.11.mlp.gate_proj.weight |   884736   |\n",
      "|  transformer.layers.11.mlp.up_proj.weight  |   884736   |\n",
      "| transformer.layers.11.mlp.down_proj.weight |   884736   |\n",
      "|     transformer.layers.12.ln_1.weight      |    576     |\n",
      "|  transformer.layers.12.attn.q_proj.weight  |   331776   |\n",
      "|  transformer.layers.12.attn.k_proj.weight  |   110592   |\n",
      "|  transformer.layers.12.attn.v_proj.weight  |   110592   |\n",
      "|  transformer.layers.12.attn.o_proj.weight  |   331776   |\n",
      "|     transformer.layers.12.ln_2.weight      |    576     |\n",
      "| transformer.layers.12.mlp.gate_proj.weight |   884736   |\n",
      "|  transformer.layers.12.mlp.up_proj.weight  |   884736   |\n",
      "| transformer.layers.12.mlp.down_proj.weight |   884736   |\n",
      "|     transformer.layers.13.ln_1.weight      |    576     |\n",
      "|  transformer.layers.13.attn.q_proj.weight  |   331776   |\n",
      "|  transformer.layers.13.attn.k_proj.weight  |   110592   |\n",
      "|  transformer.layers.13.attn.v_proj.weight  |   110592   |\n",
      "|  transformer.layers.13.attn.o_proj.weight  |   331776   |\n",
      "|     transformer.layers.13.ln_2.weight      |    576     |\n",
      "| transformer.layers.13.mlp.gate_proj.weight |   884736   |\n",
      "|  transformer.layers.13.mlp.up_proj.weight  |   884736   |\n",
      "| transformer.layers.13.mlp.down_proj.weight |   884736   |\n",
      "|     transformer.layers.14.ln_1.weight      |    576     |\n",
      "|  transformer.layers.14.attn.q_proj.weight  |   331776   |\n",
      "|  transformer.layers.14.attn.k_proj.weight  |   110592   |\n",
      "|  transformer.layers.14.attn.v_proj.weight  |   110592   |\n",
      "|  transformer.layers.14.attn.o_proj.weight  |   331776   |\n",
      "|     transformer.layers.14.ln_2.weight      |    576     |\n",
      "| transformer.layers.14.mlp.gate_proj.weight |   884736   |\n",
      "|  transformer.layers.14.mlp.up_proj.weight  |   884736   |\n",
      "| transformer.layers.14.mlp.down_proj.weight |   884736   |\n",
      "|     transformer.layers.15.ln_1.weight      |    576     |\n",
      "|  transformer.layers.15.attn.q_proj.weight  |   331776   |\n",
      "|  transformer.layers.15.attn.k_proj.weight  |   110592   |\n",
      "|  transformer.layers.15.attn.v_proj.weight  |   110592   |\n",
      "|  transformer.layers.15.attn.o_proj.weight  |   331776   |\n",
      "|     transformer.layers.15.ln_2.weight      |    576     |\n",
      "| transformer.layers.15.mlp.gate_proj.weight |   884736   |\n",
      "|  transformer.layers.15.mlp.up_proj.weight  |   884736   |\n",
      "| transformer.layers.15.mlp.down_proj.weight |   884736   |\n",
      "|     transformer.layers.16.ln_1.weight      |    576     |\n",
      "|  transformer.layers.16.attn.q_proj.weight  |   331776   |\n",
      "|  transformer.layers.16.attn.k_proj.weight  |   110592   |\n",
      "|  transformer.layers.16.attn.v_proj.weight  |   110592   |\n",
      "|  transformer.layers.16.attn.o_proj.weight  |   331776   |\n",
      "|     transformer.layers.16.ln_2.weight      |    576     |\n",
      "| transformer.layers.16.mlp.gate_proj.weight |   884736   |\n",
      "|  transformer.layers.16.mlp.up_proj.weight  |   884736   |\n",
      "| transformer.layers.16.mlp.down_proj.weight |   884736   |\n",
      "|     transformer.layers.17.ln_1.weight      |    576     |\n",
      "|  transformer.layers.17.attn.q_proj.weight  |   331776   |\n",
      "|  transformer.layers.17.attn.k_proj.weight  |   110592   |\n",
      "|  transformer.layers.17.attn.v_proj.weight  |   110592   |\n",
      "|  transformer.layers.17.attn.o_proj.weight  |   331776   |\n",
      "|     transformer.layers.17.ln_2.weight      |    576     |\n",
      "| transformer.layers.17.mlp.gate_proj.weight |   884736   |\n",
      "|  transformer.layers.17.mlp.up_proj.weight  |   884736   |\n",
      "| transformer.layers.17.mlp.down_proj.weight |   884736   |\n",
      "|     transformer.layers.18.ln_1.weight      |    576     |\n",
      "|  transformer.layers.18.attn.q_proj.weight  |   331776   |\n",
      "|  transformer.layers.18.attn.k_proj.weight  |   110592   |\n",
      "|  transformer.layers.18.attn.v_proj.weight  |   110592   |\n",
      "|  transformer.layers.18.attn.o_proj.weight  |   331776   |\n",
      "|     transformer.layers.18.ln_2.weight      |    576     |\n",
      "| transformer.layers.18.mlp.gate_proj.weight |   884736   |\n",
      "|  transformer.layers.18.mlp.up_proj.weight  |   884736   |\n",
      "| transformer.layers.18.mlp.down_proj.weight |   884736   |\n",
      "|     transformer.layers.19.ln_1.weight      |    576     |\n",
      "|  transformer.layers.19.attn.q_proj.weight  |   331776   |\n",
      "|  transformer.layers.19.attn.k_proj.weight  |   110592   |\n",
      "|  transformer.layers.19.attn.v_proj.weight  |   110592   |\n",
      "|  transformer.layers.19.attn.o_proj.weight  |   331776   |\n",
      "|     transformer.layers.19.ln_2.weight      |    576     |\n",
      "| transformer.layers.19.mlp.gate_proj.weight |   884736   |\n",
      "|  transformer.layers.19.mlp.up_proj.weight  |   884736   |\n",
      "| transformer.layers.19.mlp.down_proj.weight |   884736   |\n",
      "|     transformer.layers.20.ln_1.weight      |    576     |\n",
      "|  transformer.layers.20.attn.q_proj.weight  |   331776   |\n",
      "|  transformer.layers.20.attn.k_proj.weight  |   110592   |\n",
      "|  transformer.layers.20.attn.v_proj.weight  |   110592   |\n",
      "|  transformer.layers.20.attn.o_proj.weight  |   331776   |\n",
      "|     transformer.layers.20.ln_2.weight      |    576     |\n",
      "| transformer.layers.20.mlp.gate_proj.weight |   884736   |\n",
      "|  transformer.layers.20.mlp.up_proj.weight  |   884736   |\n",
      "| transformer.layers.20.mlp.down_proj.weight |   884736   |\n",
      "|     transformer.layers.21.ln_1.weight      |    576     |\n",
      "|  transformer.layers.21.attn.q_proj.weight  |   331776   |\n",
      "|  transformer.layers.21.attn.k_proj.weight  |   110592   |\n",
      "|  transformer.layers.21.attn.v_proj.weight  |   110592   |\n",
      "|  transformer.layers.21.attn.o_proj.weight  |   331776   |\n",
      "|     transformer.layers.21.ln_2.weight      |    576     |\n",
      "| transformer.layers.21.mlp.gate_proj.weight |   884736   |\n",
      "|  transformer.layers.21.mlp.up_proj.weight  |   884736   |\n",
      "| transformer.layers.21.mlp.down_proj.weight |   884736   |\n",
      "|     transformer.layers.22.ln_1.weight      |    576     |\n",
      "|  transformer.layers.22.attn.q_proj.weight  |   331776   |\n",
      "|  transformer.layers.22.attn.k_proj.weight  |   110592   |\n",
      "|  transformer.layers.22.attn.v_proj.weight  |   110592   |\n",
      "|  transformer.layers.22.attn.o_proj.weight  |   331776   |\n",
      "|     transformer.layers.22.ln_2.weight      |    576     |\n",
      "| transformer.layers.22.mlp.gate_proj.weight |   884736   |\n",
      "|  transformer.layers.22.mlp.up_proj.weight  |   884736   |\n",
      "| transformer.layers.22.mlp.down_proj.weight |   884736   |\n",
      "|     transformer.layers.23.ln_1.weight      |    576     |\n",
      "|  transformer.layers.23.attn.q_proj.weight  |   331776   |\n",
      "|  transformer.layers.23.attn.k_proj.weight  |   110592   |\n",
      "|  transformer.layers.23.attn.v_proj.weight  |   110592   |\n",
      "|  transformer.layers.23.attn.o_proj.weight  |   331776   |\n",
      "|     transformer.layers.23.ln_2.weight      |    576     |\n",
      "| transformer.layers.23.mlp.gate_proj.weight |   884736   |\n",
      "|  transformer.layers.23.mlp.up_proj.weight  |   884736   |\n",
      "| transformer.layers.23.mlp.down_proj.weight |   884736   |\n",
      "|     transformer.layers.24.ln_1.weight      |    576     |\n",
      "|  transformer.layers.24.attn.q_proj.weight  |   331776   |\n",
      "|  transformer.layers.24.attn.k_proj.weight  |   110592   |\n",
      "|  transformer.layers.24.attn.v_proj.weight  |   110592   |\n",
      "|  transformer.layers.24.attn.o_proj.weight  |   331776   |\n",
      "|     transformer.layers.24.ln_2.weight      |    576     |\n",
      "| transformer.layers.24.mlp.gate_proj.weight |   884736   |\n",
      "|  transformer.layers.24.mlp.up_proj.weight  |   884736   |\n",
      "| transformer.layers.24.mlp.down_proj.weight |   884736   |\n",
      "|     transformer.layers.25.ln_1.weight      |    576     |\n",
      "|  transformer.layers.25.attn.q_proj.weight  |   331776   |\n",
      "|  transformer.layers.25.attn.k_proj.weight  |   110592   |\n",
      "|  transformer.layers.25.attn.v_proj.weight  |   110592   |\n",
      "|  transformer.layers.25.attn.o_proj.weight  |   331776   |\n",
      "|     transformer.layers.25.ln_2.weight      |    576     |\n",
      "| transformer.layers.25.mlp.gate_proj.weight |   884736   |\n",
      "|  transformer.layers.25.mlp.up_proj.weight  |   884736   |\n",
      "| transformer.layers.25.mlp.down_proj.weight |   884736   |\n",
      "|     transformer.layers.26.ln_1.weight      |    576     |\n",
      "|  transformer.layers.26.attn.q_proj.weight  |   331776   |\n",
      "|  transformer.layers.26.attn.k_proj.weight  |   110592   |\n",
      "|  transformer.layers.26.attn.v_proj.weight  |   110592   |\n",
      "|  transformer.layers.26.attn.o_proj.weight  |   331776   |\n",
      "|     transformer.layers.26.ln_2.weight      |    576     |\n",
      "| transformer.layers.26.mlp.gate_proj.weight |   884736   |\n",
      "|  transformer.layers.26.mlp.up_proj.weight  |   884736   |\n",
      "| transformer.layers.26.mlp.down_proj.weight |   884736   |\n",
      "|     transformer.layers.27.ln_1.weight      |    576     |\n",
      "|  transformer.layers.27.attn.q_proj.weight  |   331776   |\n",
      "|  transformer.layers.27.attn.k_proj.weight  |   110592   |\n",
      "|  transformer.layers.27.attn.v_proj.weight  |   110592   |\n",
      "|  transformer.layers.27.attn.o_proj.weight  |   331776   |\n",
      "|     transformer.layers.27.ln_2.weight      |    576     |\n",
      "| transformer.layers.27.mlp.gate_proj.weight |   884736   |\n",
      "|  transformer.layers.27.mlp.up_proj.weight  |   884736   |\n",
      "| transformer.layers.27.mlp.down_proj.weight |   884736   |\n",
      "|     transformer.layers.28.ln_1.weight      |    576     |\n",
      "|  transformer.layers.28.attn.q_proj.weight  |   331776   |\n",
      "|  transformer.layers.28.attn.k_proj.weight  |   110592   |\n",
      "|  transformer.layers.28.attn.v_proj.weight  |   110592   |\n",
      "|  transformer.layers.28.attn.o_proj.weight  |   331776   |\n",
      "|     transformer.layers.28.ln_2.weight      |    576     |\n",
      "| transformer.layers.28.mlp.gate_proj.weight |   884736   |\n",
      "|  transformer.layers.28.mlp.up_proj.weight  |   884736   |\n",
      "| transformer.layers.28.mlp.down_proj.weight |   884736   |\n",
      "|     transformer.layers.29.ln_1.weight      |    576     |\n",
      "|  transformer.layers.29.attn.q_proj.weight  |   331776   |\n",
      "|  transformer.layers.29.attn.k_proj.weight  |   110592   |\n",
      "|  transformer.layers.29.attn.v_proj.weight  |   110592   |\n",
      "|  transformer.layers.29.attn.o_proj.weight  |   331776   |\n",
      "|     transformer.layers.29.ln_2.weight      |    576     |\n",
      "| transformer.layers.29.mlp.gate_proj.weight |   884736   |\n",
      "|  transformer.layers.29.mlp.up_proj.weight  |   884736   |\n",
      "| transformer.layers.29.mlp.down_proj.weight |   884736   |\n",
      "+--------------------------------------------+------------+\n",
      "Total Trainable Params: 134515008\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "134515008"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "CELL 14: Count Parameters in Our From-Scratch Model\n",
    "====================================================\n",
    "This cell counts all trainable parameters in our from-scratch implementation\n",
    "and displays them in a formatted table. This verifies our model has the\n",
    "correct number of parameters (134.5M) matching the reference.\n",
    "\n",
    "What it does:\n",
    "1. Counts parameters: Iterates through all model parameters\n",
    "2. Creates table: Shows parameter count per layer/component\n",
    "3. Verifies total: Should match ~134,515,008 parameters\n",
    "4. Compares with reference: Can compare with Cell 4 output\n",
    "\n",
    "This is crucial for:\n",
    "- Verifying architecture correctness\n",
    "- Understanding parameter distribution\n",
    "- Debugging any mismatches with reference model\n",
    "- Confirming we have the right model size\n",
    "\"\"\"\n",
    "print(model)\n",
    "print(sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "def count_parameters(model):\n",
    "    \"\"\"\n",
    "    Count and display all trainable parameters in our from-scratch model.\n",
    "    This should match the reference model's parameter count (~134.5M).\n",
    "    \"\"\"\n",
    "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad:\n",
    "            continue\n",
    "        params = parameter.numel()\n",
    "        table.add_row([name, params])\n",
    "        total_params += params\n",
    "    print(table)\n",
    "    print(f\"Total Trainable Params: {total_params}\")\n",
    "    return total_params\n",
    "    \n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcbb053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (341094 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 341094 tokens\n",
      "1 epoch = 27 batches\n",
      "\n",
      "================================================================================\n",
      "Starting Training for 5000 Steps\n",
      "================================================================================\n",
      "Batch size: 12, Sequence length: 1024, Steps: 10\n",
      "Text generation every 500 steps\n",
      "================================================================================\n",
      "\n",
      "Step     0 | Loss: 11.6533 | dt: 507.77ms | tok/sec: 24199.90\n",
      "Step    50 | Loss: 6.1897 | dt: 288.89ms | tok/sec: 42535.20\n",
      "Step   100 | Loss: 5.5847 | dt: 291.92ms | tok/sec: 42094.35\n",
      "Step   150 | Loss: 4.5673 | dt: 294.79ms | tok/sec: 41684.04\n",
      "Step   200 | Loss: 4.4035 | dt: 297.27ms | tok/sec: 41335.92\n",
      "Step   250 | Loss: 3.9740 | dt: 307.44ms | tok/sec: 39968.55\n",
      "Step   300 | Loss: 3.9188 | dt: 301.95ms | tok/sec: 40695.80\n",
      "Step   350 | Loss: 3.4227 | dt: 308.90ms | tok/sec: 39780.19\n",
      "Step   400 | Loss: 3.4667 | dt: 308.22ms | tok/sec: 39866.99\n",
      "Step   450 | Loss: 3.4917 | dt: 303.53ms | tok/sec: 40483.87\n",
      "\n",
      "================================================================================\n",
      "Step 500 - Model Output:\n",
      "================================================================================\n",
      "Prompt: 'The weather is'\n",
      "Generated: 'The weather is in\n",
      "To whom we have done, and so much of his face.\n",
      "\n",
      "KING RICHARD III:\n",
      "I will be so, and so shall not be a word.\n",
      "\n",
      "KING RICHARD III:\n",
      "I will'\n",
      "================================================================================\n",
      "\n",
      "Checkpoint saved to: checkpoint_step_500.pt\n",
      "Loss at step 500: 3.0544\n",
      "\n",
      "Step   500 | Loss: 3.0501 | dt: 292.21ms | tok/sec: 42052.48\n",
      "Step   550 | Loss: 2.7243 | dt: 294.83ms | tok/sec: 41678.61\n",
      "Step   600 | Loss: 2.4801 | dt: 297.37ms | tok/sec: 41322.56\n",
      "Step   650 | Loss: 2.1574 | dt: 300.65ms | tok/sec: 40870.88\n",
      "Step   700 | Loss: 1.9147 | dt: 303.36ms | tok/sec: 40506.84\n",
      "Step   750 | Loss: 1.6617 | dt: 306.73ms | tok/sec: 40061.75\n",
      "Step   800 | Loss: 1.4027 | dt: 303.02ms | tok/sec: 40552.03\n",
      "Step   850 | Loss: 1.1088 | dt: 306.10ms | tok/sec: 40143.38\n",
      "Step   900 | Loss: 0.8604 | dt: 303.50ms | tok/sec: 40487.37\n",
      "Step   950 | Loss: 0.6056 | dt: 310.36ms | tok/sec: 39592.92\n",
      "\n",
      "================================================================================\n",
      "Step 1000 - Model Output:\n",
      "================================================================================\n",
      "Prompt: 'The weather is'\n",
      "Generated: 'The weather is true:\n",
      "I prithee, sir, your lordship,\n",
      "And I will hold, my master's good.\n",
      "And I know by chance and come by this place\n",
      "That I have given a simple gentleman:\n",
      "I know my mistress'\n",
      "================================================================================\n",
      "\n",
      "Checkpoint saved to: checkpoint_step_1000.pt\n",
      "Loss at step 1000: 0.5325\n",
      "\n",
      "Step  1000 | Loss: 0.4950 | dt: 292.38ms | tok/sec: 42028.03\n",
      "Step  1050 | Loss: 0.3247 | dt: 295.91ms | tok/sec: 41526.19\n",
      "Step  1100 | Loss: 0.1984 | dt: 300.44ms | tok/sec: 40899.84\n",
      "Step  1150 | Loss: 0.0976 | dt: 308.78ms | tok/sec: 39795.88\n",
      "Step  1200 | Loss: 0.0567 | dt: 304.92ms | tok/sec: 40298.82\n",
      "Step  1250 | Loss: 0.0267 | dt: 303.70ms | tok/sec: 40460.64\n",
      "Step  1300 | Loss: 0.0161 | dt: 304.54ms | tok/sec: 40349.93\n",
      "Step  1350 | Loss: 0.0188 | dt: 307.41ms | tok/sec: 39972.80\n",
      "Step  1400 | Loss: 0.0108 | dt: 303.58ms | tok/sec: 40476.94\n",
      "Step  1450 | Loss: 0.0101 | dt: 304.10ms | tok/sec: 40407.44\n",
      "\n",
      "================================================================================\n",
      "Step 1500 - Model Output:\n",
      "================================================================================\n",
      "Prompt: 'The weather is'\n",
      "Generated: 'The weather is true:\n",
      "I prithee, is a woman's man.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "No, well for any thing is.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "How hast the matter'\n",
      "================================================================================\n",
      "\n",
      "Checkpoint saved to: checkpoint_step_1500.pt\n",
      "Loss at step 1500: 0.0092\n",
      "\n",
      "Step  1500 | Loss: 0.0086 | dt: 291.07ms | tok/sec: 42216.24\n",
      "Step  1550 | Loss: 0.0074 | dt: 297.34ms | tok/sec: 41326.14\n",
      "Step  1600 | Loss: 0.0081 | dt: 303.48ms | tok/sec: 40490.77\n",
      "Step  1650 | Loss: 0.0099 | dt: 308.35ms | tok/sec: 39850.38\n",
      "Step  1700 | Loss: 0.0058 | dt: 307.43ms | tok/sec: 39970.26\n",
      "Step  1750 | Loss: 0.0066 | dt: 309.04ms | tok/sec: 39761.99\n",
      "Step  1800 | Loss: 0.0053 | dt: 305.51ms | tok/sec: 40220.73\n",
      "Step  1850 | Loss: 0.0057 | dt: 309.46ms | tok/sec: 39708.41\n",
      "Step  1900 | Loss: 0.0062 | dt: 306.84ms | tok/sec: 40047.34\n",
      "Step  1950 | Loss: 0.0045 | dt: 308.31ms | tok/sec: 39856.60\n",
      "\n",
      "================================================================================\n",
      "Step 2000 - Model Output:\n",
      "================================================================================\n",
      "Prompt: 'The weather is'\n",
      "Generated: 'The weather is true:\n",
      "I prithee, is a woman's man.\n",
      "\n",
      "QUEEN MARGARET:\n",
      "The Earl of one, fair shepherd, as one:\n",
      "My father, I hope, is too late.\n",
      "\n",
      "QUEEN'\n",
      "================================================================================\n",
      "\n",
      "Checkpoint saved to: checkpoint_step_2000.pt\n",
      "Loss at step 2000: 0.0086\n",
      "\n",
      "Step  2000 | Loss: 0.0088 | dt: 291.77ms | tok/sec: 42115.02\n",
      "Step  2050 | Loss: 0.0041 | dt: 295.38ms | tok/sec: 41600.26\n",
      "Step  2100 | Loss: 0.0035 | dt: 303.96ms | tok/sec: 40426.58\n",
      "Step  2150 | Loss: 0.0038 | dt: 300.74ms | tok/sec: 40859.57\n",
      "Step  2200 | Loss: 0.0037 | dt: 307.35ms | tok/sec: 39980.95\n",
      "Step  2250 | Loss: 0.0045 | dt: 303.06ms | tok/sec: 40546.04\n",
      "Step  2300 | Loss: 0.0035 | dt: 311.01ms | tok/sec: 39509.54\n",
      "Step  2350 | Loss: 0.0084 | dt: 303.70ms | tok/sec: 40461.40\n",
      "Step  2400 | Loss: 0.0045 | dt: 306.96ms | tok/sec: 40031.76\n",
      "Step  2450 | Loss: 0.0033 | dt: 307.21ms | tok/sec: 39999.11\n",
      "\n",
      "================================================================================\n",
      "Step 2500 - Model Output:\n",
      "================================================================================\n",
      "Prompt: 'The weather is'\n",
      "Generated: 'The weather is true:\n",
      "I prithee, is a woman's man.\n",
      "\n",
      "QUEEN MARGARET:\n",
      "The Earl of one, fair shepherd, as one:\n",
      "My father, I hope, is so ill.\n",
      "\n",
      "QUEEN'\n",
      "================================================================================\n",
      "\n",
      "Checkpoint saved to: checkpoint_step_2500.pt\n",
      "Loss at step 2500: 0.0031\n",
      "\n",
      "Step  2500 | Loss: 0.0042 | dt: 291.93ms | tok/sec: 42092.87\n",
      "Step  2550 | Loss: 0.0039 | dt: 296.31ms | tok/sec: 41469.89\n",
      "Step  2600 | Loss: 0.0023 | dt: 302.32ms | tok/sec: 40645.93\n",
      "Step  2650 | Loss: 0.0026 | dt: 303.74ms | tok/sec: 40455.55\n",
      "Step  2700 | Loss: 0.0056 | dt: 299.57ms | tok/sec: 41018.95\n",
      "Step  2750 | Loss: 0.0025 | dt: 304.97ms | tok/sec: 40292.61\n",
      "Step  2800 | Loss: 0.0021 | dt: 302.03ms | tok/sec: 40684.85\n",
      "Step  2850 | Loss: 0.0026 | dt: 303.37ms | tok/sec: 40504.68\n",
      "Step  2900 | Loss: 0.0018 | dt: 305.62ms | tok/sec: 40206.42\n",
      "Step  2950 | Loss: 0.0031 | dt: 306.48ms | tok/sec: 40093.36\n",
      "\n",
      "================================================================================\n",
      "Step 3000 - Model Output:\n",
      "================================================================================\n",
      "Prompt: 'The weather is'\n",
      "Generated: 'The weather is true:\n",
      "I prithee, is a woman's man.\n",
      "\n",
      "QUEEN MARGARET:\n",
      "The Earl of one, that we may choose but for\n",
      "For thee I firmly am a slave of!\n",
      "\n",
      "QUEEN EL'\n",
      "================================================================================\n",
      "\n",
      "Checkpoint saved to: checkpoint_step_3000.pt\n",
      "Loss at step 3000: 0.0067\n",
      "\n",
      "Step  3000 | Loss: 0.0050 | dt: 292.46ms | tok/sec: 42016.69\n",
      "Step  3050 | Loss: 0.0019 | dt: 294.52ms | tok/sec: 41722.04\n",
      "Step  3100 | Loss: 0.0029 | dt: 300.81ms | tok/sec: 40849.40\n",
      "Step  3150 | Loss: 0.0020 | dt: 308.37ms | tok/sec: 39848.25\n",
      "Step  3200 | Loss: 0.0083 | dt: 302.66ms | tok/sec: 40599.73\n",
      "Step  3250 | Loss: 0.1520 | dt: 303.79ms | tok/sec: 40448.47\n",
      "Step  3300 | Loss: 0.5967 | dt: 303.83ms | tok/sec: 40443.08\n",
      "Step  3350 | Loss: 0.0940 | dt: 304.32ms | tok/sec: 40378.85\n",
      "Step  3400 | Loss: 0.0387 | dt: 305.90ms | tok/sec: 40169.95\n",
      "Step  3450 | Loss: 0.0384 | dt: 308.52ms | tok/sec: 39828.76\n",
      "\n",
      "================================================================================\n",
      "Step 3500 - Model Output:\n",
      "================================================================================\n",
      "Prompt: 'The weather is'\n",
      "Generated: 'The weather is true:\n",
      "I prithee, sir, with one mess of York;\n",
      "The one, for I pardy man that would stand.\n",
      "\n",
      "EDWARD:\n",
      "And I do beseech you, sir, had rather\n",
      "'\n",
      "================================================================================\n",
      "\n",
      "Checkpoint saved to: checkpoint_step_3500.pt\n",
      "Loss at step 3500: 0.0109\n",
      "\n",
      "Step  3500 | Loss: 0.0088 | dt: 291.74ms | tok/sec: 42120.01\n",
      "Step  3550 | Loss: 0.0042 | dt: 295.60ms | tok/sec: 41569.83\n",
      "Step  3600 | Loss: 0.0045 | dt: 300.59ms | tok/sec: 40879.66\n",
      "Step  3650 | Loss: 0.0033 | dt: 303.31ms | tok/sec: 40513.02\n",
      "Step  3700 | Loss: 0.0069 | dt: 303.08ms | tok/sec: 40543.64\n",
      "Step  3750 | Loss: 0.0039 | dt: 307.73ms | tok/sec: 39930.56\n",
      "Step  3800 | Loss: 0.0027 | dt: 306.95ms | tok/sec: 40032.19\n",
      "Step  3850 | Loss: 0.0036 | dt: 307.20ms | tok/sec: 39999.97\n",
      "Step  3900 | Loss: 0.0033 | dt: 306.86ms | tok/sec: 40043.98\n",
      "Step  3950 | Loss: 0.0018 | dt: 305.56ms | tok/sec: 40215.05\n",
      "\n",
      "================================================================================\n",
      "Step 4000 - Model Output:\n",
      "================================================================================\n",
      "Prompt: 'The weather is'\n",
      "Generated: 'The weather is true:\n",
      "I prithee, masters, dismalmal scene,\n",
      "That we may be consul.\n",
      "\n",
      "TRANIO:\n",
      "Yea, you know not.\n",
      "\n",
      "First Senator:\n",
      "But that's the duke?\n",
      "'\n",
      "================================================================================\n",
      "\n",
      "Checkpoint saved to: checkpoint_step_4000.pt\n",
      "Loss at step 4000: 0.0048\n",
      "\n",
      "Step  4000 | Loss: 0.0022 | dt: 292.37ms | tok/sec: 42029.06\n",
      "Step  4050 | Loss: 0.0050 | dt: 295.56ms | tok/sec: 41575.06\n",
      "Step  4100 | Loss: 0.0020 | dt: 298.97ms | tok/sec: 41100.85\n",
      "Step  4150 | Loss: 0.0015 | dt: 303.82ms | tok/sec: 40445.05\n",
      "Step  4200 | Loss: 0.0021 | dt: 304.15ms | tok/sec: 40400.47\n",
      "Step  4250 | Loss: 0.0013 | dt: 309.35ms | tok/sec: 39722.30\n",
      "Step  4300 | Loss: 0.0026 | dt: 306.69ms | tok/sec: 40066.99\n",
      "Step  4350 | Loss: 0.0044 | dt: 307.87ms | tok/sec: 39913.21\n",
      "Step  4400 | Loss: 0.0015 | dt: 305.30ms | tok/sec: 40248.68\n",
      "Step  4450 | Loss: 0.0024 | dt: 305.07ms | tok/sec: 40278.91\n",
      "\n",
      "================================================================================\n",
      "Step 4500 - Model Output:\n",
      "================================================================================\n",
      "Prompt: 'The weather is'\n",
      "Generated: 'The weather is true:\n",
      "I prithee, masters, dismalmal scene,\n",
      "That we may be consul.\n",
      "\n",
      "TRANIO:\n",
      "Yea, you know not.\n",
      "\n",
      "HORTENSIO:\n",
      "Now I have done.'\n",
      "================================================================================\n",
      "\n",
      "Checkpoint saved to: checkpoint_step_4500.pt\n",
      "Loss at step 4500: 0.0018\n",
      "\n",
      "Step  4500 | Loss: 0.0015 | dt: 295.11ms | tok/sec: 41638.17\n",
      "Step  4550 | Loss: 0.0023 | dt: 296.95ms | tok/sec: 41380.36\n",
      "Step  4600 | Loss: 0.0031 | dt: 301.60ms | tok/sec: 40742.52\n",
      "Step  4650 | Loss: 0.0014 | dt: 311.82ms | tok/sec: 39407.14\n",
      "Step  4700 | Loss: 0.0057 | dt: 305.82ms | tok/sec: 40181.03\n",
      "Step  4750 | Loss: 0.0017 | dt: 309.70ms | tok/sec: 39676.68\n",
      "Step  4800 | Loss: 0.0012 | dt: 304.65ms | tok/sec: 40335.09\n",
      "Step  4850 | Loss: 0.0016 | dt: 307.43ms | tok/sec: 39969.98\n",
      "Step  4900 | Loss: 0.0016 | dt: 310.65ms | tok/sec: 39555.91\n",
      "Step  4950 | Loss: 0.0024 | dt: 307.65ms | tok/sec: 39942.10\n",
      "Step  4999 | Loss: 0.0015 | dt: 318.37ms | tok/sec: 38597.02\n",
      "\n",
      "================================================================================\n",
      "Step 5000 - Model Output:\n",
      "================================================================================\n",
      "Prompt: 'The weather is'\n",
      "Generated: 'The weather is true:\n",
      "I prithee, masters, dismalmal scene,\n",
      "That we may be consul.\n",
      "\n",
      "TRANIO:\n",
      "Yea, you know not.\n",
      "\n",
      "HORTENSIO:\n",
      "Now I have done.'\n",
      "================================================================================\n",
      "\n",
      "Checkpoint saved to: checkpoint_step_5000.pt\n",
      "Loss at step 5000: 0.0015\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Training Complete! Saving checkpoint...\n",
      "================================================================================\n",
      "Final loss: 0.0015\n",
      "Checkpoint saved to: checkpoint_step_5000.pt\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "CELL 15: Training Loop - Train Model from Scratch\n",
    "==================================================\n",
    "This is the main training cell that trains our from-scratch SmolLMv2 model.\n",
    "This is where the actual learning happens - the model starts with random weights\n",
    "and learns to predict next tokens through gradient descent.\n",
    "\n",
    "What it does:\n",
    "1. Sets up training environment:\n",
    "   - Device selection (CPU/GPU)\n",
    "   - Random seed for reproducibility (1337)\n",
    "   - Training hyperparameters (batch size, sequence length, learning rate)\n",
    "\n",
    "2. Creates data loader:\n",
    "   - Loads and tokenizes training text from input.txt\n",
    "   - Creates batches of sequences for training\n",
    "   - Handles data streaming and epoch boundaries\n",
    "\n",
    "3. Initializes model:\n",
    "   - Creates model with random weights (from scratch!)\n",
    "   - Moves to appropriate device (GPU if available)\n",
    "   - Sets up optimizer (AdamW with learning rate 3e-4)\n",
    "\n",
    "4. Training loop:\n",
    "   - Forward pass: Compute predictions and loss\n",
    "   - Backward pass: Compute gradients\n",
    "   - Optimizer step: Update weights\n",
    "   - Logging: Print loss and metrics every 50 steps\n",
    "   - Text generation: Generate sample text every 500 steps\n",
    "   - Checkpointing: Save model every 500 steps\n",
    "\n",
    "5. Optimizations used:\n",
    "   - Flash Attention: Faster attention computation\n",
    "   - Mixed precision (bfloat16): Faster training on GPU\n",
    "   - High precision matmul: Better numerical stability\n",
    "\n",
    "Training details:\n",
    "- Batch size: 12 sequences\n",
    "- Sequence length: 1024 tokens\n",
    "- Learning rate: 3e-4 (standard for AdamW)\n",
    "- Optimizer: AdamW (adaptive learning rate)\n",
    "- Total steps: 5000 (adjustable)\n",
    "- Checkpoints: Saved every 500 steps\n",
    "\n",
    "Expected results:\n",
    "- Initial loss: ~11.65 (close to random baseline)\n",
    "- Loss decreases over time as model learns\n",
    "- Final loss: Should be < 2.0 after sufficient training\n",
    "- Training speed: ~26,000-42,000 tokens/sec on GPU\n",
    "\"\"\"\n",
    "# Device selection\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set seed for reproducibility\n",
    "torch.manual_seed(1337)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(1337)\n",
    "\n",
    "# Training hyperparameters\n",
    "B = 12  # batch size\n",
    "T = 1024  # sequence length\n",
    "num_steps = 10  # number of training steps\n",
    "learning_rate = 3e-4\n",
    "checkpoint_path = \"checkpoint_step_5000.pt\"\n",
    "\n",
    "# DataLoader class (similar to S13.ipynb but using tokenizer from config)\n",
    "class DataLoaderLite:\n",
    "    def __init__(self, B, T, tokenizer):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        # Load tokens from disk and store them in memory\n",
    "        with open('input.txt', 'r') as f:\n",
    "            text = f.read()\n",
    "        \n",
    "        # Tokenize using the tokenizer from config.yaml\n",
    "        tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "        self.tokens = torch.tensor(tokens)\n",
    "        print(f'Loaded {len(self.tokens)} tokens')\n",
    "        print(f'1 epoch = {len(self.tokens) // (B * T)} batches')\n",
    "\n",
    "        # State\n",
    "        self.current_position = 0\n",
    "    \n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "        buf = self.tokens[self.current_position: self.current_position + B * T + 1]\n",
    "        x = (buf[:-1]).view(B, T)  # inputs\n",
    "        y = (buf[1:]).view(B, T)  # targets\n",
    "        # Advance the position in the tensor\n",
    "        self.current_position += B * T\n",
    "        # If loading the next batch would be out of bounds, reset\n",
    "        if self.current_position + (B * T + 1) > len(self.tokens):\n",
    "            self.current_position = 0\n",
    "        return x, y\n",
    "\n",
    "# Ensure tokenizer is available (should be loaded from config.yaml in earlier cells)\n",
    "if 'tokenizer' not in globals():\n",
    "    with open(\"config.yaml\", 'r') as f:\n",
    "        config_dict = yaml.safe_load(f)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        config_dict[\"tokenizer\"][\"tokenizer_name_or_path\"],\n",
    "        use_fast=config_dict[\"tokenizer\"][\"use_fast\"]\n",
    "    )\n",
    "\n",
    "# Create data loader\n",
    "train_loader = DataLoaderLite(B=B, T=T, tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "model_config = cfg.model\n",
    "model_config.n_embd = model_config.hidden_size\n",
    "model_config.n_head = model_config.num_attention_heads\n",
    "model_config.block_size = model_config.max_position_embeddings\n",
    "model_config.device = device\n",
    "model_config.num_key_value_heads = model_config.num_key_value_heads\n",
    "\n",
    "model = LlamaModel(model_config)\n",
    "model.to(device)\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop with text generation every 500 steps\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Starting Training for 5000 Steps\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Batch size: {B}, Sequence length: {T}, Steps: {num_steps}\")\n",
    "print(f\"Text generation every 500 steps\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Prompt for text generation\n",
    "generation_prompt = \"The weather is\"\n",
    "max_new_tokens = 50\n",
    "num_steps = 5000\n",
    "model.train()\n",
    "for step in range(num_steps):\n",
    "    t0 = time.time()\n",
    "    \n",
    "    # Get batch\n",
    "    x, y = train_loader.next_batch()\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    \n",
    "    # Forward pass\n",
    "    optimizer.zero_grad()\n",
    "    with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "        logits, loss = model(x, y)\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Synchronize for accurate timing\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    t1 = time.time()\n",
    "    dt = (t1 - t0) * 1000  # milliseconds\n",
    "    tokens_per_sec = (train_loader.B * train_loader.T) / (t1 - t0)\n",
    "    \n",
    "    # Print metrics every step (or adjust frequency)\n",
    "    if step % 50 == 0 or step == num_steps - 1:\n",
    "        print(f'Step {step:5d} | Loss: {loss.item():.4f} | dt: {dt:6.2f}ms | tok/sec: {tokens_per_sec:8.2f}')\n",
    "    \n",
    "    # Generate text and save checkpoint every 500 steps\n",
    "    if (step + 1) % 500 == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Tokenize prompt\n",
    "            prompt_ids = tokenizer.encode(generation_prompt, return_tensors='pt').to(device)\n",
    "            \n",
    "            # Generate text\n",
    "            generated_ids = model.generate(prompt_ids, max_new_tokens=max_new_tokens)\n",
    "            \n",
    "            # Decode generated text\n",
    "            generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "            \n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"Step {step + 1} - Model Output:\")\n",
    "            print(f\"{'='*80}\")\n",
    "            print(f\"Prompt: '{generation_prompt}'\")\n",
    "            print(f\"Generated: '{generated_text}'\")\n",
    "            print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        # Save checkpoint every 500 steps\n",
    "        checkpoint_step_path = f\"checkpoint_step_{step + 1}.pt\"\n",
    "        checkpoint = {\n",
    "            'step': step + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss.item(),\n",
    "            'model_config': {\n",
    "                'n_embd': model_config.n_embd,\n",
    "                'n_head': model_config.n_head,\n",
    "                'block_size': model_config.block_size,\n",
    "                'num_key_value_heads': model_config.num_key_value_heads,\n",
    "                'hidden_size': model_config.hidden_size,\n",
    "                'vocab_size': model_config.vocab_size,\n",
    "                'num_hidden_layers': model_config.num_hidden_layers,\n",
    "                'intermediate_size': model_config.intermediate_size,\n",
    "                'rms_norm_eps': model_config.rms_norm_eps,\n",
    "                'hidden_act': model_config.hidden_act,\n",
    "                'mlp_bias': model_config.mlp_bias,\n",
    "                'device': str(device)\n",
    "            },\n",
    "            # Add data loader state for reproducibility\n",
    "            'data_loader_position': train_loader.current_position,\n",
    "            # Add random states for reproducibility\n",
    "            'rng_state': torch.get_rng_state(),\n",
    "            'cuda_rng_state': torch.cuda.get_rng_state() if torch.cuda.is_available() else None\n",
    "        }\n",
    "        torch.save(checkpoint, checkpoint_step_path)\n",
    "        print(f\"Checkpoint saved to: {checkpoint_step_path}\")\n",
    "        print(f\"Loss at step {step + 1}: {loss.item():.4f}\")\n",
    "        print(f\"Data loader position: {train_loader.current_position}\\n\")\n",
    "        \n",
    "        model.train()  # Set back to training mode\n",
    "\n",
    "# Final checkpoint save\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Training Complete! Saving checkpoint...\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Final loss: {loss.item():.4f}\")\n",
    "\n",
    "# Save checkpoint\n",
    "checkpoint = {\n",
    "    'step': num_steps,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': loss.item(),\n",
    "    'model_config': {\n",
    "        'n_embd': model_config.n_embd,\n",
    "        'n_head': model_config.n_head,\n",
    "        'block_size': model_config.block_size,\n",
    "        'num_key_value_heads': model_config.num_key_value_heads,\n",
    "        'hidden_size': model_config.hidden_size,\n",
    "        'vocab_size': model_config.vocab_size,\n",
    "        'num_hidden_layers': model_config.num_hidden_layers,\n",
    "        'intermediate_size': model_config.intermediate_size,\n",
    "        'rms_norm_eps': model_config.rms_norm_eps,\n",
    "        'hidden_act': model_config.hidden_act,\n",
    "        'mlp_bias': model_config.mlp_bias,\n",
    "        'device': str(device)\n",
    "    },\n",
    "    # Add data loader state for reproducibility\n",
    "    'data_loader_position': train_loader.current_position,\n",
    "    # Add random states for reproducibility\n",
    "    'rng_state': torch.get_rng_state(),\n",
    "    'cuda_rng_state': torch.cuda.get_rng_state() if torch.cuda.is_available() else None\n",
    "}\n",
    "\n",
    "torch.save(checkpoint, checkpoint_path)\n",
    "print(f\"Checkpoint saved to: {checkpoint_path}\")\n",
    "print(f\"Data loader position: {train_loader.current_position}\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9857aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Loading Checkpoint and Continuing Training\n",
      "================================================================================\n",
      "Loaded checkpoint from step 5000\n",
      "Checkpoint loss: 0.0015\n",
      "Model and optimizer loaded successfully!\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Continuing Training for 50 Additional Steps\n",
      "================================================================================\n",
      "Starting from step 5000, training to step 5050\n",
      "================================================================================\n",
      "\n",
      "Step  5001 | Loss: 0.8500 | dt: 461.76ms | tok/sec: 26611.20\n",
      "Step  5002 | Loss: 0.7351 | dt: 462.43ms | tok/sec: 26572.48\n",
      "Step  5003 | Loss: 0.7321 | dt: 463.81ms | tok/sec: 26493.70\n",
      "Step  5004 | Loss: 0.7072 | dt: 462.90ms | tok/sec: 26545.68\n",
      "Step  5005 | Loss: 0.7477 | dt: 460.00ms | tok/sec: 26712.94\n",
      "Step  5006 | Loss: 0.7876 | dt: 462.35ms | tok/sec: 26577.11\n",
      "Step  5007 | Loss: 0.8368 | dt: 463.83ms | tok/sec: 26492.30\n",
      "Step  5008 | Loss: 0.8565 | dt: 464.64ms | tok/sec: 26446.49\n",
      "Step  5009 | Loss: 0.8898 | dt: 469.10ms | tok/sec: 26194.60\n",
      "Step  5010 | Loss: 0.9158 | dt: 464.79ms | tok/sec: 26437.81\n",
      "Step  5011 | Loss: 0.8765 | dt: 468.53ms | tok/sec: 26226.46\n",
      "Step  5012 | Loss: 0.9574 | dt: 466.98ms | tok/sec: 26314.03\n",
      "Step  5013 | Loss: 0.9932 | dt: 468.14ms | tok/sec: 26248.70\n",
      "Step  5014 | Loss: 1.0409 | dt: 471.97ms | tok/sec: 26035.61\n",
      "Step  5015 | Loss: 1.1348 | dt: 466.28ms | tok/sec: 26353.33\n",
      "Step  5016 | Loss: 1.1245 | dt: 472.74ms | tok/sec: 25993.11\n",
      "Step  5017 | Loss: 1.1730 | dt: 469.08ms | tok/sec: 26195.96\n",
      "Step  5018 | Loss: 1.2089 | dt: 475.20ms | tok/sec: 25858.81\n",
      "Step  5019 | Loss: 1.1597 | dt: 472.07ms | tok/sec: 26029.79\n",
      "Step  5020 | Loss: 1.2100 | dt: 473.20ms | tok/sec: 25967.95\n",
      "Step  5021 | Loss: 1.3013 | dt: 474.37ms | tok/sec: 25903.63\n",
      "Step  5022 | Loss: 1.3941 | dt: 474.32ms | tok/sec: 25906.29\n",
      "Step  5023 | Loss: 1.3461 | dt: 473.77ms | tok/sec: 25936.89\n",
      "Step  5024 | Loss: 1.3492 | dt: 473.57ms | tok/sec: 25947.78\n",
      "Step  5025 | Loss: 1.3519 | dt: 475.47ms | tok/sec: 25844.17\n",
      "Step  5026 | Loss: 1.2780 | dt: 473.87ms | tok/sec: 25931.20\n",
      "Step  5027 | Loss: 1.2971 | dt: 475.62ms | tok/sec: 25835.85\n",
      "Step  5028 | Loss: 0.8944 | dt: 476.14ms | tok/sec: 25807.43\n",
      "Step  5029 | Loss: 0.8224 | dt: 477.66ms | tok/sec: 25725.38\n",
      "Step  5030 | Loss: 0.7805 | dt: 477.36ms | tok/sec: 25741.67\n",
      "Step  5031 | Loss: 0.8246 | dt: 477.48ms | tok/sec: 25734.97\n",
      "Step  5032 | Loss: 0.8061 | dt: 476.38ms | tok/sec: 25794.79\n",
      "Step  5033 | Loss: 0.7054 | dt: 476.25ms | tok/sec: 25801.44\n",
      "Step  5034 | Loss: 0.7349 | dt: 477.26ms | tok/sec: 25746.97\n",
      "Step  5035 | Loss: 0.7448 | dt: 478.20ms | tok/sec: 25696.17\n",
      "Step  5036 | Loss: 0.7003 | dt: 478.54ms | tok/sec: 25677.88\n",
      "Step  5037 | Loss: 0.6795 | dt: 479.51ms | tok/sec: 25626.31\n",
      "Step  5038 | Loss: 0.6485 | dt: 480.05ms | tok/sec: 25597.07\n",
      "Step  5039 | Loss: 0.6696 | dt: 480.16ms | tok/sec: 25591.28\n",
      "Step  5040 | Loss: 0.6328 | dt: 480.57ms | tok/sec: 25569.77\n",
      "Step  5041 | Loss: 0.6221 | dt: 480.53ms | tok/sec: 25572.00\n",
      "Step  5042 | Loss: 0.6814 | dt: 480.26ms | tok/sec: 25586.13\n",
      "Step  5043 | Loss: 0.6399 | dt: 479.47ms | tok/sec: 25628.25\n",
      "Step  5044 | Loss: 0.6242 | dt: 481.36ms | tok/sec: 25527.90\n",
      "Step  5045 | Loss: 0.6255 | dt: 479.39ms | tok/sec: 25632.80\n",
      "Step  5046 | Loss: 0.5918 | dt: 481.94ms | tok/sec: 25496.77\n",
      "Step  5047 | Loss: 0.6064 | dt: 479.80ms | tok/sec: 25610.42\n",
      "Step  5048 | Loss: 0.6497 | dt: 481.14ms | tok/sec: 25539.12\n",
      "Step  5049 | Loss: 0.6884 | dt: 481.52ms | tok/sec: 25519.10\n",
      "Step  5050 | Loss: 0.6654 | dt: 481.52ms | tok/sec: 25519.24\n",
      "\n",
      "================================================================================\n",
      "Final Text Generation After Additional Training\n",
      "================================================================================\n",
      "Prompt: 'The weather is'\n",
      "Generated: 'The weather is little stars, the fair creature died.\n",
      "\n",
      "KING EDWARD IV:\n",
      "It will believe this my heart to thy sovereign's grave.\n",
      "\n",
      "YORK:\n",
      "O wonderful, with many children, but kneelelel down.'\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Saving Final Checkpoint...\n",
      "================================================================================\n",
      "Final checkpoint saved to: checkpoint_step_5050.pt\n",
      "Final loss: 0.6654\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "CELL 16: Resume Training from Checkpoint\n",
    "=========================================\n",
    "This cell demonstrates how to resume training from a saved checkpoint. This is\n",
    "essential for:\n",
    "1. Continuing training after interruption\n",
    "2. Fine-tuning from a specific checkpoint\n",
    "3. Experimenting with different training schedules\n",
    "\n",
    "What it does:\n",
    "1. Loads checkpoint:\n",
    "   - Model weights: Restores trained parameters\n",
    "   - Optimizer state: Restores optimizer momentum/state\n",
    "   - Training step: Knows where we left off\n",
    "   - Random states: Ensures reproducibility\n",
    "\n",
    "2. Restores training state:\n",
    "   - Model weights: Loads trained parameters\n",
    "   - Optimizer: Restores AdamW state (momentum, etc.)\n",
    "   - Data loader position: Continues from same data position\n",
    "   - Random seeds: Ensures same random behavior\n",
    "\n",
    "3. Continues training:\n",
    "   - Trains for additional 50 steps (configurable)\n",
    "   - Saves final checkpoint with updated step count\n",
    "   - Generates sample text to show progress\n",
    "\n",
    "Key features:\n",
    "- Full reproducibility: Random states saved/restored\n",
    "- Seamless continuation: No loss of training progress\n",
    "- Flexible: Can resume from any checkpoint\n",
    "- Safe: Saves final state after continuation\n",
    "\n",
    "This is crucial for long training runs that may be interrupted!\n",
    "\"\"\"\n",
    "# Load checkpoint and continue training for 50 more steps\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Loading Checkpoint and Continuing Training\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Device selection (if not already set)\n",
    "if 'device' not in globals():\n",
    "    device = 'cpu'\n",
    "    if torch.cuda.is_available():\n",
    "        device = 'cuda'\n",
    "    elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "        device = \"mps\"\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "# Load checkpoint\n",
    "checkpoint_path = \"checkpoint_step_5000.pt\"\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "\n",
    "print(f\"Loaded checkpoint from step {checkpoint['step']}\")\n",
    "print(f\"Checkpoint loss: {checkpoint['loss']:.4f}\")\n",
    "\n",
    "# Restore random states for reproducibility\n",
    "if 'rng_state' in checkpoint:\n",
    "    torch.set_rng_state(checkpoint['rng_state'])\n",
    "    print(\"Restored PyTorch random state\")\n",
    "if 'cuda_rng_state' in checkpoint and checkpoint['cuda_rng_state'] is not None:\n",
    "    torch.cuda.set_rng_state(checkpoint['cuda_rng_state'])\n",
    "    print(\"Restored CUDA random state\")\n",
    "\n",
    "model_config = cfg.model\n",
    "model_config.n_embd = model_config.hidden_size\n",
    "model_config.n_head = model_config.num_attention_heads\n",
    "model_config.block_size = model_config.max_position_embeddings\n",
    "model_config.device = device\n",
    "model_config.num_key_value_heads = model_config.num_key_value_heads\n",
    "\n",
    "model = LlamaModel(model_config)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.to(device)\n",
    "\n",
    "# Create optimizer and load state\n",
    "learning_rate = 3e-4  # Use same learning rate as training\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "print(\"Model and optimizer loaded successfully!\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Recreate data loader if needed\n",
    "if 'train_loader' not in globals():\n",
    "    # Ensure tokenizer is available\n",
    "    if 'tokenizer' not in globals():\n",
    "        with open(\"config.yaml\", 'r') as f:\n",
    "            config_dict = yaml.safe_load(f)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            config_dict[\"tokenizer\"][\"tokenizer_name_or_path\"],\n",
    "            use_fast=config_dict[\"tokenizer\"][\"use_fast\"]\n",
    "        )\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    B = 8  # batch size\n",
    "    T = 1024  # sequence length\n",
    "    \n",
    "    # DataLoader class\n",
    "    class DataLoaderLite:\n",
    "        def __init__(self, B, T, tokenizer):\n",
    "            self.B = B\n",
    "            self.T = T\n",
    "            self.tokenizer = tokenizer\n",
    "            with open('input.txt', 'r') as f:\n",
    "                text = f.read()\n",
    "            tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "            self.tokens = torch.tensor(tokens)\n",
    "            self.current_position = 0\n",
    "        \n",
    "        def next_batch(self):\n",
    "            B, T = self.B, self.T\n",
    "            buf = self.tokens[self.current_position: self.current_position + B * T + 1]\n",
    "            x = (buf[:-1]).view(B, T)\n",
    "            y = (buf[1:]).view(B, T)\n",
    "            self.current_position += B * T\n",
    "            if self.current_position + (B * T + 1) > len(self.tokens):\n",
    "                self.current_position = 0\n",
    "            return x, y\n",
    "    \n",
    "    train_loader = DataLoaderLite(B=B, T=T, tokenizer=tokenizer)\n",
    "\n",
    "# Restore data loader position from checkpoint for reproducibility\n",
    "if 'data_loader_position' in checkpoint:\n",
    "    train_loader.current_position = checkpoint['data_loader_position']\n",
    "    print(f\"Restored data loader position: {train_loader.current_position}\")\n",
    "else:\n",
    "    print(\"Warning: No data_loader_position in checkpoint, starting from position 0\")\n",
    "\n",
    "# Continue training for 50 more steps\n",
    "num_additional_steps = 50\n",
    "final_checkpoint_path = \"checkpoint_step_5050.pt\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"Continuing Training for {num_additional_steps} Additional Steps\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Starting from step {checkpoint['step']}, training to step {checkpoint['step'] + num_additional_steps}\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "model.train()\n",
    "for step in range(num_additional_steps):\n",
    "    t0 = time.time()\n",
    "    \n",
    "    # Get batch\n",
    "    x, y = train_loader.next_batch()\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    \n",
    "    # Forward pass\n",
    "    optimizer.zero_grad()\n",
    "    logits, loss = model(x, y)\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Synchronize for accurate timing\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    t1 = time.time()\n",
    "    dt = (t1 - t0) * 1000  # milliseconds\n",
    "    tokens_per_sec = (train_loader.B * train_loader.T) / (t1 - t0)\n",
    "    \n",
    "    # Print metrics\n",
    "    current_step = checkpoint['step'] + step + 1\n",
    "    print(f'Step {current_step:5d} | Loss: {loss.item():.4f} | dt: {dt:6.2f}ms | tok/sec: {tokens_per_sec:8.2f}')\n",
    "\n",
    "# Generate final text sample\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Final Text Generation After Additional Training\")\n",
    "print(\"=\"*80)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    generation_prompt = \"The weather is\"\n",
    "    prompt_ids = tokenizer.encode(generation_prompt, return_tensors='pt').to(device)\n",
    "    generated_ids = model.generate(prompt_ids, max_new_tokens=50)\n",
    "    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"Prompt: '{generation_prompt}'\")\n",
    "    print(f\"Generated: '{generated_text}'\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "# Save final checkpoint\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Saving Final Checkpoint...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "final_checkpoint = {\n",
    "    'step': checkpoint['step'] + num_additional_steps,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': loss.item(),\n",
    "     'model_config': {\n",
    "                'n_embd': model_config.n_embd,\n",
    "                'n_head': model_config.n_head,\n",
    "                'block_size': model_config.block_size,\n",
    "                'num_key_value_heads': model_config.num_key_value_heads,\n",
    "                'hidden_size': model_config.hidden_size,\n",
    "                'vocab_size': model_config.vocab_size,\n",
    "                'num_hidden_layers': model_config.num_hidden_layers,\n",
    "                'intermediate_size': model_config.intermediate_size,\n",
    "                'rms_norm_eps': model_config.rms_norm_eps,\n",
    "                'hidden_act': model_config.hidden_act,\n",
    "                'mlp_bias': model_config.mlp_bias,\n",
    "                'device': str(device)\n",
    "            },\n",
    "    # Add data loader state for reproducibility\n",
    "    'data_loader_position': train_loader.current_position,\n",
    "    # Add random states for reproducibility\n",
    "    'rng_state': torch.get_rng_state(),\n",
    "    'cuda_rng_state': torch.cuda.get_rng_state() if torch.cuda.is_available() else None\n",
    "}\n",
    "\n",
    "torch.save(final_checkpoint, final_checkpoint_path)\n",
    "print(f\"Final checkpoint saved to: {final_checkpoint_path}\")\n",
    "print(f\"Final loss: {loss.item():.4f}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f4f09d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
